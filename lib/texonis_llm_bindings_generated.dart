// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
import 'dart:ffi' as ffi;

/// llama.cpp binding
class TexonisLlmBindings {
  /// Holds the symbol lookup function.
  final ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
      _lookup;

  /// The symbols are looked up in [dynamicLibrary].
  TexonisLlmBindings(ffi.DynamicLibrary dynamicLibrary)
      : _lookup = dynamicLibrary.lookup;

  /// The symbols are looked up with [lookup].
  TexonisLlmBindings.fromLookup(
      ffi.Pointer<T> Function<T extends ffi.NativeType>(String symbolName)
          lookup)
      : _lookup = lookup;

  void ggml_abort(
    ffi.Pointer<ffi.Char> file,
    int line,
    ffi.Pointer<ffi.Char> fmt,
  ) {
    return _ggml_abort(
      file,
      line,
      fmt,
    );
  }

  late final _ggml_abortPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ffi.Char>, ffi.Int,
              ffi.Pointer<ffi.Char>)>>('ggml_abort');
  late final _ggml_abort = _ggml_abortPtr.asFunction<
      void Function(ffi.Pointer<ffi.Char>, int, ffi.Pointer<ffi.Char>)>();

  /// get ggml_status name string
  ffi.Pointer<ffi.Char> ggml_status_to_string(
    int status,
  ) {
    return _ggml_status_to_string(
      status,
    );
  }

  late final _ggml_status_to_stringPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'ggml_status_to_string');
  late final _ggml_status_to_string = _ggml_status_to_stringPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  double ggml_fp16_to_fp32(
    int arg0,
  ) {
    return _ggml_fp16_to_fp32(
      arg0,
    );
  }

  late final _ggml_fp16_to_fp32Ptr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ggml_fp16_t)>>(
          'ggml_fp16_to_fp32');
  late final _ggml_fp16_to_fp32 =
      _ggml_fp16_to_fp32Ptr.asFunction<double Function(int)>();

  int ggml_fp32_to_fp16(
    double arg0,
  ) {
    return _ggml_fp32_to_fp16(
      arg0,
    );
  }

  late final _ggml_fp32_to_fp16Ptr =
      _lookup<ffi.NativeFunction<ggml_fp16_t Function(ffi.Float)>>(
          'ggml_fp32_to_fp16');
  late final _ggml_fp32_to_fp16 =
      _ggml_fp32_to_fp16Ptr.asFunction<int Function(double)>();

  void ggml_fp16_to_fp32_row(
    ffi.Pointer<ggml_fp16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_fp16_to_fp32_row(
      arg0,
      arg1,
      arg2,
    );
  }

  late final _ggml_fp16_to_fp32_rowPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>,
              ffi.Int64)>>('ggml_fp16_to_fp32_row');
  late final _ggml_fp16_to_fp32_row = _ggml_fp16_to_fp32_rowPtr.asFunction<
      void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, int)>();

  void ggml_fp32_to_fp16_row(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_fp16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_fp16_row(
      arg0,
      arg1,
      arg2,
    );
  }

  late final _ggml_fp32_to_fp16_rowPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>,
              ffi.Int64)>>('ggml_fp32_to_fp16_row');
  late final _ggml_fp32_to_fp16_row = _ggml_fp32_to_fp16_rowPtr.asFunction<
      void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, int)>();

  ggml_bf16_t ggml_fp32_to_bf16(
    double arg0,
  ) {
    return _ggml_fp32_to_bf16(
      arg0,
    );
  }

  late final _ggml_fp32_to_bf16Ptr =
      _lookup<ffi.NativeFunction<ggml_bf16_t Function(ffi.Float)>>(
          'ggml_fp32_to_bf16');
  late final _ggml_fp32_to_bf16 =
      _ggml_fp32_to_bf16Ptr.asFunction<ggml_bf16_t Function(double)>();

  double ggml_bf16_to_fp32(
    ggml_bf16_t arg0,
  ) {
    return _ggml_bf16_to_fp32(
      arg0,
    );
  }

  late final _ggml_bf16_to_fp32Ptr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ggml_bf16_t)>>(
          'ggml_bf16_to_fp32');
  late final _ggml_bf16_to_fp32 =
      _ggml_bf16_to_fp32Ptr.asFunction<double Function(ggml_bf16_t)>();

  void ggml_bf16_to_fp32_row(
    ffi.Pointer<ggml_bf16_t> arg0,
    ffi.Pointer<ffi.Float> arg1,
    int arg2,
  ) {
    return _ggml_bf16_to_fp32_row(
      arg0,
      arg1,
      arg2,
    );
  }

  late final _ggml_bf16_to_fp32_rowPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>,
              ffi.Int64)>>('ggml_bf16_to_fp32_row');
  late final _ggml_bf16_to_fp32_row = _ggml_bf16_to_fp32_rowPtr.asFunction<
      void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, int)>();

  void ggml_fp32_to_bf16_row_ref(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_bf16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_bf16_row_ref(
      arg0,
      arg1,
      arg2,
    );
  }

  late final _ggml_fp32_to_bf16_row_refPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>,
              ffi.Int64)>>('ggml_fp32_to_bf16_row_ref');
  late final _ggml_fp32_to_bf16_row_ref =
      _ggml_fp32_to_bf16_row_refPtr.asFunction<
          void Function(
              ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, int)>();

  void ggml_fp32_to_bf16_row(
    ffi.Pointer<ffi.Float> arg0,
    ffi.Pointer<ggml_bf16_t> arg1,
    int arg2,
  ) {
    return _ggml_fp32_to_bf16_row(
      arg0,
      arg1,
      arg2,
    );
  }

  late final _ggml_fp32_to_bf16_rowPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>,
              ffi.Int64)>>('ggml_fp32_to_bf16_row');
  late final _ggml_fp32_to_bf16_row = _ggml_fp32_to_bf16_rowPtr.asFunction<
      void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, int)>();

  late final ffi.Pointer<ffi.Size> _GGML_TENSOR_SIZE =
      _lookup<ffi.Size>('GGML_TENSOR_SIZE');

  int get GGML_TENSOR_SIZE => _GGML_TENSOR_SIZE.value;

  bool ggml_guid_matches(
    ggml_guid_t guid_a,
    ggml_guid_t guid_b,
  ) {
    return _ggml_guid_matches(
      guid_a,
      guid_b,
    );
  }

  late final _ggml_guid_matchesPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_guid_t, ggml_guid_t)>>(
          'ggml_guid_matches');
  late final _ggml_guid_matches = _ggml_guid_matchesPtr
      .asFunction<bool Function(ggml_guid_t, ggml_guid_t)>();

  /// misc
  void ggml_time_init() {
    return _ggml_time_init();
  }

  late final _ggml_time_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_time_init');
  late final _ggml_time_init = _ggml_time_initPtr.asFunction<void Function()>();

  int ggml_time_ms() {
    return _ggml_time_ms();
  }

  late final _ggml_time_msPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_time_ms');
  late final _ggml_time_ms = _ggml_time_msPtr.asFunction<int Function()>();

  int ggml_time_us() {
    return _ggml_time_us();
  }

  late final _ggml_time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_time_us');
  late final _ggml_time_us = _ggml_time_usPtr.asFunction<int Function()>();

  int ggml_cycles() {
    return _ggml_cycles();
  }

  late final _ggml_cyclesPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_cycles');
  late final _ggml_cycles = _ggml_cyclesPtr.asFunction<int Function()>();

  int ggml_cycles_per_ms() {
    return _ggml_cycles_per_ms();
  }

  late final _ggml_cycles_per_msPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('ggml_cycles_per_ms');
  late final _ggml_cycles_per_ms =
      _ggml_cycles_per_msPtr.asFunction<int Function()>();

  /// accepts a UTF-8 path, even on Windows
  ffi.Pointer<FILE> ggml_fopen(
    ffi.Pointer<ffi.Char> fname,
    ffi.Pointer<ffi.Char> mode,
  ) {
    return _ggml_fopen(
      fname,
      mode,
    );
  }

  late final _ggml_fopenPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<FILE> Function(
              ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>>('ggml_fopen');
  late final _ggml_fopen = _ggml_fopenPtr.asFunction<
      ffi.Pointer<FILE> Function(
          ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>();

  void ggml_print_object(
    ffi.Pointer<ggml_object> obj,
  ) {
    return _ggml_print_object(
      obj,
    );
  }

  late final _ggml_print_objectPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_object>)>>(
          'ggml_print_object');
  late final _ggml_print_object = _ggml_print_objectPtr
      .asFunction<void Function(ffi.Pointer<ggml_object>)>();

  void ggml_print_objects(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_print_objects(
      ctx,
    );
  }

  late final _ggml_print_objectsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
          'ggml_print_objects');
  late final _ggml_print_objects = _ggml_print_objectsPtr
      .asFunction<void Function(ffi.Pointer<ggml_context>)>();

  int ggml_nelements(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_nelements(
      tensor,
    );
  }

  late final _ggml_nelementsPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_nelements');
  late final _ggml_nelements =
      _ggml_nelementsPtr.asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nrows(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_nrows(
      tensor,
    );
  }

  late final _ggml_nrowsPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_nrows');
  late final _ggml_nrows =
      _ggml_nrowsPtr.asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nbytes(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_nbytes(
      tensor,
    );
  }

  late final _ggml_nbytesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_nbytes');
  late final _ggml_nbytes =
      _ggml_nbytesPtr.asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_nbytes_pad(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_nbytes_pad(
      tensor,
    );
  }

  late final _ggml_nbytes_padPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_nbytes_pad');
  late final _ggml_nbytes_pad =
      _ggml_nbytes_padPtr.asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_blck_size(
    int type,
  ) {
    return _ggml_blck_size(
      type,
    );
  }

  late final _ggml_blck_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function(ffi.Int32)>>(
          'ggml_blck_size');
  late final _ggml_blck_size =
      _ggml_blck_sizePtr.asFunction<int Function(int)>();

  int ggml_type_size(
    int type,
  ) {
    return _ggml_type_size(
      type,
    );
  }

  late final _ggml_type_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Int32)>>(
          'ggml_type_size');
  late final _ggml_type_size =
      _ggml_type_sizePtr.asFunction<int Function(int)>();

  int ggml_row_size(
    int type,
    int ne,
  ) {
    return _ggml_row_size(
      type,
      ne,
    );
  }

  late final _ggml_row_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Int32, ffi.Int64)>>(
          'ggml_row_size');
  late final _ggml_row_size =
      _ggml_row_sizePtr.asFunction<int Function(int, int)>();

  double ggml_type_sizef(
    int type,
  ) {
    return _ggml_type_sizef(
      type,
    );
  }

  late final _ggml_type_sizefPtr =
      _lookup<ffi.NativeFunction<ffi.Double Function(ffi.Int32)>>(
          'ggml_type_sizef');
  late final _ggml_type_sizef =
      _ggml_type_sizefPtr.asFunction<double Function(int)>();

  ffi.Pointer<ffi.Char> ggml_type_name(
    int type,
  ) {
    return _ggml_type_name(
      type,
    );
  }

  late final _ggml_type_namePtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'ggml_type_name');
  late final _ggml_type_name =
      _ggml_type_namePtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_name(
    int op,
  ) {
    return _ggml_op_name(
      op,
    );
  }

  late final _ggml_op_namePtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'ggml_op_name');
  late final _ggml_op_name =
      _ggml_op_namePtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_symbol(
    int op,
  ) {
    return _ggml_op_symbol(
      op,
    );
  }

  late final _ggml_op_symbolPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'ggml_op_symbol');
  late final _ggml_op_symbol =
      _ggml_op_symbolPtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_unary_op_name(
    int op,
  ) {
    return _ggml_unary_op_name(
      op,
    );
  }

  late final _ggml_unary_op_namePtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'ggml_unary_op_name');
  late final _ggml_unary_op_name =
      _ggml_unary_op_namePtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  ffi.Pointer<ffi.Char> ggml_op_desc(
    ffi.Pointer<ggml_tensor> t,
  ) {
    return _ggml_op_desc(
      t,
    );
  }

  late final _ggml_op_descPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<ggml_tensor>)>>('ggml_op_desc');
  late final _ggml_op_desc = _ggml_op_descPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_element_size(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_element_size(
      tensor,
    );
  }

  late final _ggml_element_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_element_size');
  late final _ggml_element_size = _ggml_element_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_quantized(
    int type,
  ) {
    return _ggml_is_quantized(
      type,
    );
  }

  late final _ggml_is_quantizedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Int32)>>(
          'ggml_is_quantized');
  late final _ggml_is_quantized =
      _ggml_is_quantizedPtr.asFunction<bool Function(int)>();

  /// TODO: temporary until model loading of ggml examples is refactored
  int ggml_ftype_to_ggml_type(
    int ftype,
  ) {
    return _ggml_ftype_to_ggml_type(
      ftype,
    );
  }

  late final _ggml_ftype_to_ggml_typePtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Int32)>>(
          'ggml_ftype_to_ggml_type');
  late final _ggml_ftype_to_ggml_type =
      _ggml_ftype_to_ggml_typePtr.asFunction<int Function(int)>();

  bool ggml_is_transposed(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_transposed(
      tensor,
    );
  }

  late final _ggml_is_transposedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_transposed');
  late final _ggml_is_transposed = _ggml_is_transposedPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_permuted(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_permuted(
      tensor,
    );
  }

  late final _ggml_is_permutedPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_permuted');
  late final _ggml_is_permuted = _ggml_is_permutedPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_empty(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_empty(
      tensor,
    );
  }

  late final _ggml_is_emptyPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_empty');
  late final _ggml_is_empty =
      _ggml_is_emptyPtr.asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_scalar(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_scalar(
      tensor,
    );
  }

  late final _ggml_is_scalarPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_scalar');
  late final _ggml_is_scalar =
      _ggml_is_scalarPtr.asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_vector(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_vector(
      tensor,
    );
  }

  late final _ggml_is_vectorPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_vector');
  late final _ggml_is_vector =
      _ggml_is_vectorPtr.asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_matrix(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_matrix(
      tensor,
    );
  }

  late final _ggml_is_matrixPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_matrix');
  late final _ggml_is_matrix =
      _ggml_is_matrixPtr.asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_3d(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_3d(
      tensor,
    );
  }

  late final _ggml_is_3dPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_3d');
  late final _ggml_is_3d =
      _ggml_is_3dPtr.asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  int ggml_n_dims(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_n_dims(
      tensor,
    );
  }

  late final _ggml_n_dimsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_n_dims');
  late final _ggml_n_dims =
      _ggml_n_dimsPtr.asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_contiguous(
      tensor,
    );
  }

  late final _ggml_is_contiguousPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_contiguous');
  late final _ggml_is_contiguous = _ggml_is_contiguousPtr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_0(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_contiguous_0(
      tensor,
    );
  }

  late final _ggml_is_contiguous_0Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_contiguous_0');
  late final _ggml_is_contiguous_0 = _ggml_is_contiguous_0Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_1(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_contiguous_1(
      tensor,
    );
  }

  late final _ggml_is_contiguous_1Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_contiguous_1');
  late final _ggml_is_contiguous_1 = _ggml_is_contiguous_1Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_is_contiguous_2(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_is_contiguous_2(
      tensor,
    );
  }

  late final _ggml_is_contiguous_2Ptr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_is_contiguous_2');
  late final _ggml_is_contiguous_2 = _ggml_is_contiguous_2Ptr
      .asFunction<bool Function(ffi.Pointer<ggml_tensor>)>();

  bool ggml_are_same_shape(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_are_same_shape(
      t0,
      t1,
    );
  }

  late final _ggml_are_same_shapePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_are_same_shape');
  late final _ggml_are_same_shape = _ggml_are_same_shapePtr.asFunction<
      bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  bool ggml_are_same_stride(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_are_same_stride(
      t0,
      t1,
    );
  }

  late final _ggml_are_same_stridePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_are_same_stride');
  late final _ggml_are_same_stride = _ggml_are_same_stridePtr.asFunction<
      bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  bool ggml_can_repeat(
    ffi.Pointer<ggml_tensor> t0,
    ffi.Pointer<ggml_tensor> t1,
  ) {
    return _ggml_can_repeat(
      t0,
      t1,
    );
  }

  late final _ggml_can_repeatPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_can_repeat');
  late final _ggml_can_repeat = _ggml_can_repeatPtr.asFunction<
      bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// use this to compute the memory overhead of a tensor
  int ggml_tensor_overhead() {
    return _ggml_tensor_overhead();
  }

  late final _ggml_tensor_overheadPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('ggml_tensor_overhead');
  late final _ggml_tensor_overhead =
      _ggml_tensor_overheadPtr.asFunction<int Function()>();

  bool ggml_validate_row_data(
    int type,
    ffi.Pointer<ffi.Void> data,
    int nbytes,
  ) {
    return _ggml_validate_row_data(
      type,
      data,
      nbytes,
    );
  }

  late final _ggml_validate_row_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Int32, ffi.Pointer<ffi.Void>,
              ffi.Size)>>('ggml_validate_row_data');
  late final _ggml_validate_row_data = _ggml_validate_row_dataPtr
      .asFunction<bool Function(int, ffi.Pointer<ffi.Void>, int)>();

  /// main
  ffi.Pointer<ggml_context> ggml_init(
    ggml_init_params params,
  ) {
    return _ggml_init(
      params,
    );
  }

  late final _ggml_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_context> Function(ggml_init_params)>>('ggml_init');
  late final _ggml_init = _ggml_initPtr
      .asFunction<ffi.Pointer<ggml_context> Function(ggml_init_params)>();

  void ggml_reset(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_reset(
      ctx,
    );
  }

  late final _ggml_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
          'ggml_reset');
  late final _ggml_reset =
      _ggml_resetPtr.asFunction<void Function(ffi.Pointer<ggml_context>)>();

  void ggml_free(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_free(
      ctx,
    );
  }

  late final _ggml_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_context>)>>(
          'ggml_free');
  late final _ggml_free =
      _ggml_freePtr.asFunction<void Function(ffi.Pointer<ggml_context>)>();

  int ggml_used_mem(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_used_mem(
      ctx,
    );
  }

  late final _ggml_used_memPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
          'ggml_used_mem');
  late final _ggml_used_mem =
      _ggml_used_memPtr.asFunction<int Function(ffi.Pointer<ggml_context>)>();

  bool ggml_get_no_alloc(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_no_alloc(
      ctx,
    );
  }

  late final _ggml_get_no_allocPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<ggml_context>)>>(
          'ggml_get_no_alloc');
  late final _ggml_get_no_alloc = _ggml_get_no_allocPtr
      .asFunction<bool Function(ffi.Pointer<ggml_context>)>();

  void ggml_set_no_alloc(
    ffi.Pointer<ggml_context> ctx,
    bool no_alloc,
  ) {
    return _ggml_set_no_alloc(
      ctx,
      no_alloc,
    );
  }

  late final _ggml_set_no_allocPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<ggml_context>, ffi.Bool)>>('ggml_set_no_alloc');
  late final _ggml_set_no_alloc = _ggml_set_no_allocPtr
      .asFunction<void Function(ffi.Pointer<ggml_context>, bool)>();

  ffi.Pointer<ffi.Void> ggml_get_mem_buffer(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_mem_buffer(
      ctx,
    );
  }

  late final _ggml_get_mem_bufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<ggml_context>)>>('ggml_get_mem_buffer');
  late final _ggml_get_mem_buffer = _ggml_get_mem_bufferPtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>)>();

  int ggml_get_mem_size(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_mem_size(
      ctx,
    );
  }

  late final _ggml_get_mem_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
          'ggml_get_mem_size');
  late final _ggml_get_mem_size = _ggml_get_mem_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_context>)>();

  int ggml_get_max_tensor_size(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_max_tensor_size(
      ctx,
    );
  }

  late final _ggml_get_max_tensor_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<ggml_context>)>>(
          'ggml_get_max_tensor_size');
  late final _ggml_get_max_tensor_size = _ggml_get_max_tensor_sizePtr
      .asFunction<int Function(ffi.Pointer<ggml_context>)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor(
    ffi.Pointer<ggml_context> ctx,
    int type,
    int n_dims,
    ffi.Pointer<ffi.Int64> ne,
  ) {
    return _ggml_new_tensor(
      ctx,
      type,
      n_dims,
      ne,
    );
  }

  late final _ggml_new_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Int32, ffi.Int, ffi.Pointer<ffi.Int64>)>>('ggml_new_tensor');
  late final _ggml_new_tensor = _ggml_new_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, int, int, ffi.Pointer<ffi.Int64>)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_1d(
    ffi.Pointer<ggml_context> ctx,
    int type,
    int ne0,
  ) {
    return _ggml_new_tensor_1d(
      ctx,
      type,
      ne0,
    );
  }

  late final _ggml_new_tensor_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Int32, ffi.Int64)>>('ggml_new_tensor_1d');
  late final _ggml_new_tensor_1d = _ggml_new_tensor_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_2d(
    ffi.Pointer<ggml_context> ctx,
    int type,
    int ne0,
    int ne1,
  ) {
    return _ggml_new_tensor_2d(
      ctx,
      type,
      ne0,
      ne1,
    );
  }

  late final _ggml_new_tensor_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Int32, ffi.Int64, ffi.Int64)>>('ggml_new_tensor_2d');
  late final _ggml_new_tensor_2d = _ggml_new_tensor_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_3d(
    ffi.Pointer<ggml_context> ctx,
    int type,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_new_tensor_3d(
      ctx,
      type,
      ne0,
      ne1,
      ne2,
    );
  }

  late final _ggml_new_tensor_3dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Int32,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_new_tensor_3d');
  late final _ggml_new_tensor_3d = _ggml_new_tensor_3dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, int, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_new_tensor_4d(
    ffi.Pointer<ggml_context> ctx,
    int type,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_new_tensor_4d(
      ctx,
      type,
      ne0,
      ne1,
      ne2,
      ne3,
    );
  }

  late final _ggml_new_tensor_4dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Int32,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_new_tensor_4d');
  late final _ggml_new_tensor_4d = _ggml_new_tensor_4dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, int, int, int, int, int)>();

  ffi.Pointer<ffi.Void> ggml_new_buffer(
    ffi.Pointer<ggml_context> ctx,
    int nbytes,
  ) {
    return _ggml_new_buffer(
      ctx,
      nbytes,
    );
  }

  late final _ggml_new_bufferPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<ggml_context>, ffi.Size)>>('ggml_new_buffer');
  late final _ggml_new_buffer = _ggml_new_bufferPtr.asFunction<
      ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>, int)>();

  ffi.Pointer<ggml_tensor> ggml_dup_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> src,
  ) {
    return _ggml_dup_tensor(
      ctx,
      src,
    );
  }

  late final _ggml_dup_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_dup_tensor');
  late final _ggml_dup_tensor = _ggml_dup_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_view_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> src,
  ) {
    return _ggml_view_tensor(
      ctx,
      src,
    );
  }

  late final _ggml_view_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_view_tensor');
  late final _ggml_view_tensor = _ggml_view_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// Context tensor enumeration and lookup
  ffi.Pointer<ggml_tensor> ggml_get_first_tensor(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_get_first_tensor(
      ctx,
    );
  }

  late final _ggml_get_first_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>)>>('ggml_get_first_tensor');
  late final _ggml_get_first_tensor = _ggml_get_first_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>)>();

  ffi.Pointer<ggml_tensor> ggml_get_next_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_next_tensor(
      ctx,
      tensor,
    );
  }

  late final _ggml_get_next_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_get_next_tensor');
  late final _ggml_get_next_tensor = _ggml_get_next_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_get_tensor(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_get_tensor(
      ctx,
      name,
    );
  }

  late final _ggml_get_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ffi.Char>)>>('ggml_get_tensor');
  late final _ggml_get_tensor = _ggml_get_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ffi.Char>)>();

  /// Converts a flat index into coordinates
  void ggml_unravel_index(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
    ffi.Pointer<ffi.Int64> i0,
    ffi.Pointer<ffi.Int64> i1,
    ffi.Pointer<ffi.Int64> i2,
    ffi.Pointer<ffi.Int64> i3,
  ) {
    return _ggml_unravel_index(
      tensor,
      i,
      i0,
      i1,
      i2,
      i3,
    );
  }

  late final _ggml_unravel_indexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Pointer<ffi.Int64>,
              ffi.Pointer<ffi.Int64>,
              ffi.Pointer<ffi.Int64>,
              ffi.Pointer<ffi.Int64>)>>('ggml_unravel_index');
  late final _ggml_unravel_index = _ggml_unravel_indexPtr.asFunction<
      void Function(
          ffi.Pointer<ggml_tensor>,
          int,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>,
          ffi.Pointer<ffi.Int64>)>();

  int ggml_get_unary_op(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_unary_op(
      tensor,
    );
  }

  late final _ggml_get_unary_opPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_get_unary_op');
  late final _ggml_get_unary_op = _ggml_get_unary_opPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Void> ggml_get_data(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_data(
      tensor,
    );
  }

  late final _ggml_get_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<ggml_tensor>)>>('ggml_get_data');
  late final _ggml_get_data = _ggml_get_dataPtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Float> ggml_get_data_f32(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_data_f32(
      tensor,
    );
  }

  late final _ggml_get_data_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<ggml_tensor>)>>('ggml_get_data_f32');
  late final _ggml_get_data_f32 = _ggml_get_data_f32Ptr
      .asFunction<ffi.Pointer<ffi.Float> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ffi.Char> ggml_get_name(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_get_name(
      tensor,
    );
  }

  late final _ggml_get_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<ggml_tensor>)>>('ggml_get_name');
  late final _ggml_get_name = _ggml_get_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_set_name(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_set_name(
      tensor,
      name,
    );
  }

  late final _ggml_set_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Char>)>>('ggml_set_name');
  late final _ggml_set_name = _ggml_set_namePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Char>)>();

  ffi.Pointer<ggml_tensor> ggml_format_name(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Char> fmt,
  ) {
    return _ggml_format_name(
      tensor,
      fmt,
    );
  }

  late final _ggml_format_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Char>)>>('ggml_format_name');
  late final _ggml_format_name = _ggml_format_namePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Char>)>();

  /// Tensor flags
  void ggml_set_input(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_set_input(
      tensor,
    );
  }

  late final _ggml_set_inputPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_set_input');
  late final _ggml_set_input =
      _ggml_set_inputPtr.asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_set_output(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_set_output(
      tensor,
    );
  }

  late final _ggml_set_outputPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_set_output');
  late final _ggml_set_output =
      _ggml_set_outputPtr.asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  void ggml_set_param(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_set_param(
      ctx,
      tensor,
    );
  }

  late final _ggml_set_paramPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_set_param');
  late final _ggml_set_param = _ggml_set_paramPtr.asFunction<
      void Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  void ggml_set_loss(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_set_loss(
      tensor,
    );
  }

  late final _ggml_set_lossPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_set_loss');
  late final _ggml_set_loss =
      _ggml_set_lossPtr.asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  /// operations on tensors with backpropagation
  ffi.Pointer<ggml_tensor> ggml_dup(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_dup(
      ctx,
      a,
    );
  }

  late final _ggml_dupPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_dup');
  late final _ggml_dup = _ggml_dupPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_dup_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_dup_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_dup_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_dup_inplace');
  late final _ggml_dup_inplace = _ggml_dup_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_add(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_addPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>>('ggml_add');
  late final _ggml_add = _ggml_addPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_add_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_add_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_add_inplace');
  late final _ggml_add_inplace = _ggml_add_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_add_cast(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int type,
  ) {
    return _ggml_add_cast(
      ctx,
      a,
      b,
      type,
    );
  }

  late final _ggml_add_castPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int32)>>('ggml_add_cast');
  late final _ggml_add_cast = _ggml_add_castPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_add1(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add1(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_add1Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_add1');
  late final _ggml_add1 = _ggml_add1Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_add1_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_add1_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_add1_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_add1_inplace');
  late final _ggml_add1_inplace = _ggml_add1_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// dst = a
  /// view(dst, nb1, nb2, nb3, offset) += b
  /// return dst
  ffi.Pointer<ggml_tensor> ggml_acc(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_acc(
      ctx,
      a,
      b,
      nb1,
      nb2,
      nb3,
      offset,
    );
  }

  late final _ggml_accPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_acc');
  late final _ggml_acc = _ggml_accPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int)>();

  ffi.Pointer<ggml_tensor> ggml_acc_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_acc_inplace(
      ctx,
      a,
      b,
      nb1,
      nb2,
      nb3,
      offset,
    );
  }

  late final _ggml_acc_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_acc_inplace');
  late final _ggml_acc_inplace = _ggml_acc_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int)>();

  ffi.Pointer<ggml_tensor> ggml_sub(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_sub(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_subPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>>('ggml_sub');
  late final _ggml_sub = _ggml_subPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sub_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_sub_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_sub_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sub_inplace');
  late final _ggml_sub_inplace = _ggml_sub_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_mul(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_mulPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>>('ggml_mul');
  late final _ggml_mul = _ggml_mulPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_mul_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_mul_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_mul_inplace');
  late final _ggml_mul_inplace = _ggml_mul_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_div(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_div(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_divPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>>('ggml_div');
  late final _ggml_div = _ggml_divPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_div_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_div_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_div_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_div_inplace');
  late final _ggml_div_inplace = _ggml_div_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sqr(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqr(
      ctx,
      a,
    );
  }

  late final _ggml_sqrPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sqr');
  late final _ggml_sqr = _ggml_sqrPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sqr_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqr_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_sqr_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sqr_inplace');
  late final _ggml_sqr_inplace = _ggml_sqr_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sqrt(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqrt(
      ctx,
      a,
    );
  }

  late final _ggml_sqrtPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sqrt');
  late final _ggml_sqrt = _ggml_sqrtPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sqrt_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sqrt_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_sqrt_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sqrt_inplace');
  late final _ggml_sqrt_inplace = _ggml_sqrt_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_log(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_log(
      ctx,
      a,
    );
  }

  late final _ggml_logPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_log');
  late final _ggml_log = _ggml_logPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_log_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_log_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_log_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_log_inplace');
  late final _ggml_log_inplace = _ggml_log_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sin(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sin(
      ctx,
      a,
    );
  }

  late final _ggml_sinPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sin');
  late final _ggml_sin = _ggml_sinPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sin_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sin_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_sin_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sin_inplace');
  late final _ggml_sin_inplace = _ggml_sin_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_cos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cos(
      ctx,
      a,
    );
  }

  late final _ggml_cosPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_cos');
  late final _ggml_cos = _ggml_cosPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_cos_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cos_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_cos_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_cos_inplace');
  late final _ggml_cos_inplace = _ggml_cos_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// return scalar
  ffi.Pointer<ggml_tensor> ggml_sum(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sum(
      ctx,
      a,
    );
  }

  late final _ggml_sumPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sum');
  late final _ggml_sum = _ggml_sumPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
  ffi.Pointer<ggml_tensor> ggml_sum_rows(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sum_rows(
      ctx,
      a,
    );
  }

  late final _ggml_sum_rowsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sum_rows');
  late final _ggml_sum_rows = _ggml_sum_rowsPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// mean along rows
  ffi.Pointer<ggml_tensor> ggml_mean(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_mean(
      ctx,
      a,
    );
  }

  late final _ggml_meanPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_mean');
  late final _ggml_mean = _ggml_meanPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// argmax along rows
  ffi.Pointer<ggml_tensor> ggml_argmax(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_argmax(
      ctx,
      a,
    );
  }

  late final _ggml_argmaxPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_argmax');
  late final _ggml_argmax = _ggml_argmaxPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// count number of equal elements in a and b
  ffi.Pointer<ggml_tensor> ggml_count_equal(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_count_equal(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_count_equalPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_count_equal');
  late final _ggml_count_equal = _ggml_count_equalPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// if a is the same shape as b, and a is not parameter, return a
  /// otherwise, return a new tensor: repeat(a) to fit in b
  ffi.Pointer<ggml_tensor> ggml_repeat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_repeat(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_repeatPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_repeat');
  late final _ggml_repeat = _ggml_repeatPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// sums repetitions in a into shape of b
  ffi.Pointer<ggml_tensor> ggml_repeat_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_repeat_back(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_repeat_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_repeat_back');
  late final _ggml_repeat_back = _ggml_repeat_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// concat a and b along dim
  /// used in stable-diffusion
  ffi.Pointer<ggml_tensor> ggml_concat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int dim,
  ) {
    return _ggml_concat(
      ctx,
      a,
      b,
      dim,
    );
  }

  late final _ggml_concatPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int)>>('ggml_concat');
  late final _ggml_concat = _ggml_concatPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_abs(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_abs(
      ctx,
      a,
    );
  }

  late final _ggml_absPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_abs');
  late final _ggml_abs = _ggml_absPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_abs_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_abs_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_abs_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_abs_inplace');
  late final _ggml_abs_inplace = _ggml_abs_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sgn(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sgn(
      ctx,
      a,
    );
  }

  late final _ggml_sgnPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sgn');
  late final _ggml_sgn = _ggml_sgnPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sgn_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sgn_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_sgn_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sgn_inplace');
  late final _ggml_sgn_inplace = _ggml_sgn_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_neg(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_neg(
      ctx,
      a,
    );
  }

  late final _ggml_negPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_neg');
  late final _ggml_neg = _ggml_negPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_neg_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_neg_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_neg_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_neg_inplace');
  late final _ggml_neg_inplace = _ggml_neg_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_step(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_step(
      ctx,
      a,
    );
  }

  late final _ggml_stepPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_step');
  late final _ggml_step = _ggml_stepPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_step_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_step_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_step_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_step_inplace');
  late final _ggml_step_inplace = _ggml_step_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_tanh(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_tanh(
      ctx,
      a,
    );
  }

  late final _ggml_tanhPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_tanh');
  late final _ggml_tanh = _ggml_tanhPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_tanh_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_tanh_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_tanh_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_tanh_inplace');
  late final _ggml_tanh_inplace = _ggml_tanh_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_elu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_elu(
      ctx,
      a,
    );
  }

  late final _ggml_eluPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_elu');
  late final _ggml_elu = _ggml_eluPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_elu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_elu_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_elu_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_elu_inplace');
  late final _ggml_elu_inplace = _ggml_elu_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_relu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_relu(
      ctx,
      a,
    );
  }

  late final _ggml_reluPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_relu');
  late final _ggml_relu = _ggml_reluPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_leaky_relu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double negative_slope,
    bool inplace,
  ) {
    return _ggml_leaky_relu(
      ctx,
      a,
      negative_slope,
      inplace,
    );
  }

  late final _ggml_leaky_reluPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Float,
              ffi.Bool)>>('ggml_leaky_relu');
  late final _ggml_leaky_relu = _ggml_leaky_reluPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double, bool)>();

  ffi.Pointer<ggml_tensor> ggml_relu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_relu_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_relu_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_relu_inplace');
  late final _ggml_relu_inplace = _ggml_relu_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sigmoid(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sigmoid(
      ctx,
      a,
    );
  }

  late final _ggml_sigmoidPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sigmoid');
  late final _ggml_sigmoid = _ggml_sigmoidPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_sigmoid_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_sigmoid_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_sigmoid_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_sigmoid_inplace');
  late final _ggml_sigmoid_inplace = _ggml_sigmoid_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_gelu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu(
      ctx,
      a,
    );
  }

  late final _ggml_geluPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_gelu');
  late final _ggml_gelu = _ggml_geluPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_gelu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_gelu_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_gelu_inplace');
  late final _ggml_gelu_inplace = _ggml_gelu_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_gelu_quick(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_quick(
      ctx,
      a,
    );
  }

  late final _ggml_gelu_quickPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_gelu_quick');
  late final _ggml_gelu_quick = _ggml_gelu_quickPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_gelu_quick_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_gelu_quick_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_gelu_quick_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_gelu_quick_inplace');
  late final _ggml_gelu_quick_inplace = _ggml_gelu_quick_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_silu(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_silu(
      ctx,
      a,
    );
  }

  late final _ggml_siluPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_silu');
  late final _ggml_silu = _ggml_siluPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_silu_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_silu_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_silu_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_silu_inplace');
  late final _ggml_silu_inplace = _ggml_silu_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// a - x
  /// b - dy
  ffi.Pointer<ggml_tensor> ggml_silu_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_silu_back(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_silu_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_silu_back');
  late final _ggml_silu_back = _ggml_silu_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// hardswish(x) = x * relu6(x + 3) / 6
  ffi.Pointer<ggml_tensor> ggml_hardswish(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_hardswish(
      ctx,
      a,
    );
  }

  late final _ggml_hardswishPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_hardswish');
  late final _ggml_hardswish = _ggml_hardswishPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// hardsigmoid(x) = relu6(x + 3) / 6
  ffi.Pointer<ggml_tensor> ggml_hardsigmoid(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_hardsigmoid(
      ctx,
      a,
    );
  }

  late final _ggml_hardsigmoidPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_hardsigmoid');
  late final _ggml_hardsigmoid = _ggml_hardsigmoidPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_exp(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_exp(
      ctx,
      a,
    );
  }

  late final _ggml_expPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_exp');
  late final _ggml_exp = _ggml_expPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_exp_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_exp_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_exp_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_exp_inplace');
  late final _ggml_exp_inplace = _ggml_exp_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// normalize along rows
  ffi.Pointer<ggml_tensor> ggml_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_norm(
      ctx,
      a,
      eps,
    );
  }

  late final _ggml_normPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_norm');
  late final _ggml_norm = _ggml_normPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  ffi.Pointer<ggml_tensor> ggml_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_norm_inplace(
      ctx,
      a,
      eps,
    );
  }

  late final _ggml_norm_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_norm_inplace');
  late final _ggml_norm_inplace = _ggml_norm_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  ffi.Pointer<ggml_tensor> ggml_rms_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_rms_norm(
      ctx,
      a,
      eps,
    );
  }

  late final _ggml_rms_normPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_rms_norm');
  late final _ggml_rms_norm = _ggml_rms_normPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  ffi.Pointer<ggml_tensor> ggml_rms_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double eps,
  ) {
    return _ggml_rms_norm_inplace(
      ctx,
      a,
      eps,
    );
  }

  late final _ggml_rms_norm_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_rms_norm_inplace');
  late final _ggml_rms_norm_inplace = _ggml_rms_norm_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  /// group normalize along ne0*ne1*n_groups
  /// used in stable-diffusion
  ffi.Pointer<ggml_tensor> ggml_group_norm(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_groups,
    double eps,
  ) {
    return _ggml_group_norm(
      ctx,
      a,
      n_groups,
      eps,
    );
  }

  late final _ggml_group_normPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Float)>>('ggml_group_norm');
  late final _ggml_group_norm = _ggml_group_normPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, double)>();

  ffi.Pointer<ggml_tensor> ggml_group_norm_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_groups,
    double eps,
  ) {
    return _ggml_group_norm_inplace(
      ctx,
      a,
      n_groups,
      eps,
    );
  }

  late final _ggml_group_norm_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Float)>>('ggml_group_norm_inplace');
  late final _ggml_group_norm_inplace = _ggml_group_norm_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, double)>();

  /// a - x
  /// b - dy
  ffi.Pointer<ggml_tensor> ggml_rms_norm_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    double eps,
  ) {
    return _ggml_rms_norm_back(
      ctx,
      a,
      b,
      eps,
    );
  }

  late final _ggml_rms_norm_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Float)>>('ggml_rms_norm_back');
  late final _ggml_rms_norm_back = _ggml_rms_norm_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, double)>();

  /// A: k columns, n rows => [ne03, ne02, n, k]
  /// B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
  /// result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
  ffi.Pointer<ggml_tensor> ggml_mul_mat(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_mul_mat(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_mul_matPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_mul_mat');
  late final _ggml_mul_mat = _ggml_mul_matPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// change the precision of a matrix multiplication
  /// set to GGML_PREC_F32 for higher precision (useful for phi-2)
  void ggml_mul_mat_set_prec(
    ffi.Pointer<ggml_tensor> a,
    int prec,
  ) {
    return _ggml_mul_mat_set_prec(
      a,
      prec,
    );
  }

  late final _ggml_mul_mat_set_precPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_mul_mat_set_prec');
  late final _ggml_mul_mat_set_prec = _ggml_mul_mat_set_precPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int)>();

  /// indirect matrix multiplication
  ffi.Pointer<ggml_tensor> ggml_mul_mat_id(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> as1,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> ids,
  ) {
    return _ggml_mul_mat_id(
      ctx,
      as1,
      b,
      ids,
    );
  }

  late final _ggml_mul_mat_idPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_mul_mat_id');
  late final _ggml_mul_mat_id = _ggml_mul_mat_idPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  /// A: m columns, n rows,
  /// B: p columns, n rows,
  /// result is m columns, p rows
  ffi.Pointer<ggml_tensor> ggml_out_prod(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_out_prod(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_out_prodPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_out_prod');
  late final _ggml_out_prod = _ggml_out_prodPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// operations on tensors without backpropagation
  ffi.Pointer<ggml_tensor> ggml_scale(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
  ) {
    return _ggml_scale(
      ctx,
      a,
      s,
    );
  }

  late final _ggml_scalePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_scale');
  late final _ggml_scale = _ggml_scalePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_scale_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double s,
  ) {
    return _ggml_scale_inplace(
      ctx,
      a,
      s,
    );
  }

  late final _ggml_scale_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_scale_inplace');
  late final _ggml_scale_inplace = _ggml_scale_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, double)>();

  /// b -> view(a,offset,nb1,nb2,3), return modified a
  ffi.Pointer<ggml_tensor> ggml_set(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_set(
      ctx,
      a,
      b,
      nb1,
      nb2,
      nb3,
      offset,
    );
  }

  late final _ggml_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_set');
  late final _ggml_set = _ggml_setPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int)>();

  /// b -> view(a,offset,nb1,nb2,3), return view(a)
  ffi.Pointer<ggml_tensor> ggml_set_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_set_inplace(
      ctx,
      a,
      b,
      nb1,
      nb2,
      nb3,
      offset,
    );
  }

  late final _ggml_set_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_set_inplace');
  late final _ggml_set_inplace = _ggml_set_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int)>();

  ffi.Pointer<ggml_tensor> ggml_set_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int offset,
  ) {
    return _ggml_set_1d(
      ctx,
      a,
      b,
      offset,
    );
  }

  late final _ggml_set_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size)>>('ggml_set_1d');
  late final _ggml_set_1d = _ggml_set_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_set_1d_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int offset,
  ) {
    return _ggml_set_1d_inplace(
      ctx,
      a,
      b,
      offset,
    );
  }

  late final _ggml_set_1d_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size)>>('ggml_set_1d_inplace');
  late final _ggml_set_1d_inplace = _ggml_set_1d_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int)>();

  /// b -> view(a,offset,nb1,nb2,3), return modified a
  ffi.Pointer<ggml_tensor> ggml_set_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int offset,
  ) {
    return _ggml_set_2d(
      ctx,
      a,
      b,
      nb1,
      offset,
    );
  }

  late final _ggml_set_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size)>>('ggml_set_2d');
  late final _ggml_set_2d = _ggml_set_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// b -> view(a,offset,nb1,nb2,3), return view(a)
  ffi.Pointer<ggml_tensor> ggml_set_2d_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int nb1,
    int offset,
  ) {
    return _ggml_set_2d_inplace(
      ctx,
      a,
      b,
      nb1,
      offset,
    );
  }

  late final _ggml_set_2d_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Size,
              ffi.Size)>>('ggml_set_2d_inplace');
  late final _ggml_set_2d_inplace = _ggml_set_2d_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// a -> b, return view(b)
  ffi.Pointer<ggml_tensor> ggml_cpy(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_cpy(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_cpyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>>('ggml_cpy');
  late final _ggml_cpy = _ggml_cpyPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_cast(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int type,
  ) {
    return _ggml_cast(
      ctx,
      a,
      type,
    );
  }

  late final _ggml_castPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_cast');
  late final _ggml_cast = _ggml_castPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// make contiguous
  ffi.Pointer<ggml_tensor> ggml_cont(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_cont(
      ctx,
      a,
    );
  }

  late final _ggml_contPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_cont');
  late final _ggml_cont = _ggml_contPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// make contiguous, with new shape
  ffi.Pointer<ggml_tensor> ggml_cont_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
  ) {
    return _ggml_cont_1d(
      ctx,
      a,
      ne0,
    );
  }

  late final _ggml_cont_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int64)>>('ggml_cont_1d');
  late final _ggml_cont_1d = _ggml_cont_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_cont_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
  ) {
    return _ggml_cont_2d(
      ctx,
      a,
      ne0,
      ne1,
    );
  }

  late final _ggml_cont_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Int64)>>('ggml_cont_2d');
  late final _ggml_cont_2d = _ggml_cont_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_cont_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_cont_3d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
    );
  }

  late final _ggml_cont_3dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_cont_3d');
  late final _ggml_cont_3d = _ggml_cont_3dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_cont_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_cont_4d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
      ne3,
    );
  }

  late final _ggml_cont_4dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_cont_4d');
  late final _ggml_cont_4d = _ggml_cont_4dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// return view(a), b specifies the new shape
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_reshape(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_reshapePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_reshape');
  late final _ggml_reshape = _ggml_reshapePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// return view(a)
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
  ) {
    return _ggml_reshape_1d(
      ctx,
      a,
      ne0,
    );
  }

  late final _ggml_reshape_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int64)>>('ggml_reshape_1d');
  late final _ggml_reshape_1d = _ggml_reshape_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_reshape_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
  ) {
    return _ggml_reshape_2d(
      ctx,
      a,
      ne0,
      ne1,
    );
  }

  late final _ggml_reshape_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64)>>('ggml_reshape_2d');
  late final _ggml_reshape_2d = _ggml_reshape_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// return view(a)
  /// TODO: when we start computing gradient, make a copy instead of view
  ffi.Pointer<ggml_tensor> ggml_reshape_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
  ) {
    return _ggml_reshape_3d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
    );
  }

  late final _ggml_reshape_3dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_reshape_3d');
  late final _ggml_reshape_3d = _ggml_reshape_3dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_reshape_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_reshape_4d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
      ne3,
    );
  }

  late final _ggml_reshape_4dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64)>>('ggml_reshape_4d');
  late final _ggml_reshape_4d = _ggml_reshape_4dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// offset in bytes
  ffi.Pointer<ggml_tensor> ggml_view_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int offset,
  ) {
    return _ggml_view_1d(
      ctx,
      a,
      ne0,
      offset,
    );
  }

  late final _ggml_view_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int64, ffi.Size)>>('ggml_view_1d');
  late final _ggml_view_1d = _ggml_view_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_view_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int nb1,
    int offset,
  ) {
    return _ggml_view_2d(
      ctx,
      a,
      ne0,
      ne1,
      nb1,
      offset,
    );
  }

  late final _ggml_view_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Size,
              ffi.Size)>>('ggml_view_2d');
  late final _ggml_view_2d = _ggml_view_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_view_3d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int nb1,
    int nb2,
    int offset,
  ) {
    return _ggml_view_3d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
      nb1,
      nb2,
      offset,
    );
  }

  late final _ggml_view_3dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_view_3d');
  late final _ggml_view_3d = _ggml_view_3dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_view_4d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
    int nb1,
    int nb2,
    int nb3,
    int offset,
  ) {
    return _ggml_view_4d(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
      ne3,
      nb1,
      nb2,
      nb3,
      offset,
    );
  }

  late final _ggml_view_4dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Size,
              ffi.Size,
              ffi.Size,
              ffi.Size)>>('ggml_view_4d');
  late final _ggml_view_4d = _ggml_view_4dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int, int, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_permute(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int axis0,
    int axis1,
    int axis2,
    int axis3,
  ) {
    return _ggml_permute(
      ctx,
      a,
      axis0,
      axis1,
      axis2,
      axis3,
    );
  }

  late final _ggml_permutePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_permute');
  late final _ggml_permute = _ggml_permutePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// alias for ggml_permute(ctx, a, 1, 0, 2, 3)
  ffi.Pointer<ggml_tensor> ggml_transpose(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_transpose(
      ctx,
      a,
    );
  }

  late final _ggml_transposePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_transpose');
  late final _ggml_transpose = _ggml_transposePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// supports 3D: a->ne[2] == b->ne[1]
  ffi.Pointer<ggml_tensor> ggml_get_rows(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_get_rows(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_get_rowsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_get_rows');
  late final _ggml_get_rows = _ggml_get_rowsPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_get_rows_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_get_rows_back(
      ctx,
      a,
      b,
      c,
    );
  }

  late final _ggml_get_rows_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_get_rows_back');
  late final _ggml_get_rows_back = _ggml_get_rows_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_diag(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_diag(
      ctx,
      a,
    );
  }

  late final _ggml_diagPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_diag');
  late final _ggml_diag = _ggml_diagPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// set elements above the diagonal to -INF
  ffi.Pointer<ggml_tensor> ggml_diag_mask_inf(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_inf(
      ctx,
      a,
      n_past,
    );
  }

  late final _ggml_diag_mask_infPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_diag_mask_inf');
  late final _ggml_diag_mask_inf = _ggml_diag_mask_infPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_diag_mask_inf_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_inf_inplace(
      ctx,
      a,
      n_past,
    );
  }

  late final _ggml_diag_mask_inf_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int)>>('ggml_diag_mask_inf_inplace');
  late final _ggml_diag_mask_inf_inplace =
      _ggml_diag_mask_inf_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// set elements above the diagonal to 0
  ffi.Pointer<ggml_tensor> ggml_diag_mask_zero(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_zero(
      ctx,
      a,
      n_past,
    );
  }

  late final _ggml_diag_mask_zeroPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_diag_mask_zero');
  late final _ggml_diag_mask_zero = _ggml_diag_mask_zeroPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_diag_mask_zero_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int n_past,
  ) {
    return _ggml_diag_mask_zero_inplace(
      ctx,
      a,
      n_past,
    );
  }

  late final _ggml_diag_mask_zero_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int)>>('ggml_diag_mask_zero_inplace');
  late final _ggml_diag_mask_zero_inplace =
      _ggml_diag_mask_zero_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_soft_max(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_soft_max(
      ctx,
      a,
    );
  }

  late final _ggml_soft_maxPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_soft_max');
  late final _ggml_soft_max = _ggml_soft_maxPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_soft_max_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_soft_max_inplace(
      ctx,
      a,
    );
  }

  late final _ggml_soft_max_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>)>>('ggml_soft_max_inplace');
  late final _ggml_soft_max_inplace = _ggml_soft_max_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>)>();

  /// fused soft_max(a*scale + mask*(ALiBi slope))
  /// mask is optional
  /// max_bias = 0.0f for no ALiBi
  ffi.Pointer<ggml_tensor> ggml_soft_max_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> mask,
    double scale,
    double max_bias,
  ) {
    return _ggml_soft_max_ext(
      ctx,
      a,
      mask,
      scale,
      max_bias,
    );
  }

  late final _ggml_soft_max_extPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Float,
              ffi.Float)>>('ggml_soft_max_ext');
  late final _ggml_soft_max_ext = _ggml_soft_max_extPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double)>();

  ffi.Pointer<ggml_tensor> ggml_soft_max_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_soft_max_back(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_soft_max_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_soft_max_back');
  late final _ggml_soft_max_back = _ggml_soft_max_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_soft_max_back_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_soft_max_back_inplace(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_soft_max_back_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_soft_max_back_inplace');
  late final _ggml_soft_max_back_inplace =
      _ggml_soft_max_back_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// rotary position embedding
  /// if (mode & 1) - skip n_past elements (NOT SUPPORTED)
  /// if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
  ///
  /// b is an int32 vector with size a->ne[2], it contains the positions
  ffi.Pointer<ggml_tensor> ggml_rope(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
  ) {
    return _ggml_rope(
      ctx,
      a,
      b,
      n_dims,
      mode,
    );
  }

  late final _ggml_ropePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int)>>('ggml_rope');
  late final _ggml_rope = _ggml_ropePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_rope_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
  ) {
    return _ggml_rope_inplace(
      ctx,
      a,
      b,
      n_dims,
      mode,
    );
  }

  late final _ggml_rope_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int)>>('ggml_rope_inplace');
  late final _ggml_rope_inplace = _ggml_rope_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// custom RoPE
  /// c is freq factors (e.g. phi3-128k), (optional)
  ffi.Pointer<ggml_tensor> ggml_rope_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_ext(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_extPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_rope_ext');
  late final _ggml_rope_ext = _ggml_rope_extPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double)>();

  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_rope_ext_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_ext_inplace(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_ext_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_rope_ext_inplace');
  late final _ggml_rope_ext_inplace = _ggml_rope_ext_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double)>();

  ffi.Pointer<ggml_tensor> ggml_rope_custom(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_custom(
      ctx,
      a,
      b,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_customPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_rope_custom');
  late final _ggml_rope_custom = _ggml_rope_customPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double)>();

  ffi.Pointer<ggml_tensor> ggml_rope_custom_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_custom_inplace(
      ctx,
      a,
      b,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_custom_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_rope_custom_inplace');
  late final _ggml_rope_custom_inplace =
      _ggml_rope_custom_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              int,
              int,
              int,
              double,
              double,
              double,
              double,
              double,
              double)>();

  /// compute correction dims for YaRN RoPE scaling
  void ggml_rope_yarn_corr_dims(
    int n_dims,
    int n_ctx_orig,
    double freq_base,
    double beta_fast,
    double beta_slow,
    ffi.Pointer<ffi.Float> dims,
  ) {
    return _ggml_rope_yarn_corr_dims(
      n_dims,
      n_ctx_orig,
      freq_base,
      beta_fast,
      beta_slow,
      dims,
    );
  }

  late final _ggml_rope_yarn_corr_dimsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Int, ffi.Int, ffi.Float, ffi.Float, ffi.Float,
              ffi.Pointer<ffi.Float>)>>('ggml_rope_yarn_corr_dims');
  late final _ggml_rope_yarn_corr_dims =
      _ggml_rope_yarn_corr_dimsPtr.asFunction<
          void Function(
              int, int, double, double, double, ffi.Pointer<ffi.Float>)>();

  /// rotary position embedding backward, i.e compute dx from dy
  /// a - dy
  ffi.Pointer<ggml_tensor> ggml_rope_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int n_dims,
    int mode,
    int n_ctx_orig,
    double freq_base,
    double freq_scale,
    double ext_factor,
    double attn_factor,
    double beta_fast,
    double beta_slow,
  ) {
    return _ggml_rope_back(
      ctx,
      a,
      b,
      c,
      n_dims,
      mode,
      n_ctx_orig,
      freq_base,
      freq_scale,
      ext_factor,
      attn_factor,
      beta_fast,
      beta_slow,
    );
  }

  late final _ggml_rope_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_rope_back');
  late final _ggml_rope_back = _ggml_rope_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          double,
          double,
          double,
          double,
          double,
          double)>();

  /// clamp
  /// in-place, returns view(a)
  ffi.Pointer<ggml_tensor> ggml_clamp(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    double min,
    double max,
  ) {
    return _ggml_clamp(
      ctx,
      a,
      min,
      max,
    );
  }

  late final _ggml_clampPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Float, ffi.Float)>>('ggml_clamp');
  late final _ggml_clamp = _ggml_clampPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, double, double)>();

  /// im2col
  /// converts data into a format that effectively results in a convolution when combined with matrix multiplication
  ffi.Pointer<ggml_tensor> ggml_im2col(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
    bool is_2D,
    int dst_type,
  ) {
    return _ggml_im2col(
      ctx,
      a,
      b,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
      is_2D,
      dst_type,
    );
  }

  late final _ggml_im2colPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Bool,
              ffi.Int32)>>('ggml_im2col');
  late final _ggml_im2col = _ggml_im2colPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int,
          bool,
          int)>();

  ffi.Pointer<ggml_tensor> ggml_im2col_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ffi.Int64> ne,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
    bool is_2D,
  ) {
    return _ggml_im2col_back(
      ctx,
      a,
      b,
      ne,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
      is_2D,
    );
  }

  late final _ggml_im2col_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Int64>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Bool)>>('ggml_im2col_back');
  late final _ggml_im2col_back = _ggml_im2col_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ffi.Int64>,
          int,
          int,
          int,
          int,
          int,
          int,
          bool)>();

  ffi.Pointer<ggml_tensor> ggml_conv_depthwise_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
  ) {
    return _ggml_conv_depthwise_2d(
      ctx,
      a,
      b,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
    );
  }

  late final _ggml_conv_depthwise_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_conv_depthwise_2d');
  late final _ggml_conv_depthwise_2d = _ggml_conv_depthwise_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int)>();

  ffi.Pointer<ggml_tensor> ggml_conv_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int p0,
    int d0,
  ) {
    return _ggml_conv_1d(
      ctx,
      a,
      b,
      s0,
      p0,
      d0,
    );
  }

  late final _ggml_conv_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_conv_1d');
  late final _ggml_conv_1d = _ggml_conv_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int, int)>();

  /// conv_1d with padding = half
  /// alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
  ffi.Pointer<ggml_tensor> ggml_conv_1d_ph(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s,
    int d,
  ) {
    return _ggml_conv_1d_ph(
      ctx,
      a,
      b,
      s,
      d,
    );
  }

  late final _ggml_conv_1d_phPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int)>>('ggml_conv_1d_ph');
  late final _ggml_conv_1d_ph = _ggml_conv_1d_phPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_conv_transpose_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int p0,
    int d0,
  ) {
    return _ggml_conv_transpose_1d(
      ctx,
      a,
      b,
      s0,
      p0,
      d0,
    );
  }

  late final _ggml_conv_transpose_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_conv_transpose_1d');
  late final _ggml_conv_transpose_1d = _ggml_conv_transpose_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_conv_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int s0,
    int s1,
    int p0,
    int p1,
    int d0,
    int d1,
  ) {
    return _ggml_conv_2d(
      ctx,
      a,
      b,
      s0,
      s1,
      p0,
      p1,
      d0,
      d1,
    );
  }

  late final _ggml_conv_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_conv_2d');
  late final _ggml_conv_2d = _ggml_conv_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          int)>();

  /// kernel size is a->ne[0] x a->ne[1]
  /// stride is equal to kernel size
  /// padding is zero
  /// example:
  /// a:     16   16    3  768
  /// b:   1024 1024    3    1
  /// res:   64   64  768    1
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_conv_2d_sk_p0(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_conv_2d_sk_p0(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_conv_2d_sk_p0Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_conv_2d_sk_p0');
  late final _ggml_conv_2d_sk_p0 = _ggml_conv_2d_sk_p0Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// kernel size is a->ne[0] x a->ne[1]
  /// stride is 1
  /// padding is half
  /// example:
  /// a:      3    3    256  256
  /// b:     64   64    256    1
  /// res:   64   64    256    1
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_conv_2d_s1_ph(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_conv_2d_s1_ph(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_conv_2d_s1_phPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_conv_2d_s1_ph');
  late final _ggml_conv_2d_s1_ph = _ggml_conv_2d_s1_phPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_conv_transpose_2d_p0(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int stride,
  ) {
    return _ggml_conv_transpose_2d_p0(
      ctx,
      a,
      b,
      stride,
    );
  }

  late final _ggml_conv_transpose_2d_p0Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int)>>('ggml_conv_transpose_2d_p0');
  late final _ggml_conv_transpose_2d_p0 =
      _ggml_conv_transpose_2d_p0Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_pool_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int op,
    int k0,
    int s0,
    int p0,
  ) {
    return _ggml_pool_1d(
      ctx,
      a,
      op,
      k0,
      s0,
      p0,
    );
  }

  late final _ggml_pool_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int32,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_pool_1d');
  late final _ggml_pool_1d = _ggml_pool_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// the result will have 2*p0 padding for the first dimension
  /// and 2*p1 padding for the second dimension
  ffi.Pointer<ggml_tensor> ggml_pool_2d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int op,
    int k0,
    int k1,
    int s0,
    int s1,
    double p0,
    double p1,
  ) {
    return _ggml_pool_2d(
      ctx,
      a,
      op,
      k0,
      k1,
      s0,
      s1,
      p0,
      p1,
    );
  }

  late final _ggml_pool_2dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int32,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float)>>('ggml_pool_2d');
  late final _ggml_pool_2d = _ggml_pool_2dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int, int, double, double)>();

  ffi.Pointer<ggml_tensor> ggml_pool_2d_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> af,
    int op,
    int k0,
    int k1,
    int s0,
    int s1,
    double p0,
    double p1,
  ) {
    return _ggml_pool_2d_back(
      ctx,
      a,
      af,
      op,
      k0,
      k1,
      s0,
      s1,
      p0,
      p1,
    );
  }

  late final _ggml_pool_2d_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int32,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Float,
              ffi.Float)>>('ggml_pool_2d_back');
  late final _ggml_pool_2d_back = _ggml_pool_2d_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          int,
          int,
          int,
          int,
          int,
          double,
          double)>();

  /// nearest interpolate
  /// multiplies ne0 and ne1 by scale factor
  /// used in stable-diffusion
  ffi.Pointer<ggml_tensor> ggml_upscale(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int scale_factor,
  ) {
    return _ggml_upscale(
      ctx,
      a,
      scale_factor,
    );
  }

  late final _ggml_upscalePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_upscale');
  late final _ggml_upscale = _ggml_upscalePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// nearest interpolate
  /// nearest interpolate to specified dimensions
  /// used in tortoise.cpp
  ffi.Pointer<ggml_tensor> ggml_upscale_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int ne0,
    int ne1,
    int ne2,
    int ne3,
  ) {
    return _ggml_upscale_ext(
      ctx,
      a,
      ne0,
      ne1,
      ne2,
      ne3,
    );
  }

  late final _ggml_upscale_extPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_upscale_ext');
  late final _ggml_upscale_ext = _ggml_upscale_extPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
  ffi.Pointer<ggml_tensor> ggml_pad(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int p0,
    int p1,
    int p2,
    int p3,
  ) {
    return _ggml_pad(
      ctx,
      a,
      p0,
      p1,
      p2,
      p3,
    );
  }

  late final _ggml_padPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_pad');
  late final _ggml_pad = _ggml_padPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  /// pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
  ffi.Pointer<ggml_tensor> ggml_pad_reflect_1d(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int p0,
    int p1,
  ) {
    return _ggml_pad_reflect_1d(
      ctx,
      a,
      p0,
      p1,
    );
  }

  late final _ggml_pad_reflect_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int)>>('ggml_pad_reflect_1d');
  late final _ggml_pad_reflect_1d = _ggml_pad_reflect_1dPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
  /// timesteps: [N,]
  /// return: [N, dim]
  ffi.Pointer<ggml_tensor> ggml_timestep_embedding(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> timesteps,
    int dim,
    int max_period,
  ) {
    return _ggml_timestep_embedding(
      ctx,
      timesteps,
      dim,
      max_period,
    );
  }

  late final _ggml_timestep_embeddingPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int)>>('ggml_timestep_embedding');
  late final _ggml_timestep_embedding = _ggml_timestep_embeddingPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_argsort(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int order,
  ) {
    return _ggml_argsort(
      ctx,
      a,
      order,
    );
  }

  late final _ggml_argsortPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_argsort');
  late final _ggml_argsort = _ggml_argsortPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_arange(
    ffi.Pointer<ggml_context> ctx,
    double start,
    double stop,
    double step,
  ) {
    return _ggml_arange(
      ctx,
      start,
      stop,
      step,
    );
  }

  late final _ggml_arangePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Float, ffi.Float, ffi.Float)>>('ggml_arange');
  late final _ggml_arange = _ggml_arangePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, double, double, double)>();

  /// top k elements per row
  ffi.Pointer<ggml_tensor> ggml_top_k(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int k,
  ) {
    return _ggml_top_k(
      ctx,
      a,
      k,
    );
  }

  late final _ggml_top_kPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_top_k');
  late final _ggml_top_k = _ggml_top_kPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// q:    [n_embd, n_batch,     n_head,    1]
  /// k:    [n_embd, n_kv,        n_head_kv, 1]
  /// v:    [n_embd, n_kv,        n_head_kv, 1] !! not transposed !!
  /// mask: [n_kv,   n_batch_pad, 1,         1] !! n_batch_pad = GGML_PAD(n_batch, GGML_KQ_MASK_PAD) !!
  /// res:  [n_embd, n_head,      n_batch,   1] !! permuted !!
  ffi.Pointer<ggml_tensor> ggml_flash_attn_ext(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> q,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> mask,
    double scale,
    double max_bias,
    double logit_softcap,
  ) {
    return _ggml_flash_attn_ext(
      ctx,
      q,
      k,
      v,
      mask,
      scale,
      max_bias,
      logit_softcap,
    );
  }

  late final _ggml_flash_attn_extPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Float,
              ffi.Float,
              ffi.Float)>>('ggml_flash_attn_ext');
  late final _ggml_flash_attn_ext = _ggml_flash_attn_extPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          double,
          double,
          double)>();

  void ggml_flash_attn_ext_set_prec(
    ffi.Pointer<ggml_tensor> a,
    int prec,
  ) {
    return _ggml_flash_attn_ext_set_prec(
      a,
      prec,
    );
  }

  late final _ggml_flash_attn_ext_set_precPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>,
              ffi.Int32)>>('ggml_flash_attn_ext_set_prec');
  late final _ggml_flash_attn_ext_set_prec = _ggml_flash_attn_ext_set_precPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int)>();

  int ggml_flash_attn_ext_get_prec(
    ffi.Pointer<ggml_tensor> a,
  ) {
    return _ggml_flash_attn_ext_get_prec(
      a,
    );
  }

  late final _ggml_flash_attn_ext_get_precPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_flash_attn_ext_get_prec');
  late final _ggml_flash_attn_ext_get_prec = _ggml_flash_attn_ext_get_precPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>)>();

  /// TODO: needs to be adapted to ggml_flash_attn_ext
  ffi.Pointer<ggml_tensor> ggml_flash_attn_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> q,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> d,
    bool masked,
  ) {
    return _ggml_flash_attn_back(
      ctx,
      q,
      k,
      v,
      d,
      masked,
    );
  }

  late final _ggml_flash_attn_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Bool)>>('ggml_flash_attn_back');
  late final _ggml_flash_attn_back = _ggml_flash_attn_backPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          bool)>();

  ffi.Pointer<ggml_tensor> ggml_ssm_conv(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> sx,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_ssm_conv(
      ctx,
      sx,
      c,
    );
  }

  late final _ggml_ssm_convPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_ssm_conv');
  late final _ggml_ssm_conv = _ggml_ssm_convPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_ssm_scan(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> s,
    ffi.Pointer<ggml_tensor> x,
    ffi.Pointer<ggml_tensor> dt,
    ffi.Pointer<ggml_tensor> A,
    ffi.Pointer<ggml_tensor> B,
    ffi.Pointer<ggml_tensor> C,
  ) {
    return _ggml_ssm_scan(
      ctx,
      s,
      x,
      dt,
      A,
      B,
      C,
    );
  }

  late final _ggml_ssm_scanPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_ssm_scan');
  late final _ggml_ssm_scan = _ggml_ssm_scanPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  /// partition into non-overlapping windows with padding if needed
  /// example:
  /// a:   768   64   64    1
  /// w:    14
  /// res: 768   14   14    25
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_win_part(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int w,
  ) {
    return _ggml_win_part(
      ctx,
      a,
      w,
    );
  }

  late final _ggml_win_partPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_win_part');
  late final _ggml_win_part = _ggml_win_partPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// reverse of ggml_win_part
  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_win_unpart(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int w0,
    int h0,
    int w,
  ) {
    return _ggml_win_unpart(
      ctx,
      a,
      w0,
      h0,
      w,
    );
  }

  late final _ggml_win_unpartPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Int,
              ffi.Int,
              ffi.Int)>>('ggml_win_unpart');
  late final _ggml_win_unpart = _ggml_win_unpartPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, int, int, int)>();

  ffi.Pointer<ggml_tensor> ggml_unary(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int op,
  ) {
    return _ggml_unary(
      ctx,
      a,
      op,
    );
  }

  late final _ggml_unaryPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_unary');
  late final _ggml_unary = _ggml_unaryPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_unary_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int op,
  ) {
    return _ggml_unary_inplace(
      ctx,
      a,
      op,
    );
  }

  late final _ggml_unary_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_unary_inplace');
  late final _ggml_unary_inplace = _ggml_unary_inplacePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int)>();

  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_get_rel_pos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    int qh,
    int kh,
  ) {
    return _ggml_get_rel_pos(
      ctx,
      a,
      qh,
      kh,
    );
  }

  late final _ggml_get_rel_posPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int)>>('ggml_get_rel_pos');
  late final _ggml_get_rel_pos = _ggml_get_rel_posPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_tensor>, int, int)>();

  /// used in sam
  ffi.Pointer<ggml_tensor> ggml_add_rel_pos(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> pw,
    ffi.Pointer<ggml_tensor> ph,
  ) {
    return _ggml_add_rel_pos(
      ctx,
      a,
      pw,
      ph,
    );
  }

  late final _ggml_add_rel_posPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_add_rel_pos');
  late final _ggml_add_rel_pos = _ggml_add_rel_posPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_add_rel_pos_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> pw,
    ffi.Pointer<ggml_tensor> ph,
  ) {
    return _ggml_add_rel_pos_inplace(
      ctx,
      a,
      pw,
      ph,
    );
  }

  late final _ggml_add_rel_pos_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_add_rel_pos_inplace');
  late final _ggml_add_rel_pos_inplace =
      _ggml_add_rel_pos_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_rwkv_wkv6(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> k,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> r,
    ffi.Pointer<ggml_tensor> tf,
    ffi.Pointer<ggml_tensor> td,
    ffi.Pointer<ggml_tensor> state,
  ) {
    return _ggml_rwkv_wkv6(
      ctx,
      k,
      v,
      r,
      tf,
      td,
      state,
    );
  }

  late final _ggml_rwkv_wkv6Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_rwkv_wkv6');
  late final _ggml_rwkv_wkv6 = _ggml_rwkv_wkv6Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_map_unary_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_unary_op_f32_t fun,
  ) {
    return _ggml_map_unary_f32(
      ctx,
      a,
      fun,
    );
  }

  late final _ggml_map_unary_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_unary_op_f32_t)>>('ggml_map_unary_f32');
  late final _ggml_map_unary_f32 = _ggml_map_unary_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ggml_unary_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_unary_inplace_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_unary_op_f32_t fun,
  ) {
    return _ggml_map_unary_inplace_f32(
      ctx,
      a,
      fun,
    );
  }

  late final _ggml_map_unary_inplace_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_unary_op_f32_t)>>('ggml_map_unary_inplace_f32');
  late final _ggml_map_unary_inplace_f32 =
      _ggml_map_unary_inplace_f32Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ggml_unary_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_binary_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_binary_op_f32_t fun,
  ) {
    return _ggml_map_binary_f32(
      ctx,
      a,
      b,
      fun,
    );
  }

  late final _ggml_map_binary_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_binary_op_f32_t)>>('ggml_map_binary_f32');
  late final _ggml_map_binary_f32 = _ggml_map_binary_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_binary_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_binary_inplace_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_binary_op_f32_t fun,
  ) {
    return _ggml_map_binary_inplace_f32(
      ctx,
      a,
      b,
      fun,
    );
  }

  late final _ggml_map_binary_inplace_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_binary_op_f32_t)>>('ggml_map_binary_inplace_f32');
  late final _ggml_map_binary_inplace_f32 =
      _ggml_map_binary_inplace_f32Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_binary_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom1_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_f32_t fun,
  ) {
    return _ggml_map_custom1_f32(
      ctx,
      a,
      fun,
    );
  }

  late final _ggml_map_custom1_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom1_op_f32_t)>>('ggml_map_custom1_f32');
  late final _ggml_map_custom1_f32 = _ggml_map_custom1_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ggml_custom1_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_f32_t fun,
  ) {
    return _ggml_map_custom1_inplace_f32(
      ctx,
      a,
      fun,
    );
  }

  late final _ggml_map_custom1_inplace_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom1_op_f32_t)>>('ggml_map_custom1_inplace_f32');
  late final _ggml_map_custom1_inplace_f32 =
      _ggml_map_custom1_inplace_f32Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>, ggml_custom1_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom2_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_f32_t fun,
  ) {
    return _ggml_map_custom2_f32(
      ctx,
      a,
      b,
      fun,
    );
  }

  late final _ggml_map_custom2_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_f32_t)>>('ggml_map_custom2_f32');
  late final _ggml_map_custom2_f32 = _ggml_map_custom2_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom2_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_f32_t fun,
  ) {
    return _ggml_map_custom2_inplace_f32(
      ctx,
      a,
      b,
      fun,
    );
  }

  late final _ggml_map_custom2_inplace_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_f32_t)>>('ggml_map_custom2_inplace_f32');
  late final _ggml_map_custom2_inplace_f32 =
      _ggml_map_custom2_inplace_f32Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom3_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_f32_t fun,
  ) {
    return _ggml_map_custom3_f32(
      ctx,
      a,
      b,
      c,
      fun,
    );
  }

  late final _ggml_map_custom3_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_f32_t)>>('ggml_map_custom3_f32');
  late final _ggml_map_custom3_f32 = _ggml_map_custom3_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom3_op_f32_t)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace_f32(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_f32_t fun,
  ) {
    return _ggml_map_custom3_inplace_f32(
      ctx,
      a,
      b,
      c,
      fun,
    );
  }

  late final _ggml_map_custom3_inplace_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_f32_t)>>('ggml_map_custom3_inplace_f32');
  late final _ggml_map_custom3_inplace_f32 =
      _ggml_map_custom3_inplace_f32Ptr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_f32_t)>();

  /// n_tasks == GGML_N_TASKS_MAX means to use max number of tasks
  ffi.Pointer<ggml_tensor> ggml_map_custom1(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom1(
      ctx,
      a,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom1Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom1_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom1');
  late final _ggml_map_custom1 = _ggml_map_custom1Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom1_op_t,
          int,
          ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ggml_custom1_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom1_inplace(
      ctx,
      a,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom1_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom1_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom1_inplace');
  late final _ggml_map_custom1_inplace =
      _ggml_map_custom1_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom1_op_t,
              int,
              ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom2(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom2(
      ctx,
      a,
      b,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom2Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom2');
  late final _ggml_map_custom2 = _ggml_map_custom2Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom2_op_t,
          int,
          ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ggml_custom2_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom2_inplace(
      ctx,
      a,
      b,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom2_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom2_inplace');
  late final _ggml_map_custom2_inplace =
      _ggml_map_custom2_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom2_op_t,
              int,
              ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom3(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom3(
      ctx,
      a,
      b,
      c,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom3Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom3');
  late final _ggml_map_custom3 = _ggml_map_custom3Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ggml_custom3_op_t,
          int,
          ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ggml_custom3_op_t fun,
    int n_tasks,
    ffi.Pointer<ffi.Void> userdata,
  ) {
    return _ggml_map_custom3_inplace(
      ctx,
      a,
      b,
      c,
      fun,
      n_tasks,
      userdata,
    );
  }

  late final _ggml_map_custom3_inplacePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_t,
              ffi.Int,
              ffi.Pointer<ffi.Void>)>>('ggml_map_custom3_inplace');
  late final _ggml_map_custom3_inplace =
      _ggml_map_custom3_inplacePtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ggml_custom3_op_t,
              int,
              ffi.Pointer<ffi.Void>)>();

  /// loss function
  ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
  ) {
    return _ggml_cross_entropy_loss(
      ctx,
      a,
      b,
    );
  }

  late final _ggml_cross_entropy_lossPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_cross_entropy_loss');
  late final _ggml_cross_entropy_loss = _ggml_cross_entropy_lossPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss_back(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
  ) {
    return _ggml_cross_entropy_loss_back(
      ctx,
      a,
      b,
      c,
    );
  }

  late final _ggml_cross_entropy_loss_backPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_cross_entropy_loss_back');
  late final _ggml_cross_entropy_loss_back =
      _ggml_cross_entropy_loss_backPtr.asFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>();

  /// AdamW optimizer step
  /// Paper: https://arxiv.org/pdf/1711.05101v3.pdf
  /// PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
  ffi.Pointer<ggml_tensor> ggml_opt_step_adamw(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> grad,
    ffi.Pointer<ggml_tensor> m,
    ffi.Pointer<ggml_tensor> v,
    ffi.Pointer<ggml_tensor> adamw_params,
  ) {
    return _ggml_opt_step_adamw(
      ctx,
      a,
      grad,
      m,
      v,
      adamw_params,
    );
  }

  late final _ggml_opt_step_adamwPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_opt_step_adamw');
  late final _ggml_opt_step_adamw = _ggml_opt_step_adamwPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_context>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>,
          ffi.Pointer<ggml_tensor>)>();

  /// automatic differentiation
  void ggml_build_forward_expand(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_build_forward_expand(
      cgraph,
      tensor,
    );
  }

  late final _ggml_build_forward_expandPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_tensor>)>>('ggml_build_forward_expand');
  late final _ggml_build_forward_expand =
      _ggml_build_forward_expandPtr.asFunction<
          void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>();

  void ggml_build_backward_expand(
    ffi.Pointer<ggml_context> ctx_static,
    ffi.Pointer<ggml_context> ctx_compute,
    ffi.Pointer<ggml_cgraph> cgraph,
    bool accumulate,
  ) {
    return _ggml_build_backward_expand(
      ctx_static,
      ctx_compute,
      cgraph,
      accumulate,
    );
  }

  late final _ggml_build_backward_expandPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_cgraph>,
              ffi.Bool)>>('ggml_build_backward_expand');
  late final _ggml_build_backward_expand =
      _ggml_build_backward_expandPtr.asFunction<
          void Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_cgraph>, bool)>();

  /// graph allocation in a context
  ffi.Pointer<ggml_cgraph> ggml_new_graph(
    ffi.Pointer<ggml_context> ctx,
  ) {
    return _ggml_new_graph(
      ctx,
    );
  }

  late final _ggml_new_graphPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(
              ffi.Pointer<ggml_context>)>>('ggml_new_graph');
  late final _ggml_new_graph = _ggml_new_graphPtr.asFunction<
      ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>)>();

  ffi.Pointer<ggml_cgraph> ggml_new_graph_custom(
    ffi.Pointer<ggml_context> ctx,
    int size,
    bool grads,
  ) {
    return _ggml_new_graph_custom(
      ctx,
      size,
      grads,
    );
  }

  late final _ggml_new_graph_customPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>, ffi.Size,
              ffi.Bool)>>('ggml_new_graph_custom');
  late final _ggml_new_graph_custom = _ggml_new_graph_customPtr.asFunction<
      ffi.Pointer<ggml_cgraph> Function(
          ffi.Pointer<ggml_context>, int, bool)>();

  ffi.Pointer<ggml_cgraph> ggml_graph_dup(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_dup(
      ctx,
      cgraph,
    );
  }

  late final _ggml_graph_dupPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_cgraph>)>>('ggml_graph_dup');
  late final _ggml_graph_dup = _ggml_graph_dupPtr.asFunction<
      ffi.Pointer<ggml_cgraph> Function(
          ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_cpy(
    ffi.Pointer<ggml_cgraph> src,
    ffi.Pointer<ggml_cgraph> dst,
  ) {
    return _ggml_graph_cpy(
      src,
      dst,
    );
  }

  late final _ggml_graph_cpyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_cgraph>)>>('ggml_graph_cpy');
  late final _ggml_graph_cpy = _ggml_graph_cpyPtr.asFunction<
      void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_reset(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_reset(
      cgraph,
    );
  }

  late final _ggml_graph_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
          'ggml_graph_reset');
  late final _ggml_graph_reset = _ggml_graph_resetPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_clear(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_clear(
      cgraph,
    );
  }

  late final _ggml_graph_clearPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
          'ggml_graph_clear');
  late final _ggml_graph_clear = _ggml_graph_clearPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  int ggml_graph_size(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_size(
      cgraph,
    );
  }

  late final _ggml_graph_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>>(
          'ggml_graph_size');
  late final _ggml_graph_size =
      _ggml_graph_sizePtr.asFunction<int Function(ffi.Pointer<ggml_cgraph>)>();

  ffi.Pointer<ggml_tensor> ggml_graph_node(
    ffi.Pointer<ggml_cgraph> cgraph,
    int i,
  ) {
    return _ggml_graph_node(
      cgraph,
      i,
    );
  }

  late final _ggml_graph_nodePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_cgraph>, ffi.Int)>>('ggml_graph_node');
  late final _ggml_graph_node = _ggml_graph_nodePtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>, int)>();

  ffi.Pointer<ffi.Pointer<ggml_tensor>> ggml_graph_nodes(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_nodes(
      cgraph,
    );
  }

  late final _ggml_graph_nodesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(
              ffi.Pointer<ggml_cgraph>)>>('ggml_graph_nodes');
  late final _ggml_graph_nodes = _ggml_graph_nodesPtr.asFunction<
      ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(
          ffi.Pointer<ggml_cgraph>)>();

  int ggml_graph_n_nodes(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_n_nodes(
      cgraph,
    );
  }

  late final _ggml_graph_n_nodesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>>(
          'ggml_graph_n_nodes');
  late final _ggml_graph_n_nodes = _ggml_graph_n_nodesPtr
      .asFunction<int Function(ffi.Pointer<ggml_cgraph>)>();

  void ggml_graph_add_node(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_graph_add_node(
      cgraph,
      tensor,
    );
  }

  late final _ggml_graph_add_nodePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_tensor>)>>('ggml_graph_add_node');
  late final _ggml_graph_add_node = _ggml_graph_add_nodePtr.asFunction<
      void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>();

  int ggml_graph_overhead() {
    return _ggml_graph_overhead();
  }

  late final _ggml_graph_overheadPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('ggml_graph_overhead');
  late final _ggml_graph_overhead =
      _ggml_graph_overheadPtr.asFunction<int Function()>();

  int ggml_graph_overhead_custom(
    int size,
    bool grads,
  ) {
    return _ggml_graph_overhead_custom(
      size,
      grads,
    );
  }

  late final _ggml_graph_overhead_customPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Size, ffi.Bool)>>(
          'ggml_graph_overhead_custom');
  late final _ggml_graph_overhead_custom =
      _ggml_graph_overhead_customPtr.asFunction<int Function(int, bool)>();

  ffi.Pointer<ggml_tensor> ggml_graph_get_tensor(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_graph_get_tensor(
      cgraph,
      name,
    );
  }

  late final _ggml_graph_get_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ffi.Char>)>>('ggml_graph_get_tensor');
  late final _ggml_graph_get_tensor = _ggml_graph_get_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>, ffi.Pointer<ffi.Char>)>();

  ffi.Pointer<ggml_tensor> ggml_graph_get_grad(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_graph_get_grad(
      cgraph,
      node,
    );
  }

  late final _ggml_graph_get_gradPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_tensor>)>>('ggml_graph_get_grad');
  late final _ggml_graph_get_grad = _ggml_graph_get_gradPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>();

  ffi.Pointer<ggml_tensor> ggml_graph_get_grad_acc(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_graph_get_grad_acc(
      cgraph,
      node,
    );
  }

  late final _ggml_graph_get_grad_accPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_tensor>)>>('ggml_graph_get_grad_acc');
  late final _ggml_graph_get_grad_acc = _ggml_graph_get_grad_accPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)>();

  void ggml_graph_export(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ffi.Char> fname,
  ) {
    return _ggml_graph_export(
      cgraph,
      fname,
    );
  }

  late final _ggml_graph_exportPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ffi.Char>)>>('ggml_graph_export');
  late final _ggml_graph_export = _ggml_graph_exportPtr.asFunction<
      void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ffi.Char>)>();

  ffi.Pointer<ggml_cgraph> ggml_graph_import(
    ffi.Pointer<ffi.Char> fname,
    ffi.Pointer<ffi.Pointer<ggml_context>> ctx_data,
    ffi.Pointer<ffi.Pointer<ggml_context>> ctx_eval,
  ) {
    return _ggml_graph_import(
      fname,
      ctx_data,
      ctx_eval,
    );
  }

  late final _ggml_graph_importPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_cgraph> Function(
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Pointer<ggml_context>>,
              ffi.Pointer<ffi.Pointer<ggml_context>>)>>('ggml_graph_import');
  late final _ggml_graph_import = _ggml_graph_importPtr.asFunction<
      ffi.Pointer<ggml_cgraph> Function(
          ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Pointer<ggml_context>>,
          ffi.Pointer<ffi.Pointer<ggml_context>>)>();

  /// print info and performance information for the graph
  void ggml_graph_print(
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_graph_print(
      cgraph,
    );
  }

  late final _ggml_graph_printPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>>(
          'ggml_graph_print');
  late final _ggml_graph_print = _ggml_graph_printPtr
      .asFunction<void Function(ffi.Pointer<ggml_cgraph>)>();

  /// dump the graph into a file using the dot format
  void ggml_graph_dump_dot(
    ffi.Pointer<ggml_cgraph> gb,
    ffi.Pointer<ggml_cgraph> gf,
    ffi.Pointer<ffi.Char> filename,
  ) {
    return _ggml_graph_dump_dot(
      gb,
      gf,
      filename,
    );
  }

  late final _ggml_graph_dump_dotPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ffi.Char>)>>('ggml_graph_dump_dot');
  late final _ggml_graph_dump_dot = _ggml_graph_dump_dotPtr.asFunction<
      void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Char>)>();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void ggml_log_set(
    ggml_log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _ggml_log_set(
      log_callback,
      user_data,
    );
  }

  late final _ggml_log_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_log_callback, ffi.Pointer<ffi.Void>)>>('ggml_log_set');
  late final _ggml_log_set = _ggml_log_setPtr
      .asFunction<void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_tensor> ggml_set_zero(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_set_zero(
      tensor,
    );
  }

  late final _ggml_set_zeroPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_tensor>)>>('ggml_set_zero');
  late final _ggml_set_zero = _ggml_set_zeroPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>)>();

  /// - ggml_quantize_init can be called multiple times with the same type
  /// it will only initialize the quantization tables for the first call or after ggml_quantize_free
  /// automatically called by ggml_quantize_chunk for convenience
  ///
  /// - ggml_quantize_free will free any memory allocated by ggml_quantize_init
  /// call this at the end of the program to avoid memory leaks
  ///
  /// note: these are thread-safe
  void ggml_quantize_init(
    int type,
  ) {
    return _ggml_quantize_init(
      type,
    );
  }

  late final _ggml_quantize_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Int32)>>(
          'ggml_quantize_init');
  late final _ggml_quantize_init =
      _ggml_quantize_initPtr.asFunction<void Function(int)>();

  void ggml_quantize_free() {
    return _ggml_quantize_free();
  }

  late final _ggml_quantize_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_quantize_free');
  late final _ggml_quantize_free =
      _ggml_quantize_freePtr.asFunction<void Function()>();

  /// some quantization type cannot be used without an importance matrix
  bool ggml_quantize_requires_imatrix(
    int type,
  ) {
    return _ggml_quantize_requires_imatrix(
      type,
    );
  }

  late final _ggml_quantize_requires_imatrixPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Int32)>>(
          'ggml_quantize_requires_imatrix');
  late final _ggml_quantize_requires_imatrix =
      _ggml_quantize_requires_imatrixPtr.asFunction<bool Function(int)>();

  /// calls ggml_quantize_init internally (i.e. can allocate memory)
  int ggml_quantize_chunk(
    int type,
    ffi.Pointer<ffi.Float> src,
    ffi.Pointer<ffi.Void> dst,
    int start,
    int nrows,
    int n_per_row,
    ffi.Pointer<ffi.Float> imatrix,
  ) {
    return _ggml_quantize_chunk(
      type,
      src,
      dst,
      start,
      nrows,
      n_per_row,
      imatrix,
    );
  }

  late final _ggml_quantize_chunkPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(
              ffi.Int32,
              ffi.Pointer<ffi.Float>,
              ffi.Pointer<ffi.Void>,
              ffi.Int64,
              ffi.Int64,
              ffi.Int64,
              ffi.Pointer<ffi.Float>)>>('ggml_quantize_chunk');
  late final _ggml_quantize_chunk = _ggml_quantize_chunkPtr.asFunction<
      int Function(int, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Void>, int, int,
          int, ffi.Pointer<ffi.Float>)>();

  ffi.Pointer<gguf_context> gguf_init_empty() {
    return _gguf_init_empty();
  }

  late final _gguf_init_emptyPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<gguf_context> Function()>>(
          'gguf_init_empty');
  late final _gguf_init_empty =
      _gguf_init_emptyPtr.asFunction<ffi.Pointer<gguf_context> Function()>();

  ffi.Pointer<gguf_context> gguf_init_from_file(
    ffi.Pointer<ffi.Char> fname,
    gguf_init_params params,
  ) {
    return _gguf_init_from_file(
      fname,
      params,
    );
  }

  late final _gguf_init_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<gguf_context> Function(
              ffi.Pointer<ffi.Char>, gguf_init_params)>>('gguf_init_from_file');
  late final _gguf_init_from_file = _gguf_init_from_filePtr.asFunction<
      ffi.Pointer<gguf_context> Function(
          ffi.Pointer<ffi.Char>, gguf_init_params)>();

  /// GGML_API struct gguf_context * gguf_init_from_buffer(..);
  void gguf_free(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_free(
      ctx,
    );
  }

  late final _gguf_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<gguf_context>)>>(
          'gguf_free');
  late final _gguf_free =
      _gguf_freePtr.asFunction<void Function(ffi.Pointer<gguf_context>)>();

  ffi.Pointer<ffi.Char> gguf_type_name(
    int type,
  ) {
    return _gguf_type_name(
      type,
    );
  }

  late final _gguf_type_namePtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ffi.Int32)>>(
          'gguf_type_name');
  late final _gguf_type_name =
      _gguf_type_namePtr.asFunction<ffi.Pointer<ffi.Char> Function(int)>();

  int gguf_get_version(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_version(
      ctx,
    );
  }

  late final _gguf_get_versionPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_version');
  late final _gguf_get_version = _gguf_get_versionPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_alignment(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_alignment(
      ctx,
    );
  }

  late final _gguf_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_alignment');
  late final _gguf_get_alignment = _gguf_get_alignmentPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_data_offset(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_data_offset(
      ctx,
    );
  }

  late final _gguf_get_data_offsetPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_data_offset');
  late final _gguf_get_data_offset = _gguf_get_data_offsetPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  ffi.Pointer<ffi.Void> gguf_get_data(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_data(
      ctx,
    );
  }

  late final _gguf_get_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<gguf_context>)>>('gguf_get_data');
  late final _gguf_get_data = _gguf_get_dataPtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>)>();

  int gguf_get_n_kv(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_n_kv(
      ctx,
    );
  }

  late final _gguf_get_n_kvPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_n_kv');
  late final _gguf_get_n_kv =
      _gguf_get_n_kvPtr.asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_find_key(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
  ) {
    return _gguf_find_key(
      ctx,
      key,
    );
  }

  late final _gguf_find_keyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<ffi.Char>)>>('gguf_find_key');
  late final _gguf_find_key = _gguf_find_keyPtr.asFunction<
      int Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)>();

  ffi.Pointer<ffi.Char> gguf_get_key(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_key(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_keyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_key');
  late final _gguf_get_key = _gguf_get_keyPtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_kv_type(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_kv_type(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_kv_typePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_kv_type');
  late final _gguf_get_kv_type = _gguf_get_kv_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_arr_type(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_arr_type(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_arr_typePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_arr_type');
  late final _gguf_get_arr_type = _gguf_get_arr_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  /// will abort if the wrong type is used for the key
  int gguf_get_val_u8(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_u8(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_u8Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Uint8 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_u8');
  late final _gguf_get_val_u8 = _gguf_get_val_u8Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i8(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_i8(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_i8Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int8 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_i8');
  late final _gguf_get_val_i8 = _gguf_get_val_i8Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u16(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_u16(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_u16Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Uint16 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_u16');
  late final _gguf_get_val_u16 = _gguf_get_val_u16Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i16(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_i16(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_i16Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int16 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_i16');
  late final _gguf_get_val_i16 = _gguf_get_val_i16Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u32(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_u32(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_u32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Uint32 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_u32');
  late final _gguf_get_val_u32 = _gguf_get_val_u32Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i32(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_i32(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_i32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_i32');
  late final _gguf_get_val_i32 = _gguf_get_val_i32Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  double gguf_get_val_f32(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_f32(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Float Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_f32');
  late final _gguf_get_val_f32 = _gguf_get_val_f32Ptr
      .asFunction<double Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_u64(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_u64(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_u64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Uint64 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_u64');
  late final _gguf_get_val_u64 = _gguf_get_val_u64Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_val_i64(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_i64(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_i64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Int64 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_i64');
  late final _gguf_get_val_i64 = _gguf_get_val_i64Ptr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  double gguf_get_val_f64(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_f64(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_f64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Double Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_f64');
  late final _gguf_get_val_f64 = _gguf_get_val_f64Ptr
      .asFunction<double Function(ffi.Pointer<gguf_context>, int)>();

  bool gguf_get_val_bool(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_bool(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_boolPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_bool');
  late final _gguf_get_val_bool = _gguf_get_val_boolPtr
      .asFunction<bool Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Char> gguf_get_val_str(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_str(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_str');
  late final _gguf_get_val_str = _gguf_get_val_strPtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Void> gguf_get_val_data(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_val_data(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_val_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_val_data');
  late final _gguf_get_val_data = _gguf_get_val_dataPtr.asFunction<
      ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_arr_n(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_arr_n(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_arr_nPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_arr_n');
  late final _gguf_get_arr_n = _gguf_get_arr_nPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Void> gguf_get_arr_data(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
  ) {
    return _gguf_get_arr_data(
      ctx,
      key_id,
    );
  }

  late final _gguf_get_arr_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_arr_data');
  late final _gguf_get_arr_data = _gguf_get_arr_dataPtr.asFunction<
      ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Char> gguf_get_arr_str(
    ffi.Pointer<gguf_context> ctx,
    int key_id,
    int i,
  ) {
    return _gguf_get_arr_str(
      ctx,
      key_id,
      i,
    );
  }

  late final _gguf_get_arr_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int,
              ffi.Int)>>('gguf_get_arr_str');
  late final _gguf_get_arr_str = _gguf_get_arr_strPtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int, int)>();

  int gguf_get_n_tensors(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_n_tensors(
      ctx,
    );
  }

  late final _gguf_get_n_tensorsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_n_tensors');
  late final _gguf_get_n_tensors = _gguf_get_n_tensorsPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  int gguf_find_tensor(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _gguf_find_tensor(
      ctx,
      name,
    );
  }

  late final _gguf_find_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<ffi.Char>)>>('gguf_find_tensor');
  late final _gguf_find_tensor = _gguf_find_tensorPtr.asFunction<
      int Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)>();

  int gguf_get_tensor_offset(
    ffi.Pointer<gguf_context> ctx,
    int i,
  ) {
    return _gguf_get_tensor_offset(
      ctx,
      i,
    );
  }

  late final _gguf_get_tensor_offsetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_tensor_offset');
  late final _gguf_get_tensor_offset = _gguf_get_tensor_offsetPtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  ffi.Pointer<ffi.Char> gguf_get_tensor_name(
    ffi.Pointer<gguf_context> ctx,
    int i,
  ) {
    return _gguf_get_tensor_name(
      ctx,
      i,
    );
  }

  late final _gguf_get_tensor_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_tensor_name');
  late final _gguf_get_tensor_name = _gguf_get_tensor_namePtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, int)>();

  int gguf_get_tensor_type(
    ffi.Pointer<gguf_context> ctx,
    int i,
  ) {
    return _gguf_get_tensor_type(
      ctx,
      i,
    );
  }

  late final _gguf_get_tensor_typePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<gguf_context>, ffi.Int)>>('gguf_get_tensor_type');
  late final _gguf_get_tensor_type = _gguf_get_tensor_typePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>, int)>();

  /// removes key if it exists
  void gguf_remove_key(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
  ) {
    return _gguf_remove_key(
      ctx,
      key,
    );
  }

  late final _gguf_remove_keyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<ffi.Char>)>>('gguf_remove_key');
  late final _gguf_remove_key = _gguf_remove_keyPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)>();

  /// overrides existing values or adds a new one
  void gguf_set_val_u8(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u8(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_u8Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Uint8)>>('gguf_set_val_u8');
  late final _gguf_set_val_u8 = _gguf_set_val_u8Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_i8(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i8(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_i8Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int8)>>('gguf_set_val_i8');
  late final _gguf_set_val_i8 = _gguf_set_val_i8Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_u16(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u16(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_u16Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Uint16)>>('gguf_set_val_u16');
  late final _gguf_set_val_u16 = _gguf_set_val_u16Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_i16(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i16(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_i16Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int16)>>('gguf_set_val_i16');
  late final _gguf_set_val_i16 = _gguf_set_val_i16Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_u32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u32(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_u32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Uint32)>>('gguf_set_val_u32');
  late final _gguf_set_val_u32 = _gguf_set_val_u32Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_i32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i32(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_i32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int32)>>('gguf_set_val_i32');
  late final _gguf_set_val_i32 = _gguf_set_val_i32Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_f32(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    double val,
  ) {
    return _gguf_set_val_f32(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Float)>>('gguf_set_val_f32');
  late final _gguf_set_val_f32 = _gguf_set_val_f32Ptr.asFunction<
      void Function(
          ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, double)>();

  void gguf_set_val_u64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_u64(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_u64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Uint64)>>('gguf_set_val_u64');
  late final _gguf_set_val_u64 = _gguf_set_val_u64Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_i64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int val,
  ) {
    return _gguf_set_val_i64(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_i64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int64)>>('gguf_set_val_i64');
  late final _gguf_set_val_i64 = _gguf_set_val_i64Ptr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_val_f64(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    double val,
  ) {
    return _gguf_set_val_f64(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_f64Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Double)>>('gguf_set_val_f64');
  late final _gguf_set_val_f64 = _gguf_set_val_f64Ptr.asFunction<
      void Function(
          ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, double)>();

  void gguf_set_val_bool(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    bool val,
  ) {
    return _gguf_set_val_bool(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_boolPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Bool)>>('gguf_set_val_bool');
  late final _gguf_set_val_bool = _gguf_set_val_boolPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, bool)>();

  void gguf_set_val_str(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> val,
  ) {
    return _gguf_set_val_str(
      ctx,
      key,
      val,
    );
  }

  late final _gguf_set_val_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>)>>('gguf_set_val_str');
  late final _gguf_set_val_str = _gguf_set_val_strPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Char>)>();

  void gguf_set_arr_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    int type,
    ffi.Pointer<ffi.Void> data,
    int n,
  ) {
    return _gguf_set_arr_data(
      ctx,
      key,
      type,
      data,
      n,
    );
  }

  late final _gguf_set_arr_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int32, ffi.Pointer<ffi.Void>, ffi.Int)>>('gguf_set_arr_data');
  late final _gguf_set_arr_data = _gguf_set_arr_dataPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int,
          ffi.Pointer<ffi.Void>, int)>();

  void gguf_set_arr_str(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Pointer<ffi.Char>> data,
    int n,
  ) {
    return _gguf_set_arr_str(
      ctx,
      key,
      data,
      n,
    );
  }

  late final _gguf_set_arr_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<gguf_context>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Pointer<ffi.Char>>,
              ffi.Int)>>('gguf_set_arr_str');
  late final _gguf_set_arr_str = _gguf_set_arr_strPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Pointer<ffi.Char>>, int)>();

  /// set or add KV pairs from another context
  void gguf_set_kv(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<gguf_context> src,
  ) {
    return _gguf_set_kv(
      ctx,
      src,
    );
  }

  late final _gguf_set_kvPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<gguf_context>)>>('gguf_set_kv');
  late final _gguf_set_kv = _gguf_set_kvPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<gguf_context>)>();

  /// manage tensor info
  void gguf_add_tensor(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _gguf_add_tensor(
      ctx,
      tensor,
    );
  }

  late final _gguf_add_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<ggml_tensor>)>>('gguf_add_tensor');
  late final _gguf_add_tensor = _gguf_add_tensorPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ggml_tensor>)>();

  void gguf_set_tensor_type(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
    int type,
  ) {
    return _gguf_set_tensor_type(
      ctx,
      name,
      type,
    );
  }

  late final _gguf_set_tensor_typePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Int32)>>('gguf_set_tensor_type');
  late final _gguf_set_tensor_type = _gguf_set_tensor_typePtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, int)>();

  void gguf_set_tensor_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<ffi.Void> data,
    int size,
  ) {
    return _gguf_set_tensor_data(
      ctx,
      name,
      data,
      size,
    );
  }

  late final _gguf_set_tensor_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Void>, ffi.Size)>>('gguf_set_tensor_data');
  late final _gguf_set_tensor_data = _gguf_set_tensor_dataPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<ffi.Void>, int)>();

  /// write the entire context to a binary file
  void gguf_write_to_file(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Char> fname,
    bool only_meta,
  ) {
    return _gguf_write_to_file(
      ctx,
      fname,
      only_meta,
    );
  }

  late final _gguf_write_to_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>,
              ffi.Bool)>>('gguf_write_to_file');
  late final _gguf_write_to_file = _gguf_write_to_filePtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, bool)>();

  /// get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
  int gguf_get_meta_size(
    ffi.Pointer<gguf_context> ctx,
  ) {
    return _gguf_get_meta_size(
      ctx,
    );
  }

  late final _gguf_get_meta_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ffi.Pointer<gguf_context>)>>(
          'gguf_get_meta_size');
  late final _gguf_get_meta_size = _gguf_get_meta_sizePtr
      .asFunction<int Function(ffi.Pointer<gguf_context>)>();

  void gguf_get_meta_data(
    ffi.Pointer<gguf_context> ctx,
    ffi.Pointer<ffi.Void> data,
  ) {
    return _gguf_get_meta_data(
      ctx,
      data,
    );
  }

  late final _gguf_get_meta_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<gguf_context>,
              ffi.Pointer<ffi.Void>)>>('gguf_get_meta_data');
  late final _gguf_get_meta_data = _gguf_get_meta_dataPtr.asFunction<
      void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Void>)>();

  ffi.Pointer<ggml_type_traits> ggml_get_type_traits(
    int type,
  ) {
    return _ggml_get_type_traits(
      type,
    );
  }

  late final _ggml_get_type_traitsPtr = _lookup<
          ffi
          .NativeFunction<ffi.Pointer<ggml_type_traits> Function(ffi.Int32)>>(
      'ggml_get_type_traits');
  late final _ggml_get_type_traits = _ggml_get_type_traitsPtr
      .asFunction<ffi.Pointer<ggml_type_traits> Function(int)>();

  ggml_threadpool_params ggml_threadpool_params_default(
    int n_threads,
  ) {
    return _ggml_threadpool_params_default(
      n_threads,
    );
  }

  late final _ggml_threadpool_params_defaultPtr =
      _lookup<ffi.NativeFunction<ggml_threadpool_params Function(ffi.Int)>>(
          'ggml_threadpool_params_default');
  late final _ggml_threadpool_params_default =
      _ggml_threadpool_params_defaultPtr
          .asFunction<ggml_threadpool_params Function(int)>();

  void ggml_threadpool_params_init(
    ffi.Pointer<ggml_threadpool_params> p,
    int n_threads,
  ) {
    return _ggml_threadpool_params_init(
      p,
      n_threads,
    );
  }

  late final _ggml_threadpool_params_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_threadpool_params>,
              ffi.Int)>>('ggml_threadpool_params_init');
  late final _ggml_threadpool_params_init = _ggml_threadpool_params_initPtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool_params>, int)>();

  bool ggml_threadpool_params_match(
    ffi.Pointer<ggml_threadpool_params> p0,
    ffi.Pointer<ggml_threadpool_params> p1,
  ) {
    return _ggml_threadpool_params_match(
      p0,
      p1,
    );
  }

  late final _ggml_threadpool_params_matchPtr = _lookup<
          ffi.NativeFunction<
              ffi.Bool Function(ffi.Pointer<ggml_threadpool_params>,
                  ffi.Pointer<ggml_threadpool_params>)>>(
      'ggml_threadpool_params_match');
  late final _ggml_threadpool_params_match =
      _ggml_threadpool_params_matchPtr.asFunction<
          bool Function(ffi.Pointer<ggml_threadpool_params>,
              ffi.Pointer<ggml_threadpool_params>)>();

  ggml_tallocr ggml_tallocr_new(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_tallocr_new(
      buffer,
    );
  }

  late final _ggml_tallocr_newPtr =
      _lookup<ffi.NativeFunction<ggml_tallocr Function(ggml_backend_buffer_t)>>(
          'ggml_tallocr_new');
  late final _ggml_tallocr_new = _ggml_tallocr_newPtr
      .asFunction<ggml_tallocr Function(ggml_backend_buffer_t)>();

  void ggml_tallocr_alloc(
    ffi.Pointer<ggml_tallocr> talloc,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_tallocr_alloc(
      talloc,
      tensor,
    );
  }

  late final _ggml_tallocr_allocPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tallocr>,
              ffi.Pointer<ggml_tensor>)>>('ggml_tallocr_alloc');
  late final _ggml_tallocr_alloc = _ggml_tallocr_allocPtr.asFunction<
      void Function(ffi.Pointer<ggml_tallocr>, ffi.Pointer<ggml_tensor>)>();

  ggml_gallocr_t ggml_gallocr_new(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_gallocr_new(
      buft,
    );
  }

  late final _ggml_gallocr_newPtr = _lookup<
          ffi
          .NativeFunction<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>>(
      'ggml_gallocr_new');
  late final _ggml_gallocr_new = _ggml_gallocr_newPtr
      .asFunction<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>();

  ggml_gallocr_t ggml_gallocr_new_n(
    ffi.Pointer<ggml_backend_buffer_type_t> bufts,
    int n_bufs,
  ) {
    return _ggml_gallocr_new_n(
      bufts,
      n_bufs,
    );
  }

  late final _ggml_gallocr_new_nPtr = _lookup<
      ffi.NativeFunction<
          ggml_gallocr_t Function(ffi.Pointer<ggml_backend_buffer_type_t>,
              ffi.Int)>>('ggml_gallocr_new_n');
  late final _ggml_gallocr_new_n = _ggml_gallocr_new_nPtr.asFunction<
      ggml_gallocr_t Function(ffi.Pointer<ggml_backend_buffer_type_t>, int)>();

  void ggml_gallocr_free(
    ggml_gallocr_t galloc,
  ) {
    return _ggml_gallocr_free(
      galloc,
    );
  }

  late final _ggml_gallocr_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_gallocr_t)>>(
          'ggml_gallocr_free');
  late final _ggml_gallocr_free =
      _ggml_gallocr_freePtr.asFunction<void Function(ggml_gallocr_t)>();

  /// pre-allocate buffers from a measure graph - does not allocate or modify the graph
  /// call with a worst-case graph to avoid buffer reallocations
  /// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
  /// returns false if the buffer allocation failed
  bool ggml_gallocr_reserve(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_gallocr_reserve(
      galloc,
      graph,
    );
  }

  late final _ggml_gallocr_reservePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_gallocr_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_gallocr_reserve');
  late final _ggml_gallocr_reserve = _ggml_gallocr_reservePtr
      .asFunction<bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>();

  bool ggml_gallocr_reserve_n(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
    ffi.Pointer<ffi.Int> node_buffer_ids,
    ffi.Pointer<ffi.Int> leaf_buffer_ids,
  ) {
    return _ggml_gallocr_reserve_n(
      galloc,
      graph,
      node_buffer_ids,
      leaf_buffer_ids,
    );
  }

  late final _ggml_gallocr_reserve_nPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ggml_gallocr_t,
              ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ffi.Int>,
              ffi.Pointer<ffi.Int>)>>('ggml_gallocr_reserve_n');
  late final _ggml_gallocr_reserve_n = _ggml_gallocr_reserve_nPtr.asFunction<
      bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>,
          ffi.Pointer<ffi.Int>, ffi.Pointer<ffi.Int>)>();

  /// automatic reallocation if the topology changes when using a single buffer
  /// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
  bool ggml_gallocr_alloc_graph(
    ggml_gallocr_t galloc,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_gallocr_alloc_graph(
      galloc,
      graph,
    );
  }

  late final _ggml_gallocr_alloc_graphPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_gallocr_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_gallocr_alloc_graph');
  late final _ggml_gallocr_alloc_graph = _ggml_gallocr_alloc_graphPtr
      .asFunction<bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_gallocr_get_buffer_size(
    ggml_gallocr_t galloc,
    int buffer_id,
  ) {
    return _ggml_gallocr_get_buffer_size(
      galloc,
      buffer_id,
    );
  }

  late final _ggml_gallocr_get_buffer_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_gallocr_t, ffi.Int)>>(
          'ggml_gallocr_get_buffer_size');
  late final _ggml_gallocr_get_buffer_size = _ggml_gallocr_get_buffer_sizePtr
      .asFunction<int Function(ggml_gallocr_t, int)>();

  /// Utils
  /// Create a buffer and allocate all the tensors in a ggml_context
  ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors_from_buft(
    ffi.Pointer<ggml_context> ctx,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_alloc_ctx_tensors_from_buft(
      ctx,
      buft,
    );
  }

  late final _ggml_backend_alloc_ctx_tensors_from_buftPtr = _lookup<
          ffi.NativeFunction<
              ffi.Pointer<ggml_backend_buffer> Function(
                  ffi.Pointer<ggml_context>, ggml_backend_buffer_type_t)>>(
      'ggml_backend_alloc_ctx_tensors_from_buft');
  late final _ggml_backend_alloc_ctx_tensors_from_buft =
      _ggml_backend_alloc_ctx_tensors_from_buftPtr.asFunction<
          ffi.Pointer<ggml_backend_buffer> Function(
              ffi.Pointer<ggml_context>, ggml_backend_buffer_type_t)>();

  ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors(
    ffi.Pointer<ggml_context> ctx,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_alloc_ctx_tensors(
      ctx,
      backend,
    );
  }

  late final _ggml_backend_alloc_ctx_tensorsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_backend_buffer> Function(ffi.Pointer<ggml_context>,
              ggml_backend_t)>>('ggml_backend_alloc_ctx_tensors');
  late final _ggml_backend_alloc_ctx_tensors =
      _ggml_backend_alloc_ctx_tensorsPtr.asFunction<
          ffi.Pointer<ggml_backend_buffer> Function(
              ffi.Pointer<ggml_context>, ggml_backend_t)>();

  /// Backend buffer type
  ffi.Pointer<ffi.Char> ggml_backend_buft_name(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_name(
      buft,
    );
  }

  late final _ggml_backend_buft_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ggml_backend_buffer_type_t)>>('ggml_backend_buft_name');
  late final _ggml_backend_buft_name = _ggml_backend_buft_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_type_t)>();

  ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(
    ggml_backend_buffer_type_t buft,
    int size,
  ) {
    return _ggml_backend_buft_alloc_buffer(
      buft,
      size,
    );
  }

  late final _ggml_backend_buft_alloc_bufferPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_t Function(ggml_backend_buffer_type_t,
              ffi.Size)>>('ggml_backend_buft_alloc_buffer');
  late final _ggml_backend_buft_alloc_buffer =
      _ggml_backend_buft_alloc_bufferPtr.asFunction<
          ggml_backend_buffer_t Function(ggml_backend_buffer_type_t, int)>();

  int ggml_backend_buft_get_alignment(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_get_alignment(
      buft,
    );
  }

  late final _ggml_backend_buft_get_alignmentPtr = _lookup<
          ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_type_t)>>(
      'ggml_backend_buft_get_alignment');
  late final _ggml_backend_buft_get_alignment =
      _ggml_backend_buft_get_alignmentPtr
          .asFunction<int Function(ggml_backend_buffer_type_t)>();

  int ggml_backend_buft_get_max_size(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_get_max_size(
      buft,
    );
  }

  late final _ggml_backend_buft_get_max_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_type_t)>>(
      'ggml_backend_buft_get_max_size');
  late final _ggml_backend_buft_get_max_size =
      _ggml_backend_buft_get_max_sizePtr
          .asFunction<int Function(ggml_backend_buffer_type_t)>();

  int ggml_backend_buft_get_alloc_size(
    ggml_backend_buffer_type_t buft,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_buft_get_alloc_size(
      buft,
      tensor,
    );
  }

  late final _ggml_backend_buft_get_alloc_sizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ggml_backend_buffer_type_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_buft_get_alloc_size');
  late final _ggml_backend_buft_get_alloc_size =
      _ggml_backend_buft_get_alloc_sizePtr.asFunction<
          int Function(ggml_backend_buffer_type_t, ffi.Pointer<ggml_tensor>)>();

  bool ggml_backend_buft_is_host(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_is_host(
      buft,
    );
  }

  late final _ggml_backend_buft_is_hostPtr = _lookup<
          ffi.NativeFunction<ffi.Bool Function(ggml_backend_buffer_type_t)>>(
      'ggml_backend_buft_is_host');
  late final _ggml_backend_buft_is_host = _ggml_backend_buft_is_hostPtr
      .asFunction<bool Function(ggml_backend_buffer_type_t)>();

  ggml_backend_dev_t ggml_backend_buft_get_device(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_buft_get_device(
      buft,
    );
  }

  late final _ggml_backend_buft_get_devicePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_dev_t Function(
              ggml_backend_buffer_type_t)>>('ggml_backend_buft_get_device');
  late final _ggml_backend_buft_get_device = _ggml_backend_buft_get_devicePtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_buffer_type_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_buffer_name(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_name(
      buffer,
    );
  }

  late final _ggml_backend_buffer_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ggml_backend_buffer_t)>>('ggml_backend_buffer_name');
  late final _ggml_backend_buffer_name = _ggml_backend_buffer_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_free(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_free(
      buffer,
    );
  }

  late final _ggml_backend_buffer_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_free');
  late final _ggml_backend_buffer_free = _ggml_backend_buffer_freePtr
      .asFunction<void Function(ggml_backend_buffer_t)>();

  ffi.Pointer<ffi.Void> ggml_backend_buffer_get_base(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_base(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_basePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(
              ggml_backend_buffer_t)>>('ggml_backend_buffer_get_base');
  late final _ggml_backend_buffer_get_base = _ggml_backend_buffer_get_basePtr
      .asFunction<ffi.Pointer<ffi.Void> Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_size(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_size(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_get_size');
  late final _ggml_backend_buffer_get_size = _ggml_backend_buffer_get_sizePtr
      .asFunction<int Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_init_tensor(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_buffer_init_tensor(
      buffer,
      tensor,
    );
  }

  late final _ggml_backend_buffer_init_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_buffer_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_buffer_init_tensor');
  late final _ggml_backend_buffer_init_tensor =
      _ggml_backend_buffer_init_tensorPtr.asFunction<
          void Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)>();

  int ggml_backend_buffer_get_alignment(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_alignment(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_get_alignment');
  late final _ggml_backend_buffer_get_alignment =
      _ggml_backend_buffer_get_alignmentPtr
          .asFunction<int Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_max_size(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_max_size(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_max_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_get_max_size');
  late final _ggml_backend_buffer_get_max_size =
      _ggml_backend_buffer_get_max_sizePtr
          .asFunction<int Function(ggml_backend_buffer_t)>();

  int ggml_backend_buffer_get_alloc_size(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_buffer_get_alloc_size(
      buffer,
      tensor,
    );
  }

  late final _ggml_backend_buffer_get_alloc_sizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ggml_backend_buffer_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_buffer_get_alloc_size');
  late final _ggml_backend_buffer_get_alloc_size =
      _ggml_backend_buffer_get_alloc_sizePtr.asFunction<
          int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)>();

  void ggml_backend_buffer_clear(
    ggml_backend_buffer_t buffer,
    int value,
  ) {
    return _ggml_backend_buffer_clear(
      buffer,
      value,
    );
  }

  late final _ggml_backend_buffer_clearPtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ggml_backend_buffer_t, ffi.Uint8)>>(
      'ggml_backend_buffer_clear');
  late final _ggml_backend_buffer_clear = _ggml_backend_buffer_clearPtr
      .asFunction<void Function(ggml_backend_buffer_t, int)>();

  bool ggml_backend_buffer_is_host(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_is_host(
      buffer,
    );
  }

  late final _ggml_backend_buffer_is_hostPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_is_host');
  late final _ggml_backend_buffer_is_host = _ggml_backend_buffer_is_hostPtr
      .asFunction<bool Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_set_usage(
    ggml_backend_buffer_t buffer,
    int usage,
  ) {
    return _ggml_backend_buffer_set_usage(
      buffer,
      usage,
    );
  }

  late final _ggml_backend_buffer_set_usagePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ggml_backend_buffer_t, ffi.Int32)>>(
      'ggml_backend_buffer_set_usage');
  late final _ggml_backend_buffer_set_usage = _ggml_backend_buffer_set_usagePtr
      .asFunction<void Function(ggml_backend_buffer_t, int)>();

  int ggml_backend_buffer_get_usage(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_usage(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_usagePtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_get_usage');
  late final _ggml_backend_buffer_get_usage = _ggml_backend_buffer_get_usagePtr
      .asFunction<int Function(ggml_backend_buffer_t)>();

  ggml_backend_buffer_type_t ggml_backend_buffer_get_type(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_get_type(
      buffer,
    );
  }

  late final _ggml_backend_buffer_get_typePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(
              ggml_backend_buffer_t)>>('ggml_backend_buffer_get_type');
  late final _ggml_backend_buffer_get_type = _ggml_backend_buffer_get_typePtr
      .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_buffer_t)>();

  void ggml_backend_buffer_reset(
    ggml_backend_buffer_t buffer,
  ) {
    return _ggml_backend_buffer_reset(
      buffer,
    );
  }

  late final _ggml_backend_buffer_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_buffer_t)>>(
          'ggml_backend_buffer_reset');
  late final _ggml_backend_buffer_reset = _ggml_backend_buffer_resetPtr
      .asFunction<void Function(ggml_backend_buffer_t)>();

  /// tensor copy between different backends
  void ggml_backend_tensor_copy(
    ffi.Pointer<ggml_tensor> src,
    ffi.Pointer<ggml_tensor> dst,
  ) {
    return _ggml_backend_tensor_copy(
      src,
      dst,
    );
  }

  late final _ggml_backend_tensor_copyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_tensor_copy');
  late final _ggml_backend_tensor_copy =
      _ggml_backend_tensor_copyPtr.asFunction<
          void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  /// Backend (stream)
  ggml_guid_t ggml_backend_guid(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_guid(
      backend,
    );
  }

  late final _ggml_backend_guidPtr =
      _lookup<ffi.NativeFunction<ggml_guid_t Function(ggml_backend_t)>>(
          'ggml_backend_guid');
  late final _ggml_backend_guid =
      _ggml_backend_guidPtr.asFunction<ggml_guid_t Function(ggml_backend_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_name(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_name(
      backend,
    );
  }

  late final _ggml_backend_namePtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>>(
      'ggml_backend_name');
  late final _ggml_backend_name = _ggml_backend_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>();

  void ggml_backend_free(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_free(
      backend,
    );
  }

  late final _ggml_backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t)>>(
          'ggml_backend_free');
  late final _ggml_backend_free =
      _ggml_backend_freePtr.asFunction<void Function(ggml_backend_t)>();

  ggml_backend_buffer_type_t ggml_backend_get_default_buffer_type(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_get_default_buffer_type(
      backend,
    );
  }

  late final _ggml_backend_get_default_buffer_typePtr = _lookup<
          ffi
          .NativeFunction<ggml_backend_buffer_type_t Function(ggml_backend_t)>>(
      'ggml_backend_get_default_buffer_type');
  late final _ggml_backend_get_default_buffer_type =
      _ggml_backend_get_default_buffer_typePtr
          .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_t)>();

  ggml_backend_buffer_t ggml_backend_alloc_buffer(
    ggml_backend_t backend,
    int size,
  ) {
    return _ggml_backend_alloc_buffer(
      backend,
      size,
    );
  }

  late final _ggml_backend_alloc_bufferPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_t Function(
              ggml_backend_t, ffi.Size)>>('ggml_backend_alloc_buffer');
  late final _ggml_backend_alloc_buffer = _ggml_backend_alloc_bufferPtr
      .asFunction<ggml_backend_buffer_t Function(ggml_backend_t, int)>();

  int ggml_backend_get_alignment(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_get_alignment(
      backend,
    );
  }

  late final _ggml_backend_get_alignmentPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_t)>>(
          'ggml_backend_get_alignment');
  late final _ggml_backend_get_alignment =
      _ggml_backend_get_alignmentPtr.asFunction<int Function(ggml_backend_t)>();

  int ggml_backend_get_max_size(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_get_max_size(
      backend,
    );
  }

  late final _ggml_backend_get_max_sizePtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_t)>>(
          'ggml_backend_get_max_size');
  late final _ggml_backend_get_max_size =
      _ggml_backend_get_max_sizePtr.asFunction<int Function(ggml_backend_t)>();

  void ggml_backend_tensor_set_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_set_async(
      backend,
      tensor,
      data,
      offset,
      size,
    );
  }

  late final _ggml_backend_tensor_set_asyncPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_backend_t,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Size)>>('ggml_backend_tensor_set_async');
  late final _ggml_backend_tensor_set_async =
      _ggml_backend_tensor_set_asyncPtr.asFunction<
          void Function(ggml_backend_t, ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>, int, int)>();

  void ggml_backend_tensor_get_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_get_async(
      backend,
      tensor,
      data,
      offset,
      size,
    );
  }

  late final _ggml_backend_tensor_get_asyncPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_backend_t,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Size)>>('ggml_backend_tensor_get_async');
  late final _ggml_backend_tensor_get_async =
      _ggml_backend_tensor_get_asyncPtr.asFunction<
          void Function(ggml_backend_t, ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>, int, int)>();

  /// "offset" refers to the offset in tensor->data for setting/getting data
  void ggml_backend_tensor_set(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_set(
      tensor,
      data,
      offset,
      size,
    );
  }

  late final _ggml_backend_tensor_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>,
              ffi.Size, ffi.Size)>>('ggml_backend_tensor_set');
  late final _ggml_backend_tensor_set = _ggml_backend_tensor_setPtr.asFunction<
      void Function(
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, int, int)>();

  void ggml_backend_tensor_get(
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> data,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_get(
      tensor,
      data,
      offset,
      size,
    );
  }

  late final _ggml_backend_tensor_getPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>,
              ffi.Size, ffi.Size)>>('ggml_backend_tensor_get');
  late final _ggml_backend_tensor_get = _ggml_backend_tensor_getPtr.asFunction<
      void Function(
          ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>, int, int)>();

  void ggml_backend_tensor_memset(
    ffi.Pointer<ggml_tensor> tensor,
    int value,
    int offset,
    int size,
  ) {
    return _ggml_backend_tensor_memset(
      tensor,
      value,
      offset,
      size,
    );
  }

  late final _ggml_backend_tensor_memsetPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Uint8, ffi.Size,
              ffi.Size)>>('ggml_backend_tensor_memset');
  late final _ggml_backend_tensor_memset = _ggml_backend_tensor_memsetPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, int, int)>();

  void ggml_backend_synchronize(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_synchronize(
      backend,
    );
  }

  late final _ggml_backend_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t)>>(
          'ggml_backend_synchronize');
  late final _ggml_backend_synchronize =
      _ggml_backend_synchronizePtr.asFunction<void Function(ggml_backend_t)>();

  ggml_backend_graph_plan_t ggml_backend_graph_plan_create(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_backend_graph_plan_create(
      backend,
      cgraph,
    );
  }

  late final _ggml_backend_graph_plan_createPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_graph_plan_t Function(ggml_backend_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_graph_plan_create');
  late final _ggml_backend_graph_plan_create =
      _ggml_backend_graph_plan_createPtr.asFunction<
          ggml_backend_graph_plan_t Function(
              ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  void ggml_backend_graph_plan_free(
    ggml_backend_t backend,
    ggml_backend_graph_plan_t plan,
  ) {
    return _ggml_backend_graph_plan_free(
      backend,
      plan,
    );
  }

  late final _ggml_backend_graph_plan_freePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t,
              ggml_backend_graph_plan_t)>>('ggml_backend_graph_plan_free');
  late final _ggml_backend_graph_plan_free = _ggml_backend_graph_plan_freePtr
      .asFunction<void Function(ggml_backend_t, ggml_backend_graph_plan_t)>();

  int ggml_backend_graph_plan_compute(
    ggml_backend_t backend,
    ggml_backend_graph_plan_t plan,
  ) {
    return _ggml_backend_graph_plan_compute(
      backend,
      plan,
    );
  }

  late final _ggml_backend_graph_plan_computePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ggml_backend_t,
              ggml_backend_graph_plan_t)>>('ggml_backend_graph_plan_compute');
  late final _ggml_backend_graph_plan_compute =
      _ggml_backend_graph_plan_computePtr.asFunction<
          int Function(ggml_backend_t, ggml_backend_graph_plan_t)>();

  int ggml_backend_graph_compute(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_backend_graph_compute(
      backend,
      cgraph,
    );
  }

  late final _ggml_backend_graph_computePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ggml_backend_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_graph_compute');
  late final _ggml_backend_graph_compute = _ggml_backend_graph_computePtr
      .asFunction<int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_backend_graph_compute_async(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> cgraph,
  ) {
    return _ggml_backend_graph_compute_async(
      backend,
      cgraph,
    );
  }

  late final _ggml_backend_graph_compute_asyncPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ggml_backend_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_graph_compute_async');
  late final _ggml_backend_graph_compute_async =
      _ggml_backend_graph_compute_asyncPtr
          .asFunction<int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  /// NOTE: will be removed, use device version instead
  bool ggml_backend_supports_op(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_supports_op(
      backend,
      op,
    );
  }

  late final _ggml_backend_supports_opPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_supports_op');
  late final _ggml_backend_supports_op = _ggml_backend_supports_opPtr
      .asFunction<bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>();

  bool ggml_backend_supports_buft(
    ggml_backend_t backend,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_supports_buft(
      backend,
      buft,
    );
  }

  late final _ggml_backend_supports_buftPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t,
              ggml_backend_buffer_type_t)>>('ggml_backend_supports_buft');
  late final _ggml_backend_supports_buft = _ggml_backend_supports_buftPtr
      .asFunction<bool Function(ggml_backend_t, ggml_backend_buffer_type_t)>();

  bool ggml_backend_offload_op(
    ggml_backend_t backend,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_offload_op(
      backend,
      op,
    );
  }

  late final _ggml_backend_offload_opPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_offload_op');
  late final _ggml_backend_offload_op = _ggml_backend_offload_opPtr
      .asFunction<bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>();

  /// asynchronous copy
  /// the copy is performed after all the currently queued operations in backend_src
  /// backend_dst will wait for the copy to complete before performing other operations
  /// automatic fallback to sync copy if async is not supported
  void ggml_backend_tensor_copy_async(
    ggml_backend_t backend_src,
    ggml_backend_t backend_dst,
    ffi.Pointer<ggml_tensor> src,
    ffi.Pointer<ggml_tensor> dst,
  ) {
    return _ggml_backend_tensor_copy_async(
      backend_src,
      backend_dst,
      src,
      dst,
    );
  }

  late final _ggml_backend_tensor_copy_asyncPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_backend_t,
              ggml_backend_t,
              ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_tensor_copy_async');
  late final _ggml_backend_tensor_copy_async =
      _ggml_backend_tensor_copy_asyncPtr.asFunction<
          void Function(ggml_backend_t, ggml_backend_t,
              ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)>();

  ggml_backend_dev_t ggml_backend_get_device(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_get_device(
      backend,
    );
  }

  late final _ggml_backend_get_devicePtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ggml_backend_t)>>(
          'ggml_backend_get_device');
  late final _ggml_backend_get_device = _ggml_backend_get_devicePtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_t)>();

  /// Events
  ggml_backend_event_t ggml_backend_event_new(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_event_new(
      device,
    );
  }

  late final _ggml_backend_event_newPtr = _lookup<
          ffi
          .NativeFunction<ggml_backend_event_t Function(ggml_backend_dev_t)>>(
      'ggml_backend_event_new');
  late final _ggml_backend_event_new = _ggml_backend_event_newPtr
      .asFunction<ggml_backend_event_t Function(ggml_backend_dev_t)>();

  void ggml_backend_event_free(
    ggml_backend_event_t event,
  ) {
    return _ggml_backend_event_free(
      event,
    );
  }

  late final _ggml_backend_event_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_event_t)>>(
          'ggml_backend_event_free');
  late final _ggml_backend_event_free = _ggml_backend_event_freePtr
      .asFunction<void Function(ggml_backend_event_t)>();

  void ggml_backend_event_record(
    ggml_backend_event_t event,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_event_record(
      event,
      backend,
    );
  }

  late final _ggml_backend_event_recordPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_event_t,
              ggml_backend_t)>>('ggml_backend_event_record');
  late final _ggml_backend_event_record = _ggml_backend_event_recordPtr
      .asFunction<void Function(ggml_backend_event_t, ggml_backend_t)>();

  void ggml_backend_event_synchronize(
    ggml_backend_event_t event,
  ) {
    return _ggml_backend_event_synchronize(
      event,
    );
  }

  late final _ggml_backend_event_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_event_t)>>(
          'ggml_backend_event_synchronize');
  late final _ggml_backend_event_synchronize =
      _ggml_backend_event_synchronizePtr
          .asFunction<void Function(ggml_backend_event_t)>();

  void ggml_backend_event_wait(
    ggml_backend_t backend,
    ggml_backend_event_t event,
  ) {
    return _ggml_backend_event_wait(
      backend,
      event,
    );
  }

  late final _ggml_backend_event_waitPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t,
              ggml_backend_event_t)>>('ggml_backend_event_wait');
  late final _ggml_backend_event_wait = _ggml_backend_event_waitPtr
      .asFunction<void Function(ggml_backend_t, ggml_backend_event_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_dev_name(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_name(
      device,
    );
  }

  late final _ggml_backend_dev_namePtr = _lookup<
          ffi
          .NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>>(
      'ggml_backend_dev_name');
  late final _ggml_backend_dev_name = _ggml_backend_dev_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>();

  ffi.Pointer<ffi.Char> ggml_backend_dev_description(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_description(
      device,
    );
  }

  late final _ggml_backend_dev_descriptionPtr = _lookup<
          ffi
          .NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>>(
      'ggml_backend_dev_description');
  late final _ggml_backend_dev_description = _ggml_backend_dev_descriptionPtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>();

  void ggml_backend_dev_memory(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Size> free,
    ffi.Pointer<ffi.Size> total,
  ) {
    return _ggml_backend_dev_memory(
      device,
      free,
      total,
    );
  }

  late final _ggml_backend_dev_memoryPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_dev_t, ffi.Pointer<ffi.Size>,
              ffi.Pointer<ffi.Size>)>>('ggml_backend_dev_memory');
  late final _ggml_backend_dev_memory = _ggml_backend_dev_memoryPtr.asFunction<
      void Function(
          ggml_backend_dev_t, ffi.Pointer<ffi.Size>, ffi.Pointer<ffi.Size>)>();

  int ggml_backend_dev_type1(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_type1(
      device,
    );
  }

  late final _ggml_backend_dev_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ggml_backend_dev_t)>>(
          'ggml_backend_dev_type');
  late final _ggml_backend_dev_type1 =
      _ggml_backend_dev_type1Ptr.asFunction<int Function(ggml_backend_dev_t)>();

  void ggml_backend_dev_get_props(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_backend_dev_props> props,
  ) {
    return _ggml_backend_dev_get_props(
      device,
      props,
    );
  }

  late final _ggml_backend_dev_get_propsPtr = _lookup<
          ffi.NativeFunction<
              ffi.Void Function(
                  ggml_backend_dev_t, ffi.Pointer<ggml_backend_dev_props>)>>(
      'ggml_backend_dev_get_props');
  late final _ggml_backend_dev_get_props =
      _ggml_backend_dev_get_propsPtr.asFunction<
          void Function(
              ggml_backend_dev_t, ffi.Pointer<ggml_backend_dev_props>)>();

  ggml_backend_reg_t ggml_backend_dev_backend_reg(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_backend_reg(
      device,
    );
  }

  late final _ggml_backend_dev_backend_regPtr = _lookup<
          ffi.NativeFunction<ggml_backend_reg_t Function(ggml_backend_dev_t)>>(
      'ggml_backend_dev_backend_reg');
  late final _ggml_backend_dev_backend_reg = _ggml_backend_dev_backend_regPtr
      .asFunction<ggml_backend_reg_t Function(ggml_backend_dev_t)>();

  ggml_backend_t ggml_backend_dev_init(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_dev_init(
      device,
      params,
    );
  }

  late final _ggml_backend_dev_initPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_t Function(ggml_backend_dev_t,
              ffi.Pointer<ffi.Char>)>>('ggml_backend_dev_init');
  late final _ggml_backend_dev_init = _ggml_backend_dev_initPtr.asFunction<
      ggml_backend_t Function(ggml_backend_dev_t, ffi.Pointer<ffi.Char>)>();

  ggml_backend_buffer_type_t ggml_backend_dev_buffer_type(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_buffer_type(
      device,
    );
  }

  late final _ggml_backend_dev_buffer_typePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(
              ggml_backend_dev_t)>>('ggml_backend_dev_buffer_type');
  late final _ggml_backend_dev_buffer_type = _ggml_backend_dev_buffer_typePtr
      .asFunction<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>();

  ggml_backend_buffer_type_t ggml_backend_dev_host_buffer_type(
    ggml_backend_dev_t device,
  ) {
    return _ggml_backend_dev_host_buffer_type(
      device,
    );
  }

  late final _ggml_backend_dev_host_buffer_typePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_type_t Function(
              ggml_backend_dev_t)>>('ggml_backend_dev_host_buffer_type');
  late final _ggml_backend_dev_host_buffer_type =
      _ggml_backend_dev_host_buffer_typePtr.asFunction<
          ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>();

  ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(
    ggml_backend_dev_t device,
    ffi.Pointer<ffi.Void> ptr,
    int size,
    int max_tensor_size,
  ) {
    return _ggml_backend_dev_buffer_from_host_ptr(
      device,
      ptr,
      size,
      max_tensor_size,
    );
  }

  late final _ggml_backend_dev_buffer_from_host_ptrPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_t Function(
              ggml_backend_dev_t,
              ffi.Pointer<ffi.Void>,
              ffi.Size,
              ffi.Size)>>('ggml_backend_dev_buffer_from_host_ptr');
  late final _ggml_backend_dev_buffer_from_host_ptr =
      _ggml_backend_dev_buffer_from_host_ptrPtr.asFunction<
          ggml_backend_buffer_t Function(
              ggml_backend_dev_t, ffi.Pointer<ffi.Void>, int, int)>();

  bool ggml_backend_dev_supports_op(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_dev_supports_op(
      device,
      op,
    );
  }

  late final _ggml_backend_dev_supports_opPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_dev_supports_op');
  late final _ggml_backend_dev_supports_op =
      _ggml_backend_dev_supports_opPtr.asFunction<
          bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>();

  bool ggml_backend_dev_supports_buft(
    ggml_backend_dev_t device,
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_dev_supports_buft(
      device,
      buft,
    );
  }

  late final _ggml_backend_dev_supports_buftPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t,
              ggml_backend_buffer_type_t)>>('ggml_backend_dev_supports_buft');
  late final _ggml_backend_dev_supports_buft =
      _ggml_backend_dev_supports_buftPtr.asFunction<
          bool Function(ggml_backend_dev_t, ggml_backend_buffer_type_t)>();

  bool ggml_backend_dev_offload_op(
    ggml_backend_dev_t device,
    ffi.Pointer<ggml_tensor> op,
  ) {
    return _ggml_backend_dev_offload_op(
      device,
      op,
    );
  }

  late final _ggml_backend_dev_offload_opPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_dev_t,
              ffi.Pointer<ggml_tensor>)>>('ggml_backend_dev_offload_op');
  late final _ggml_backend_dev_offload_op =
      _ggml_backend_dev_offload_opPtr.asFunction<
          bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>();

  /// Backend (reg)
  ffi.Pointer<ffi.Char> ggml_backend_reg_name(
    ggml_backend_reg_t reg,
  ) {
    return _ggml_backend_reg_name(
      reg,
    );
  }

  late final _ggml_backend_reg_namePtr = _lookup<
          ffi
          .NativeFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>>(
      'ggml_backend_reg_name');
  late final _ggml_backend_reg_name = _ggml_backend_reg_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>();

  int ggml_backend_reg_dev_count(
    ggml_backend_reg_t reg,
  ) {
    return _ggml_backend_reg_dev_count(
      reg,
    );
  }

  late final _ggml_backend_reg_dev_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function(ggml_backend_reg_t)>>(
          'ggml_backend_reg_dev_count');
  late final _ggml_backend_reg_dev_count = _ggml_backend_reg_dev_countPtr
      .asFunction<int Function(ggml_backend_reg_t)>();

  ggml_backend_dev_t ggml_backend_reg_dev_get(
    ggml_backend_reg_t reg,
    int index,
  ) {
    return _ggml_backend_reg_dev_get(
      reg,
      index,
    );
  }

  late final _ggml_backend_reg_dev_getPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_dev_t Function(
              ggml_backend_reg_t, ffi.Size)>>('ggml_backend_reg_dev_get');
  late final _ggml_backend_reg_dev_get = _ggml_backend_reg_dev_getPtr
      .asFunction<ggml_backend_dev_t Function(ggml_backend_reg_t, int)>();

  ffi.Pointer<ffi.Void> ggml_backend_reg_get_proc_address(
    ggml_backend_reg_t reg,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_backend_reg_get_proc_address(
      reg,
      name,
    );
  }

  late final _ggml_backend_reg_get_proc_addressPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Void> Function(ggml_backend_reg_t,
              ffi.Pointer<ffi.Char>)>>('ggml_backend_reg_get_proc_address');
  late final _ggml_backend_reg_get_proc_address =
      _ggml_backend_reg_get_proc_addressPtr.asFunction<
          ffi.Pointer<ffi.Void> Function(
              ggml_backend_reg_t, ffi.Pointer<ffi.Char>)>();

  /// Backend (reg) enumeration
  int ggml_backend_reg_count() {
    return _ggml_backend_reg_count();
  }

  late final _ggml_backend_reg_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>(
          'ggml_backend_reg_count');
  late final _ggml_backend_reg_count =
      _ggml_backend_reg_countPtr.asFunction<int Function()>();

  ggml_backend_reg_t ggml_backend_reg_get(
    int index,
  ) {
    return _ggml_backend_reg_get(
      index,
    );
  }

  late final _ggml_backend_reg_getPtr =
      _lookup<ffi.NativeFunction<ggml_backend_reg_t Function(ffi.Size)>>(
          'ggml_backend_reg_get');
  late final _ggml_backend_reg_get =
      _ggml_backend_reg_getPtr.asFunction<ggml_backend_reg_t Function(int)>();

  ggml_backend_reg_t ggml_backend_reg_by_name(
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_backend_reg_by_name(
      name,
    );
  }

  late final _ggml_backend_reg_by_namePtr = _lookup<
          ffi
          .NativeFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>>(
      'ggml_backend_reg_by_name');
  late final _ggml_backend_reg_by_name = _ggml_backend_reg_by_namePtr
      .asFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>();

  /// Device enumeration
  int ggml_backend_dev_count() {
    return _ggml_backend_dev_count();
  }

  late final _ggml_backend_dev_countPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>(
          'ggml_backend_dev_count');
  late final _ggml_backend_dev_count =
      _ggml_backend_dev_countPtr.asFunction<int Function()>();

  ggml_backend_dev_t ggml_backend_dev_get(
    int index,
  ) {
    return _ggml_backend_dev_get(
      index,
    );
  }

  late final _ggml_backend_dev_getPtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ffi.Size)>>(
          'ggml_backend_dev_get');
  late final _ggml_backend_dev_get =
      _ggml_backend_dev_getPtr.asFunction<ggml_backend_dev_t Function(int)>();

  ggml_backend_dev_t ggml_backend_dev_by_name(
    ffi.Pointer<ffi.Char> name,
  ) {
    return _ggml_backend_dev_by_name(
      name,
    );
  }

  late final _ggml_backend_dev_by_namePtr = _lookup<
          ffi
          .NativeFunction<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>>(
      'ggml_backend_dev_by_name');
  late final _ggml_backend_dev_by_name = _ggml_backend_dev_by_namePtr
      .asFunction<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>();

  ggml_backend_dev_t ggml_backend_dev_by_type(
    int type,
  ) {
    return _ggml_backend_dev_by_type(
      type,
    );
  }

  late final _ggml_backend_dev_by_typePtr =
      _lookup<ffi.NativeFunction<ggml_backend_dev_t Function(ffi.Int32)>>(
          'ggml_backend_dev_by_type');
  late final _ggml_backend_dev_by_type = _ggml_backend_dev_by_typePtr
      .asFunction<ggml_backend_dev_t Function(int)>();

  /// Direct backend (stream) initialization
  /// = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
  ggml_backend_t ggml_backend_init_by_name(
    ffi.Pointer<ffi.Char> name,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_init_by_name(
      name,
      params,
    );
  }

  late final _ggml_backend_init_by_namePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_t Function(ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>)>>('ggml_backend_init_by_name');
  late final _ggml_backend_init_by_name =
      _ggml_backend_init_by_namePtr.asFunction<
          ggml_backend_t Function(
              ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>();

  /// = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
  ggml_backend_t ggml_backend_init_by_type(
    int type,
    ffi.Pointer<ffi.Char> params,
  ) {
    return _ggml_backend_init_by_type(
      type,
      params,
    );
  }

  late final _ggml_backend_init_by_typePtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_t Function(
              ffi.Int32, ffi.Pointer<ffi.Char>)>>('ggml_backend_init_by_type');
  late final _ggml_backend_init_by_type = _ggml_backend_init_by_typePtr
      .asFunction<ggml_backend_t Function(int, ffi.Pointer<ffi.Char>)>();

  /// = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
  ggml_backend_t ggml_backend_init_best() {
    return _ggml_backend_init_best();
  }

  late final _ggml_backend_init_bestPtr =
      _lookup<ffi.NativeFunction<ggml_backend_t Function()>>(
          'ggml_backend_init_best');
  late final _ggml_backend_init_best =
      _ggml_backend_init_bestPtr.asFunction<ggml_backend_t Function()>();

  /// Load a backend from a dynamic library and register it
  ggml_backend_reg_t ggml_backend_load(
    ffi.Pointer<ffi.Char> path,
  ) {
    return _ggml_backend_load(
      path,
    );
  }

  late final _ggml_backend_loadPtr = _lookup<
          ffi
          .NativeFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>>(
      'ggml_backend_load');
  late final _ggml_backend_load = _ggml_backend_loadPtr
      .asFunction<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>();

  /// Unload a backend if loaded dynamically and unregister it
  void ggml_backend_unload(
    ggml_backend_reg_t reg,
  ) {
    return _ggml_backend_unload(
      reg,
    );
  }

  late final _ggml_backend_unloadPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_reg_t)>>(
          'ggml_backend_unload');
  late final _ggml_backend_unload =
      _ggml_backend_unloadPtr.asFunction<void Function(ggml_backend_reg_t)>();

  /// Load all known backends from dynamic libraries
  void ggml_backend_load_all() {
    return _ggml_backend_load_all();
  }

  late final _ggml_backend_load_allPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_backend_load_all');
  late final _ggml_backend_load_all =
      _ggml_backend_load_allPtr.asFunction<void Function()>();

  /// Initialize a backend scheduler, backends with low index are given priority over backends with high index
  ggml_backend_sched_t ggml_backend_sched_new(
    ffi.Pointer<ggml_backend_t> backends,
    ffi.Pointer<ggml_backend_buffer_type_t> bufts,
    int n_backends,
    int graph_size,
    bool parallel,
  ) {
    return _ggml_backend_sched_new(
      backends,
      bufts,
      n_backends,
      graph_size,
      parallel,
    );
  }

  late final _ggml_backend_sched_newPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_sched_t Function(
              ffi.Pointer<ggml_backend_t>,
              ffi.Pointer<ggml_backend_buffer_type_t>,
              ffi.Int,
              ffi.Size,
              ffi.Bool)>>('ggml_backend_sched_new');
  late final _ggml_backend_sched_new = _ggml_backend_sched_newPtr.asFunction<
      ggml_backend_sched_t Function(ffi.Pointer<ggml_backend_t>,
          ffi.Pointer<ggml_backend_buffer_type_t>, int, int, bool)>();

  void ggml_backend_sched_free(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_free(
      sched,
    );
  }

  late final _ggml_backend_sched_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_free');
  late final _ggml_backend_sched_free = _ggml_backend_sched_freePtr
      .asFunction<void Function(ggml_backend_sched_t)>();

  /// Initialize backend buffers from a measure graph
  bool ggml_backend_sched_reserve(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> measure_graph,
  ) {
    return _ggml_backend_sched_reserve(
      sched,
      measure_graph,
    );
  }

  late final _ggml_backend_sched_reservePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_sched_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_sched_reserve');
  late final _ggml_backend_sched_reserve =
      _ggml_backend_sched_reservePtr.asFunction<
          bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_backend_sched_get_n_backends(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_get_n_backends(
      sched,
    );
  }

  late final _ggml_backend_sched_get_n_backendsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_get_n_backends');
  late final _ggml_backend_sched_get_n_backends =
      _ggml_backend_sched_get_n_backendsPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  ggml_backend_t ggml_backend_sched_get_backend(
    ggml_backend_sched_t sched,
    int i,
  ) {
    return _ggml_backend_sched_get_backend(
      sched,
      i,
    );
  }

  late final _ggml_backend_sched_get_backendPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_t Function(ggml_backend_sched_t,
              ffi.Int)>>('ggml_backend_sched_get_backend');
  late final _ggml_backend_sched_get_backend =
      _ggml_backend_sched_get_backendPtr
          .asFunction<ggml_backend_t Function(ggml_backend_sched_t, int)>();

  /// Get the number of splits of the last graph
  int ggml_backend_sched_get_n_splits(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_get_n_splits(
      sched,
    );
  }

  late final _ggml_backend_sched_get_n_splitsPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_get_n_splits');
  late final _ggml_backend_sched_get_n_splits =
      _ggml_backend_sched_get_n_splitsPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  int ggml_backend_sched_get_n_copies(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_get_n_copies(
      sched,
    );
  }

  late final _ggml_backend_sched_get_n_copiesPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_get_n_copies');
  late final _ggml_backend_sched_get_n_copies =
      _ggml_backend_sched_get_n_copiesPtr
          .asFunction<int Function(ggml_backend_sched_t)>();

  int ggml_backend_sched_get_buffer_size(
    ggml_backend_sched_t sched,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_sched_get_buffer_size(
      sched,
      backend,
    );
  }

  late final _ggml_backend_sched_get_buffer_sizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ggml_backend_sched_t,
              ggml_backend_t)>>('ggml_backend_sched_get_buffer_size');
  late final _ggml_backend_sched_get_buffer_size =
      _ggml_backend_sched_get_buffer_sizePtr
          .asFunction<int Function(ggml_backend_sched_t, ggml_backend_t)>();

  void ggml_backend_sched_set_tensor_backend(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_tensor> node,
    ggml_backend_t backend,
  ) {
    return _ggml_backend_sched_set_tensor_backend(
      sched,
      node,
      backend,
    );
  }

  late final _ggml_backend_sched_set_tensor_backendPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_sched_t, ffi.Pointer<ggml_tensor>,
              ggml_backend_t)>>('ggml_backend_sched_set_tensor_backend');
  late final _ggml_backend_sched_set_tensor_backend =
      _ggml_backend_sched_set_tensor_backendPtr.asFunction<
          void Function(ggml_backend_sched_t, ffi.Pointer<ggml_tensor>,
              ggml_backend_t)>();

  ggml_backend_t ggml_backend_sched_get_tensor_backend(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_tensor> node,
  ) {
    return _ggml_backend_sched_get_tensor_backend(
      sched,
      node,
    );
  }

  late final _ggml_backend_sched_get_tensor_backendPtr = _lookup<
          ffi.NativeFunction<
              ggml_backend_t Function(
                  ggml_backend_sched_t, ffi.Pointer<ggml_tensor>)>>(
      'ggml_backend_sched_get_tensor_backend');
  late final _ggml_backend_sched_get_tensor_backend =
      _ggml_backend_sched_get_tensor_backendPtr.asFunction<
          ggml_backend_t Function(
              ggml_backend_sched_t, ffi.Pointer<ggml_tensor>)>();

  /// Allocate and compute graph on the backend scheduler
  bool ggml_backend_sched_alloc_graph(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_sched_alloc_graph(
      sched,
      graph,
    );
  }

  late final _ggml_backend_sched_alloc_graphPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ggml_backend_sched_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_sched_alloc_graph');
  late final _ggml_backend_sched_alloc_graph =
      _ggml_backend_sched_alloc_graphPtr.asFunction<
          bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_backend_sched_graph_compute(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_sched_graph_compute(
      sched,
      graph,
    );
  }

  late final _ggml_backend_sched_graph_computePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ggml_backend_sched_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_sched_graph_compute');
  late final _ggml_backend_sched_graph_compute =
      _ggml_backend_sched_graph_computePtr.asFunction<
          int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>();

  int ggml_backend_sched_graph_compute_async(
    ggml_backend_sched_t sched,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_sched_graph_compute_async(
      sched,
      graph,
    );
  }

  late final _ggml_backend_sched_graph_compute_asyncPtr = _lookup<
          ffi.NativeFunction<
              ffi.Int32 Function(
                  ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>>(
      'ggml_backend_sched_graph_compute_async');
  late final _ggml_backend_sched_graph_compute_async =
      _ggml_backend_sched_graph_compute_asyncPtr.asFunction<
          int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>();

  void ggml_backend_sched_synchronize(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_synchronize(
      sched,
    );
  }

  late final _ggml_backend_sched_synchronizePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_synchronize');
  late final _ggml_backend_sched_synchronize =
      _ggml_backend_sched_synchronizePtr
          .asFunction<void Function(ggml_backend_sched_t)>();

  /// Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
  /// This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
  /// The correct way to use this API is to discard the deallocated tensors and create new ones.
  void ggml_backend_sched_reset(
    ggml_backend_sched_t sched,
  ) {
    return _ggml_backend_sched_reset(
      sched,
    );
  }

  late final _ggml_backend_sched_resetPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_sched_t)>>(
          'ggml_backend_sched_reset');
  late final _ggml_backend_sched_reset = _ggml_backend_sched_resetPtr
      .asFunction<void Function(ggml_backend_sched_t)>();

  /// Set a callback to be called for each resulting node during graph compute
  void ggml_backend_sched_set_eval_callback(
    ggml_backend_sched_t sched,
    ggml_backend_sched_eval_callback callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _ggml_backend_sched_set_eval_callback(
      sched,
      callback,
      user_data,
    );
  }

  late final _ggml_backend_sched_set_eval_callbackPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_backend_sched_t,
              ggml_backend_sched_eval_callback,
              ffi.Pointer<ffi.Void>)>>('ggml_backend_sched_set_eval_callback');
  late final _ggml_backend_sched_set_eval_callback =
      _ggml_backend_sched_set_eval_callbackPtr.asFunction<
          void Function(ggml_backend_sched_t, ggml_backend_sched_eval_callback,
              ffi.Pointer<ffi.Void>)>();

  /// Copy a graph to a different backend
  ggml_backend_graph_copy ggml_backend_graph_copy1(
    ggml_backend_t backend,
    ffi.Pointer<ggml_cgraph> graph,
  ) {
    return _ggml_backend_graph_copy1(
      backend,
      graph,
    );
  }

  late final _ggml_backend_graph_copy1Ptr = _lookup<
      ffi.NativeFunction<
          ggml_backend_graph_copy Function(ggml_backend_t,
              ffi.Pointer<ggml_cgraph>)>>('ggml_backend_graph_copy');
  late final _ggml_backend_graph_copy1 =
      _ggml_backend_graph_copy1Ptr.asFunction<
          ggml_backend_graph_copy Function(
              ggml_backend_t, ffi.Pointer<ggml_cgraph>)>();

  void ggml_backend_graph_copy_free(
    ggml_backend_graph_copy copy,
  ) {
    return _ggml_backend_graph_copy_free(
      copy,
    );
  }

  late final _ggml_backend_graph_copy_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_graph_copy)>>(
          'ggml_backend_graph_copy_free');
  late final _ggml_backend_graph_copy_free = _ggml_backend_graph_copy_freePtr
      .asFunction<void Function(ggml_backend_graph_copy)>();

  /// Compare the output of two backends
  bool ggml_backend_compare_graph_backend(
    ggml_backend_t backend1,
    ggml_backend_t backend2,
    ffi.Pointer<ggml_cgraph> graph,
    ggml_backend_eval_callback callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _ggml_backend_compare_graph_backend(
      backend1,
      backend2,
      graph,
      callback,
      user_data,
    );
  }

  late final _ggml_backend_compare_graph_backendPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ggml_backend_t,
              ggml_backend_t,
              ffi.Pointer<ggml_cgraph>,
              ggml_backend_eval_callback,
              ffi.Pointer<ffi.Void>)>>('ggml_backend_compare_graph_backend');
  late final _ggml_backend_compare_graph_backend =
      _ggml_backend_compare_graph_backendPtr.asFunction<
          bool Function(
              ggml_backend_t,
              ggml_backend_t,
              ffi.Pointer<ggml_cgraph>,
              ggml_backend_eval_callback,
              ffi.Pointer<ffi.Void>)>();

  /// Tensor initialization
  void ggml_backend_tensor_alloc(
    ggml_backend_buffer_t buffer,
    ffi.Pointer<ggml_tensor> tensor,
    ffi.Pointer<ffi.Void> addr,
  ) {
    return _ggml_backend_tensor_alloc(
      buffer,
      tensor,
      addr,
    );
  }

  late final _ggml_backend_tensor_allocPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>)>>('ggml_backend_tensor_alloc');
  late final _ggml_backend_tensor_alloc =
      _ggml_backend_tensor_allocPtr.asFunction<
          void Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>,
              ffi.Pointer<ffi.Void>)>();

  void ggml_backend_view_init(
    ffi.Pointer<ggml_tensor> tensor,
  ) {
    return _ggml_backend_view_init(
      tensor,
    );
  }

  late final _ggml_backend_view_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_tensor>)>>(
          'ggml_backend_view_init');
  late final _ggml_backend_view_init = _ggml_backend_view_initPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>)>();

  /// CPU buffer types are always available
  ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(
    ffi.Pointer<ffi.Void> ptr,
    int size,
  ) {
    return _ggml_backend_cpu_buffer_from_ptr(
      ptr,
      size,
    );
  }

  late final _ggml_backend_cpu_buffer_from_ptrPtr = _lookup<
      ffi.NativeFunction<
          ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>,
              ffi.Size)>>('ggml_backend_cpu_buffer_from_ptr');
  late final _ggml_backend_cpu_buffer_from_ptr =
      _ggml_backend_cpu_buffer_from_ptrPtr.asFunction<
          ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>, int)>();

  ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type() {
    return _ggml_backend_cpu_buffer_type();
  }

  late final _ggml_backend_cpu_buffer_typePtr =
      _lookup<ffi.NativeFunction<ggml_backend_buffer_type_t Function()>>(
          'ggml_backend_cpu_buffer_type');
  late final _ggml_backend_cpu_buffer_type = _ggml_backend_cpu_buffer_typePtr
      .asFunction<ggml_backend_buffer_type_t Function()>();

  void ggml_numa_init(
    int numa,
  ) {
    return _ggml_numa_init(
      numa,
    );
  }

  late final _ggml_numa_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Int32)>>(
          'ggml_numa_init');
  late final _ggml_numa_init =
      _ggml_numa_initPtr.asFunction<void Function(int)>();

  bool ggml_is_numa() {
    return _ggml_is_numa();
  }

  late final _ggml_is_numaPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('ggml_is_numa');
  late final _ggml_is_numa = _ggml_is_numaPtr.asFunction<bool Function()>();

  ffi.Pointer<ggml_tensor> ggml_new_i32(
    ffi.Pointer<ggml_context> ctx,
    int value,
  ) {
    return _ggml_new_i32(
      ctx,
      value,
    );
  }

  late final _ggml_new_i32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>, ffi.Int32)>>('ggml_new_i32');
  late final _ggml_new_i32 = _ggml_new_i32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, int)>();

  ffi.Pointer<ggml_tensor> ggml_new_f32(
    ffi.Pointer<ggml_context> ctx,
    double value,
  ) {
    return _ggml_new_f32(
      ctx,
      value,
    );
  }

  late final _ggml_new_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_context>, ffi.Float)>>('ggml_new_f32');
  late final _ggml_new_f32 = _ggml_new_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, double)>();

  ffi.Pointer<ggml_tensor> ggml_set_i32(
    ffi.Pointer<ggml_tensor> tensor,
    int value,
  ) {
    return _ggml_set_i32(
      tensor,
      value,
    );
  }

  late final _ggml_set_i32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_tensor>, ffi.Int32)>>('ggml_set_i32');
  late final _ggml_set_i32 = _ggml_set_i32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, int)>();

  ffi.Pointer<ggml_tensor> ggml_set_f32(
    ffi.Pointer<ggml_tensor> tensor,
    double value,
  ) {
    return _ggml_set_f32(
      tensor,
      value,
    );
  }

  late final _ggml_set_f32Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(
              ffi.Pointer<ggml_tensor>, ffi.Float)>>('ggml_set_f32');
  late final _ggml_set_f32 = _ggml_set_f32Ptr.asFunction<
      ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, double)>();

  int ggml_get_i32_1d(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
  ) {
    return _ggml_get_i32_1d(
      tensor,
      i,
    );
  }

  late final _ggml_get_i32_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_get_i32_1d');
  late final _ggml_get_i32_1d = _ggml_get_i32_1dPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>, int)>();

  void ggml_set_i32_1d(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
    int value,
  ) {
    return _ggml_set_i32_1d(
      tensor,
      i,
      value,
    );
  }

  late final _ggml_set_i32_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int,
              ffi.Int32)>>('ggml_set_i32_1d');
  late final _ggml_set_i32_1d = _ggml_set_i32_1dPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, int)>();

  int ggml_get_i32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
  ) {
    return _ggml_get_i32_nd(
      tensor,
      i0,
      i1,
      i2,
      i3,
    );
  }

  late final _ggml_get_i32_ndPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int,
              ffi.Int, ffi.Int)>>('ggml_get_i32_nd');
  late final _ggml_get_i32_nd = _ggml_get_i32_ndPtr
      .asFunction<int Function(ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  void ggml_set_i32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
    int value,
  ) {
    return _ggml_set_i32_nd(
      tensor,
      i0,
      i1,
      i2,
      i3,
      value,
    );
  }

  late final _ggml_set_i32_ndPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int,
              ffi.Int, ffi.Int32)>>('ggml_set_i32_nd');
  late final _ggml_set_i32_nd = _ggml_set_i32_ndPtr.asFunction<
      void Function(ffi.Pointer<ggml_tensor>, int, int, int, int, int)>();

  double ggml_get_f32_1d(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
  ) {
    return _ggml_get_f32_1d(
      tensor,
      i,
    );
  }

  late final _ggml_get_f32_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Float Function(
              ffi.Pointer<ggml_tensor>, ffi.Int)>>('ggml_get_f32_1d');
  late final _ggml_get_f32_1d = _ggml_get_f32_1dPtr
      .asFunction<double Function(ffi.Pointer<ggml_tensor>, int)>();

  void ggml_set_f32_1d(
    ffi.Pointer<ggml_tensor> tensor,
    int i,
    double value,
  ) {
    return _ggml_set_f32_1d(
      tensor,
      i,
      value,
    );
  }

  late final _ggml_set_f32_1dPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int,
              ffi.Float)>>('ggml_set_f32_1d');
  late final _ggml_set_f32_1d = _ggml_set_f32_1dPtr
      .asFunction<void Function(ffi.Pointer<ggml_tensor>, int, double)>();

  double ggml_get_f32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
  ) {
    return _ggml_get_f32_nd(
      tensor,
      i0,
      i1,
      i2,
      i3,
    );
  }

  late final _ggml_get_f32_ndPtr = _lookup<
      ffi.NativeFunction<
          ffi.Float Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int,
              ffi.Int, ffi.Int)>>('ggml_get_f32_nd');
  late final _ggml_get_f32_nd = _ggml_get_f32_ndPtr.asFunction<
      double Function(ffi.Pointer<ggml_tensor>, int, int, int, int)>();

  void ggml_set_f32_nd(
    ffi.Pointer<ggml_tensor> tensor,
    int i0,
    int i1,
    int i2,
    int i3,
    double value,
  ) {
    return _ggml_set_f32_nd(
      tensor,
      i0,
      i1,
      i2,
      i3,
      value,
    );
  }

  late final _ggml_set_f32_ndPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int, ffi.Int,
              ffi.Int, ffi.Float)>>('ggml_set_f32_nd');
  late final _ggml_set_f32_nd = _ggml_set_f32_ndPtr.asFunction<
      void Function(ffi.Pointer<ggml_tensor>, int, int, int, int, double)>();

  ffi.Pointer<ggml_threadpool> ggml_threadpool_new(
    ffi.Pointer<ggml_threadpool_params> params,
  ) {
    return _ggml_threadpool_new(
      params,
    );
  }

  late final _ggml_threadpool_newPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_threadpool> Function(
              ffi.Pointer<ggml_threadpool_params>)>>('ggml_threadpool_new');
  late final _ggml_threadpool_new = _ggml_threadpool_newPtr.asFunction<
      ffi.Pointer<ggml_threadpool> Function(
          ffi.Pointer<ggml_threadpool_params>)>();

  void ggml_threadpool_free(
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_threadpool_free(
      threadpool,
    );
  }

  late final _ggml_threadpool_freePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>>(
      'ggml_threadpool_free');
  late final _ggml_threadpool_free = _ggml_threadpool_freePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  int ggml_threadpool_get_n_threads(
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_threadpool_get_n_threads(
      threadpool,
    );
  }

  late final _ggml_threadpool_get_n_threadsPtr = _lookup<
          ffi.NativeFunction<ffi.Int Function(ffi.Pointer<ggml_threadpool>)>>(
      'ggml_threadpool_get_n_threads');
  late final _ggml_threadpool_get_n_threads = _ggml_threadpool_get_n_threadsPtr
      .asFunction<int Function(ffi.Pointer<ggml_threadpool>)>();

  void ggml_threadpool_pause(
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_threadpool_pause(
      threadpool,
    );
  }

  late final _ggml_threadpool_pausePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>>(
      'ggml_threadpool_pause');
  late final _ggml_threadpool_pause = _ggml_threadpool_pausePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  void ggml_threadpool_resume(
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_threadpool_resume(
      threadpool,
    );
  }

  late final _ggml_threadpool_resumePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>>(
      'ggml_threadpool_resume');
  late final _ggml_threadpool_resume = _ggml_threadpool_resumePtr
      .asFunction<void Function(ffi.Pointer<ggml_threadpool>)>();

  /// ggml_graph_plan() has to be called before ggml_graph_compute()
  /// when plan.work_size > 0, caller must allocate memory for plan.work_data
  ggml_cplan ggml_graph_plan(
    ffi.Pointer<ggml_cgraph> cgraph,
    int n_threads,
    ffi.Pointer<ggml_threadpool> threadpool,
  ) {
    return _ggml_graph_plan(
      cgraph,
      n_threads,
      threadpool,
    );
  }

  late final _ggml_graph_planPtr = _lookup<
      ffi.NativeFunction<
          ggml_cplan Function(ffi.Pointer<ggml_cgraph>, ffi.Int,
              ffi.Pointer<ggml_threadpool>)>>('ggml_graph_plan');
  late final _ggml_graph_plan = _ggml_graph_planPtr.asFunction<
      ggml_cplan Function(
          ffi.Pointer<ggml_cgraph>, int, ffi.Pointer<ggml_threadpool>)>();

  int ggml_graph_compute(
    ffi.Pointer<ggml_cgraph> cgraph,
    ffi.Pointer<ggml_cplan> cplan,
  ) {
    return _ggml_graph_compute(
      cgraph,
      cplan,
    );
  }

  late final _ggml_graph_computePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ggml_cgraph>,
              ffi.Pointer<ggml_cplan>)>>('ggml_graph_compute');
  late final _ggml_graph_compute = _ggml_graph_computePtr.asFunction<
      int Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cplan>)>();

  /// same as ggml_graph_compute() but the work data is allocated as a part of the context
  /// note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
  int ggml_graph_compute_with_ctx(
    ffi.Pointer<ggml_context> ctx,
    ffi.Pointer<ggml_cgraph> cgraph,
    int n_threads,
  ) {
    return _ggml_graph_compute_with_ctx(
      ctx,
      cgraph,
      n_threads,
    );
  }

  late final _ggml_graph_compute_with_ctxPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<ggml_context>,
              ffi.Pointer<ggml_cgraph>,
              ffi.Int)>>('ggml_graph_compute_with_ctx');
  late final _ggml_graph_compute_with_ctx =
      _ggml_graph_compute_with_ctxPtr.asFunction<
          int Function(
              ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>, int)>();

  /// x86
  int ggml_cpu_has_sse3() {
    return _ggml_cpu_has_sse3();
  }

  late final _ggml_cpu_has_sse3Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_sse3');
  late final _ggml_cpu_has_sse3 =
      _ggml_cpu_has_sse3Ptr.asFunction<int Function()>();

  int ggml_cpu_has_ssse3() {
    return _ggml_cpu_has_ssse3();
  }

  late final _ggml_cpu_has_ssse3Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_ssse3');
  late final _ggml_cpu_has_ssse3 =
      _ggml_cpu_has_ssse3Ptr.asFunction<int Function()>();

  int ggml_cpu_has_avx() {
    return _ggml_cpu_has_avx();
  }

  late final _ggml_cpu_has_avxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx');
  late final _ggml_cpu_has_avx =
      _ggml_cpu_has_avxPtr.asFunction<int Function()>();

  int ggml_cpu_has_avx_vnni() {
    return _ggml_cpu_has_avx_vnni();
  }

  late final _ggml_cpu_has_avx_vnniPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx_vnni');
  late final _ggml_cpu_has_avx_vnni =
      _ggml_cpu_has_avx_vnniPtr.asFunction<int Function()>();

  int ggml_cpu_has_avx2() {
    return _ggml_cpu_has_avx2();
  }

  late final _ggml_cpu_has_avx2Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx2');
  late final _ggml_cpu_has_avx2 =
      _ggml_cpu_has_avx2Ptr.asFunction<int Function()>();

  int ggml_cpu_has_f16c() {
    return _ggml_cpu_has_f16c();
  }

  late final _ggml_cpu_has_f16cPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_f16c');
  late final _ggml_cpu_has_f16c =
      _ggml_cpu_has_f16cPtr.asFunction<int Function()>();

  int ggml_cpu_has_fma() {
    return _ggml_cpu_has_fma();
  }

  late final _ggml_cpu_has_fmaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_fma');
  late final _ggml_cpu_has_fma =
      _ggml_cpu_has_fmaPtr.asFunction<int Function()>();

  int ggml_cpu_has_avx512() {
    return _ggml_cpu_has_avx512();
  }

  late final _ggml_cpu_has_avx512Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_avx512');
  late final _ggml_cpu_has_avx512 =
      _ggml_cpu_has_avx512Ptr.asFunction<int Function()>();

  int ggml_cpu_has_avx512_vbmi() {
    return _ggml_cpu_has_avx512_vbmi();
  }

  late final _ggml_cpu_has_avx512_vbmiPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
          'ggml_cpu_has_avx512_vbmi');
  late final _ggml_cpu_has_avx512_vbmi =
      _ggml_cpu_has_avx512_vbmiPtr.asFunction<int Function()>();

  int ggml_cpu_has_avx512_vnni() {
    return _ggml_cpu_has_avx512_vnni();
  }

  late final _ggml_cpu_has_avx512_vnniPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
          'ggml_cpu_has_avx512_vnni');
  late final _ggml_cpu_has_avx512_vnni =
      _ggml_cpu_has_avx512_vnniPtr.asFunction<int Function()>();

  int ggml_cpu_has_avx512_bf16() {
    return _ggml_cpu_has_avx512_bf16();
  }

  late final _ggml_cpu_has_avx512_bf16Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
          'ggml_cpu_has_avx512_bf16');
  late final _ggml_cpu_has_avx512_bf16 =
      _ggml_cpu_has_avx512_bf16Ptr.asFunction<int Function()>();

  int ggml_cpu_has_amx_int8() {
    return _ggml_cpu_has_amx_int8();
  }

  late final _ggml_cpu_has_amx_int8Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_amx_int8');
  late final _ggml_cpu_has_amx_int8 =
      _ggml_cpu_has_amx_int8Ptr.asFunction<int Function()>();

  /// ARM
  int ggml_cpu_has_neon() {
    return _ggml_cpu_has_neon();
  }

  late final _ggml_cpu_has_neonPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_neon');
  late final _ggml_cpu_has_neon =
      _ggml_cpu_has_neonPtr.asFunction<int Function()>();

  int ggml_cpu_has_arm_fma() {
    return _ggml_cpu_has_arm_fma();
  }

  late final _ggml_cpu_has_arm_fmaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_arm_fma');
  late final _ggml_cpu_has_arm_fma =
      _ggml_cpu_has_arm_fmaPtr.asFunction<int Function()>();

  int ggml_cpu_has_fp16_va() {
    return _ggml_cpu_has_fp16_va();
  }

  late final _ggml_cpu_has_fp16_vaPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_fp16_va');
  late final _ggml_cpu_has_fp16_va =
      _ggml_cpu_has_fp16_vaPtr.asFunction<int Function()>();

  int ggml_cpu_has_dotprod() {
    return _ggml_cpu_has_dotprod();
  }

  late final _ggml_cpu_has_dotprodPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_dotprod');
  late final _ggml_cpu_has_dotprod =
      _ggml_cpu_has_dotprodPtr.asFunction<int Function()>();

  int ggml_cpu_has_matmul_int8() {
    return _ggml_cpu_has_matmul_int8();
  }

  late final _ggml_cpu_has_matmul_int8Ptr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>(
          'ggml_cpu_has_matmul_int8');
  late final _ggml_cpu_has_matmul_int8 =
      _ggml_cpu_has_matmul_int8Ptr.asFunction<int Function()>();

  int ggml_cpu_has_sve() {
    return _ggml_cpu_has_sve();
  }

  late final _ggml_cpu_has_svePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_sve');
  late final _ggml_cpu_has_sve =
      _ggml_cpu_has_svePtr.asFunction<int Function()>();

  int ggml_cpu_get_sve_cnt() {
    return _ggml_cpu_get_sve_cnt();
  }

  late final _ggml_cpu_get_sve_cntPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_get_sve_cnt');
  late final _ggml_cpu_get_sve_cnt =
      _ggml_cpu_get_sve_cntPtr.asFunction<int Function()>();

  /// other
  int ggml_cpu_has_riscv_v() {
    return _ggml_cpu_has_riscv_v();
  }

  late final _ggml_cpu_has_riscv_vPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_riscv_v');
  late final _ggml_cpu_has_riscv_v =
      _ggml_cpu_has_riscv_vPtr.asFunction<int Function()>();

  int ggml_cpu_has_vsx() {
    return _ggml_cpu_has_vsx();
  }

  late final _ggml_cpu_has_vsxPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_vsx');
  late final _ggml_cpu_has_vsx =
      _ggml_cpu_has_vsxPtr.asFunction<int Function()>();

  int ggml_cpu_has_wasm_simd() {
    return _ggml_cpu_has_wasm_simd();
  }

  late final _ggml_cpu_has_wasm_simdPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_wasm_simd');
  late final _ggml_cpu_has_wasm_simd =
      _ggml_cpu_has_wasm_simdPtr.asFunction<int Function()>();

  int ggml_cpu_has_llamafile() {
    return _ggml_cpu_has_llamafile();
  }

  late final _ggml_cpu_has_llamafilePtr =
      _lookup<ffi.NativeFunction<ffi.Int Function()>>('ggml_cpu_has_llamafile');
  late final _ggml_cpu_has_llamafile =
      _ggml_cpu_has_llamafilePtr.asFunction<int Function()>();

  ffi.Pointer<ggml_type_traits_cpu> ggml_get_type_traits_cpu(
    int type,
  ) {
    return _ggml_get_type_traits_cpu(
      type,
    );
  }

  late final _ggml_get_type_traits_cpuPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_type_traits_cpu> Function(
              ffi.Int32)>>('ggml_get_type_traits_cpu');
  late final _ggml_get_type_traits_cpu = _ggml_get_type_traits_cpuPtr
      .asFunction<ffi.Pointer<ggml_type_traits_cpu> Function(int)>();

  void ggml_cpu_init() {
    return _ggml_cpu_init();
  }

  late final _ggml_cpu_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('ggml_cpu_init');
  late final _ggml_cpu_init = _ggml_cpu_initPtr.asFunction<void Function()>();

  /// CPU backend
  ggml_backend_t ggml_backend_cpu_init() {
    return _ggml_backend_cpu_init();
  }

  late final _ggml_backend_cpu_initPtr =
      _lookup<ffi.NativeFunction<ggml_backend_t Function()>>(
          'ggml_backend_cpu_init');
  late final _ggml_backend_cpu_init =
      _ggml_backend_cpu_initPtr.asFunction<ggml_backend_t Function()>();

  bool ggml_backend_is_cpu(
    ggml_backend_t backend,
  ) {
    return _ggml_backend_is_cpu(
      backend,
    );
  }

  late final _ggml_backend_is_cpuPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ggml_backend_t)>>(
          'ggml_backend_is_cpu');
  late final _ggml_backend_is_cpu =
      _ggml_backend_is_cpuPtr.asFunction<bool Function(ggml_backend_t)>();

  void ggml_backend_cpu_set_n_threads(
    ggml_backend_t backend_cpu,
    int n_threads,
  ) {
    return _ggml_backend_cpu_set_n_threads(
      backend_cpu,
      n_threads,
    );
  }

  late final _ggml_backend_cpu_set_n_threadsPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ggml_backend_t, ffi.Int)>>(
          'ggml_backend_cpu_set_n_threads');
  late final _ggml_backend_cpu_set_n_threads =
      _ggml_backend_cpu_set_n_threadsPtr
          .asFunction<void Function(ggml_backend_t, int)>();

  void ggml_backend_cpu_set_threadpool(
    ggml_backend_t backend_cpu,
    ggml_threadpool_t threadpool,
  ) {
    return _ggml_backend_cpu_set_threadpool(
      backend_cpu,
      threadpool,
    );
  }

  late final _ggml_backend_cpu_set_threadpoolPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t,
              ggml_threadpool_t)>>('ggml_backend_cpu_set_threadpool');
  late final _ggml_backend_cpu_set_threadpool =
      _ggml_backend_cpu_set_threadpoolPtr
          .asFunction<void Function(ggml_backend_t, ggml_threadpool_t)>();

  void ggml_backend_cpu_set_abort_callback(
    ggml_backend_t backend_cpu,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data,
  ) {
    return _ggml_backend_cpu_set_abort_callback(
      backend_cpu,
      abort_callback,
      abort_callback_data,
    );
  }

  late final _ggml_backend_cpu_set_abort_callbackPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ggml_backend_t, ggml_abort_callback,
              ffi.Pointer<ffi.Void>)>>('ggml_backend_cpu_set_abort_callback');
  late final _ggml_backend_cpu_set_abort_callback =
      _ggml_backend_cpu_set_abort_callbackPtr.asFunction<
          void Function(
              ggml_backend_t, ggml_abort_callback, ffi.Pointer<ffi.Void>)>();

  ggml_backend_reg_t ggml_backend_cpu_reg() {
    return _ggml_backend_cpu_reg();
  }

  late final _ggml_backend_cpu_regPtr =
      _lookup<ffi.NativeFunction<ggml_backend_reg_t Function()>>(
          'ggml_backend_cpu_reg');
  late final _ggml_backend_cpu_reg =
      _ggml_backend_cpu_regPtr.asFunction<ggml_backend_reg_t Function()>();

  ggml_backend_buffer_type_t ggml_backend_cpu_aarch64_buffer_type() {
    return _ggml_backend_cpu_aarch64_buffer_type();
  }

  late final _ggml_backend_cpu_aarch64_buffer_typePtr =
      _lookup<ffi.NativeFunction<ggml_backend_buffer_type_t Function()>>(
          'ggml_backend_cpu_aarch64_buffer_type');
  late final _ggml_backend_cpu_aarch64_buffer_type =
      _ggml_backend_cpu_aarch64_buffer_typePtr
          .asFunction<ggml_backend_buffer_type_t Function()>();

  bool ggml_backend_cpu_buft_is_aarch64(
    ggml_backend_buffer_type_t buft,
  ) {
    return _ggml_backend_cpu_buft_is_aarch64(
      buft,
    );
  }

  late final _ggml_backend_cpu_buft_is_aarch64Ptr = _lookup<
          ffi.NativeFunction<ffi.Bool Function(ggml_backend_buffer_type_t)>>(
      'ggml_backend_cpu_buft_is_aarch64');
  late final _ggml_backend_cpu_buft_is_aarch64 =
      _ggml_backend_cpu_buft_is_aarch64Ptr
          .asFunction<bool Function(ggml_backend_buffer_type_t)>();

  /// Helpers for getting default parameters
  /// TODO: update API to start accepting pointers to params structs (https://github.com/ggerganov/llama.cpp/discussions/9172)
  llama_model_params llama_model_default_params() {
    return _llama_model_default_params();
  }

  late final _llama_model_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_params Function()>>(
          'llama_model_default_params');
  late final _llama_model_default_params = _llama_model_default_paramsPtr
      .asFunction<llama_model_params Function()>();

  llama_context_params llama_context_default_params() {
    return _llama_context_default_params();
  }

  late final _llama_context_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_context_params Function()>>(
          'llama_context_default_params');
  late final _llama_context_default_params = _llama_context_default_paramsPtr
      .asFunction<llama_context_params Function()>();

  llama_sampler_chain_params llama_sampler_chain_default_params() {
    return _llama_sampler_chain_default_params();
  }

  late final _llama_sampler_chain_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_sampler_chain_params Function()>>(
          'llama_sampler_chain_default_params');
  late final _llama_sampler_chain_default_params =
      _llama_sampler_chain_default_paramsPtr
          .asFunction<llama_sampler_chain_params Function()>();

  llama_model_quantize_params llama_model_quantize_default_params() {
    return _llama_model_quantize_default_params();
  }

  late final _llama_model_quantize_default_paramsPtr =
      _lookup<ffi.NativeFunction<llama_model_quantize_params Function()>>(
          'llama_model_quantize_default_params');
  late final _llama_model_quantize_default_params =
      _llama_model_quantize_default_paramsPtr
          .asFunction<llama_model_quantize_params Function()>();

  /// Initialize the llama + ggml backend
  /// If numa is true, use NUMA optimizations
  /// Call once at the start of the program
  void llama_backend_init() {
    return _llama_backend_init();
  }

  late final _llama_backend_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_init');
  late final _llama_backend_init =
      _llama_backend_initPtr.asFunction<void Function()>();

  /// optional:
  void llama_numa_init(
    int numa,
  ) {
    return _llama_numa_init(
      numa,
    );
  }

  late final _llama_numa_initPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Int32)>>(
          'llama_numa_init');
  late final _llama_numa_init =
      _llama_numa_initPtr.asFunction<void Function(int)>();

  /// Optional: an auto threadpool gets created in ggml if not passed explicitly
  void llama_attach_threadpool(
    ffi.Pointer<llama_context> ctx,
    ggml_threadpool_t threadpool,
    ggml_threadpool_t threadpool_batch,
  ) {
    return _llama_attach_threadpool(
      ctx,
      threadpool,
      threadpool_batch,
    );
  }

  late final _llama_attach_threadpoolPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ggml_threadpool_t,
              ggml_threadpool_t)>>('llama_attach_threadpool');
  late final _llama_attach_threadpool = _llama_attach_threadpoolPtr.asFunction<
      void Function(
          ffi.Pointer<llama_context>, ggml_threadpool_t, ggml_threadpool_t)>();

  void llama_detach_threadpool(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_detach_threadpool(
      ctx,
    );
  }

  late final _llama_detach_threadpoolPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_detach_threadpool');
  late final _llama_detach_threadpool = _llama_detach_threadpoolPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Call once at the end of the program - currently only used for MPI
  void llama_backend_free() {
    return _llama_backend_free();
  }

  late final _llama_backend_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function()>>('llama_backend_free');
  late final _llama_backend_free =
      _llama_backend_freePtr.asFunction<void Function()>();

  ffi.Pointer<llama_model> llama_load_model_from_file(
    ffi.Pointer<ffi.Char> path_model,
    llama_model_params params,
  ) {
    return _llama_load_model_from_file(
      path_model,
      params,
    );
  }

  late final _llama_load_model_from_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>,
              llama_model_params)>>('llama_load_model_from_file');
  late final _llama_load_model_from_file =
      _llama_load_model_from_filePtr.asFunction<
          ffi.Pointer<llama_model> Function(
              ffi.Pointer<ffi.Char>, llama_model_params)>();

  void llama_free_model(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_free_model(
      model,
    );
  }

  late final _llama_free_modelPtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_model>)>>(
          'llama_free_model');
  late final _llama_free_model = _llama_free_modelPtr
      .asFunction<void Function(ffi.Pointer<llama_model>)>();

  /// TODO: rename to llama_init_from_model
  ffi.Pointer<llama_context> llama_new_context_with_model(
    ffi.Pointer<llama_model> model,
    llama_context_params params,
  ) {
    return _llama_new_context_with_model(
      model,
      params,
    );
  }

  late final _llama_new_context_with_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_context> Function(ffi.Pointer<llama_model>,
              llama_context_params)>>('llama_new_context_with_model');
  late final _llama_new_context_with_model =
      _llama_new_context_with_modelPtr.asFunction<
          ffi.Pointer<llama_context> Function(
              ffi.Pointer<llama_model>, llama_context_params)>();

  /// Frees all allocated memory
  void llama_free(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_free(
      ctx,
    );
  }

  late final _llama_freePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_free');
  late final _llama_free =
      _llama_freePtr.asFunction<void Function(ffi.Pointer<llama_context>)>();

  int llama_time_us() {
    return _llama_time_us();
  }

  late final _llama_time_usPtr =
      _lookup<ffi.NativeFunction<ffi.Int64 Function()>>('llama_time_us');
  late final _llama_time_us = _llama_time_usPtr.asFunction<int Function()>();

  int llama_max_devices() {
    return _llama_max_devices();
  }

  late final _llama_max_devicesPtr =
      _lookup<ffi.NativeFunction<ffi.Size Function()>>('llama_max_devices');
  late final _llama_max_devices =
      _llama_max_devicesPtr.asFunction<int Function()>();

  bool llama_supports_mmap() {
    return _llama_supports_mmap();
  }

  late final _llama_supports_mmapPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mmap');
  late final _llama_supports_mmap =
      _llama_supports_mmapPtr.asFunction<bool Function()>();

  bool llama_supports_mlock() {
    return _llama_supports_mlock();
  }

  late final _llama_supports_mlockPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_mlock');
  late final _llama_supports_mlock =
      _llama_supports_mlockPtr.asFunction<bool Function()>();

  bool llama_supports_gpu_offload() {
    return _llama_supports_gpu_offload();
  }

  late final _llama_supports_gpu_offloadPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>(
          'llama_supports_gpu_offload');
  late final _llama_supports_gpu_offload =
      _llama_supports_gpu_offloadPtr.asFunction<bool Function()>();

  bool llama_supports_rpc() {
    return _llama_supports_rpc();
  }

  late final _llama_supports_rpcPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function()>>('llama_supports_rpc');
  late final _llama_supports_rpc =
      _llama_supports_rpcPtr.asFunction<bool Function()>();

  int llama_n_ctx(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_ctx(
      ctx,
    );
  }

  late final _llama_n_ctxPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_ctx');
  late final _llama_n_ctx =
      _llama_n_ctxPtr.asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_batch(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_batch(
      ctx,
    );
  }

  late final _llama_n_batchPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_batch');
  late final _llama_n_batch =
      _llama_n_batchPtr.asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_ubatch(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_ubatch(
      ctx,
    );
  }

  late final _llama_n_ubatchPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_ubatch');
  late final _llama_n_ubatch =
      _llama_n_ubatchPtr.asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_seq_max(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_seq_max(
      ctx,
    );
  }

  late final _llama_n_seq_maxPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_seq_max');
  late final _llama_n_seq_max = _llama_n_seq_maxPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_n_vocab(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_vocab(
      model,
    );
  }

  late final _llama_n_vocabPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_vocab');
  late final _llama_n_vocab =
      _llama_n_vocabPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_ctx_train(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_ctx_train(
      model,
    );
  }

  late final _llama_n_ctx_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_ctx_train');
  late final _llama_n_ctx_train = _llama_n_ctx_trainPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_embd(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_embd(
      model,
    );
  }

  late final _llama_n_embdPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_embd');
  late final _llama_n_embd =
      _llama_n_embdPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_layer(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_layer(
      model,
    );
  }

  late final _llama_n_layerPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_layer');
  late final _llama_n_layer =
      _llama_n_layerPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_n_head(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_n_head(
      model,
    );
  }

  late final _llama_n_headPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_n_head');
  late final _llama_n_head =
      _llama_n_headPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  ffi.Pointer<llama_model> llama_get_model(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_model(
      ctx,
    );
  }

  late final _llama_get_modelPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_model> Function(
              ffi.Pointer<llama_context>)>>('llama_get_model');
  late final _llama_get_model = _llama_get_modelPtr.asFunction<
      ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)>();

  int llama_pooling_type1(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_pooling_type1(
      ctx,
    );
  }

  late final _llama_pooling_type1Ptr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_pooling_type');
  late final _llama_pooling_type1 = _llama_pooling_type1Ptr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_vocab_type1(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_vocab_type1(
      model,
    );
  }

  late final _llama_vocab_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_vocab_type');
  late final _llama_vocab_type1 = _llama_vocab_type1Ptr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_rope_type1(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_rope_type1(
      model,
    );
  }

  late final _llama_rope_type1Ptr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_rope_type');
  late final _llama_rope_type1 =
      _llama_rope_type1Ptr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get the model's RoPE frequency scaling factor
  double llama_rope_freq_scale_train(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_rope_freq_scale_train(
      model,
    );
  }

  late final _llama_rope_freq_scale_trainPtr =
      _lookup<ffi.NativeFunction<ffi.Float Function(ffi.Pointer<llama_model>)>>(
          'llama_rope_freq_scale_train');
  late final _llama_rope_freq_scale_train = _llama_rope_freq_scale_trainPtr
      .asFunction<double Function(ffi.Pointer<llama_model>)>();

  /// Get metadata value as a string by key name
  int llama_model_meta_val_str(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> key,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str(
      model,
      key,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_val_strPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, ffi.Size)>>('llama_model_meta_val_str');
  late final _llama_model_meta_val_str =
      _llama_model_meta_val_strPtr.asFunction<
          int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>, int)>();

  /// Get the number of metadata key/value pairs
  int llama_model_meta_count(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_meta_count(
      model,
    );
  }

  late final _llama_model_meta_countPtr =
      _lookup<ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_model>)>>(
          'llama_model_meta_count');
  late final _llama_model_meta_count = _llama_model_meta_countPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get metadata key name by index
  int llama_model_meta_key_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_key_by_index(
      model,
      i,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_key_by_indexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_meta_key_by_index');
  late final _llama_model_meta_key_by_index =
      _llama_model_meta_key_by_indexPtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Get metadata value as a string by index
  int llama_model_meta_val_str_by_index(
    ffi.Pointer<llama_model> model,
    int i,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_meta_val_str_by_index(
      model,
      i,
      buf,
      buf_size,
    );
  }

  late final _llama_model_meta_val_str_by_indexPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_meta_val_str_by_index');
  late final _llama_model_meta_val_str_by_index =
      _llama_model_meta_val_str_by_indexPtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int)>();

  /// Get a string describing the model type
  int llama_model_desc(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> buf,
    int buf_size,
  ) {
    return _llama_model_desc(
      model,
      buf,
      buf_size,
    );
  }

  late final _llama_model_descPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>,
              ffi.Size)>>('llama_model_desc');
  late final _llama_model_desc = _llama_model_descPtr.asFunction<
      int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, int)>();

  /// Returns the total size of all the tensors in the model in bytes
  int llama_model_size(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_size(
      model,
    );
  }

  late final _llama_model_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>>(
      'llama_model_size');
  late final _llama_model_size =
      _llama_model_sizePtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns the total number of parameters in the model
  int llama_model_n_params(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_n_params(
      model,
    );
  }

  late final _llama_model_n_paramsPtr = _lookup<
          ffi.NativeFunction<ffi.Uint64 Function(ffi.Pointer<llama_model>)>>(
      'llama_model_n_params');
  late final _llama_model_n_params = _llama_model_n_paramsPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Get a llama model tensor
  ffi.Pointer<ggml_tensor> llama_get_model_tensor(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> name,
  ) {
    return _llama_get_model_tensor(
      model,
      name,
    );
  }

  late final _llama_get_model_tensorPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ggml_tensor> Function(ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>)>>('llama_get_model_tensor');
  late final _llama_get_model_tensor = _llama_get_model_tensorPtr.asFunction<
      ffi.Pointer<ggml_tensor> Function(
          ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)>();

  /// Returns true if the model contains an encoder that requires llama_encode() call
  bool llama_model_has_encoder(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_has_encoder(
      model,
    );
  }

  late final _llama_model_has_encoderPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
          'llama_model_has_encoder');
  late final _llama_model_has_encoder = _llama_model_has_encoderPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model contains a decoder that requires llama_decode() call
  bool llama_model_has_decoder(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_has_decoder(
      model,
    );
  }

  late final _llama_model_has_decoderPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
          'llama_model_has_decoder');
  late final _llama_model_has_decoder = _llama_model_has_decoderPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// For encoder-decoder models, this function returns id of the token that must be provided
  /// to the decoder to start generating output sequence. For other models, it returns -1.
  int llama_model_decoder_start_token(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_decoder_start_token(
      model,
    );
  }

  late final _llama_model_decoder_start_tokenPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_model_decoder_start_token');
  late final _llama_model_decoder_start_token =
      _llama_model_decoder_start_tokenPtr
          .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// Returns true if the model is recurrent (like Mamba, RWKV, etc.)
  bool llama_model_is_recurrent(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_model_is_recurrent(
      model,
    );
  }

  late final _llama_model_is_recurrentPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
          'llama_model_is_recurrent');
  late final _llama_model_is_recurrent = _llama_model_is_recurrentPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// Returns 0 on success
  int llama_model_quantize(
    ffi.Pointer<ffi.Char> fname_inp,
    ffi.Pointer<ffi.Char> fname_out,
    ffi.Pointer<llama_model_quantize_params> params,
  ) {
    return _llama_model_quantize(
      fname_inp,
      fname_out,
      params,
    );
  }

  late final _llama_model_quantizePtr = _lookup<
          ffi.NativeFunction<
              ffi.Uint32 Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
                  ffi.Pointer<llama_model_quantize_params>)>>(
      'llama_model_quantize');
  late final _llama_model_quantize = _llama_model_quantizePtr.asFunction<
      int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_model_quantize_params>)>();

  /// Load a LoRA adapter from file
  /// The loaded adapter will be associated to the given model, and will be free when the model is deleted
  ffi.Pointer<llama_lora_adapter> llama_lora_adapter_init(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> path_lora,
  ) {
    return _llama_lora_adapter_init(
      model,
      path_lora,
    );
  }

  late final _llama_lora_adapter_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_lora_adapter> Function(ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>)>>('llama_lora_adapter_init');
  late final _llama_lora_adapter_init = _llama_lora_adapter_initPtr.asFunction<
      ffi.Pointer<llama_lora_adapter> Function(
          ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)>();

  /// Add a loaded LoRA adapter to given context
  /// This will not modify model's weight
  int llama_lora_adapter_set(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_lora_adapter> adapter,
    double scale,
  ) {
    return _llama_lora_adapter_set(
      ctx,
      adapter,
      scale,
    );
  }

  late final _llama_lora_adapter_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<llama_lora_adapter>,
              ffi.Float)>>('llama_lora_adapter_set');
  late final _llama_lora_adapter_set = _llama_lora_adapter_setPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<llama_lora_adapter>,
          double)>();

  /// Remove a specific LoRA adapter from given context
  /// Return -1 if the adapter is not present in the context
  int llama_lora_adapter_remove(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_lora_adapter> adapter,
  ) {
    return _llama_lora_adapter_remove(
      ctx,
      adapter,
    );
  }

  late final _llama_lora_adapter_removePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_lora_adapter>)>>('llama_lora_adapter_remove');
  late final _llama_lora_adapter_remove =
      _llama_lora_adapter_removePtr.asFunction<
          int Function(
              ffi.Pointer<llama_context>, ffi.Pointer<llama_lora_adapter>)>();

  /// Remove all LoRA adapters from given context
  void llama_lora_adapter_clear(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_lora_adapter_clear(
      ctx,
    );
  }

  late final _llama_lora_adapter_clearPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_lora_adapter_clear');
  late final _llama_lora_adapter_clear = _llama_lora_adapter_clearPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Manually free a LoRA adapter
  /// Note: loaded adapters will be free when the associated model is deleted
  void llama_lora_adapter_free(
    ffi.Pointer<llama_lora_adapter> adapter,
  ) {
    return _llama_lora_adapter_free(
      adapter,
    );
  }

  late final _llama_lora_adapter_freePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_lora_adapter>)>>(
      'llama_lora_adapter_free');
  late final _llama_lora_adapter_free = _llama_lora_adapter_freePtr
      .asFunction<void Function(ffi.Pointer<llama_lora_adapter>)>();

  /// Apply a loaded control vector to a llama_context, or if data is NULL, clear
  /// the currently loaded vector.
  /// n_embd should be the size of a single layer's control, and data should point
  /// to an n_embd x n_layers buffer starting from layer 1.
  /// il_start and il_end are the layer range the vector should apply to (both inclusive)
  /// See llama_control_vector_load in common to load a control vector.
  int llama_control_vector_apply(
    ffi.Pointer<llama_context> lctx,
    ffi.Pointer<ffi.Float> data,
    int len,
    int n_embd,
    int il_start,
    int il_end,
  ) {
    return _llama_control_vector_apply(
      lctx,
      data,
      len,
      n_embd,
      il_start,
      il_end,
    );
  }

  late final _llama_control_vector_applyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Float>,
              ffi.Size,
              ffi.Int32,
              ffi.Int32,
              ffi.Int32)>>('llama_control_vector_apply');
  late final _llama_control_vector_apply =
      _llama_control_vector_applyPtr.asFunction<
          int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Float>, int,
              int, int, int)>();

  /// Create an empty KV cache view. (use only for debugging purposes)
  llama_kv_cache_view llama_kv_cache_view_init(
    ffi.Pointer<llama_context> ctx,
    int n_seq_max,
  ) {
    return _llama_kv_cache_view_init(
      ctx,
      n_seq_max,
    );
  }

  late final _llama_kv_cache_view_initPtr = _lookup<
      ffi.NativeFunction<
          llama_kv_cache_view Function(ffi.Pointer<llama_context>,
              ffi.Int32)>>('llama_kv_cache_view_init');
  late final _llama_kv_cache_view_init =
      _llama_kv_cache_view_initPtr.asFunction<
          llama_kv_cache_view Function(ffi.Pointer<llama_context>, int)>();

  /// Free a KV cache view. (use only for debugging purposes)
  void llama_kv_cache_view_free(
    ffi.Pointer<llama_kv_cache_view> view,
  ) {
    return _llama_kv_cache_view_free(
      view,
    );
  }

  late final _llama_kv_cache_view_freePtr = _lookup<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_kv_cache_view>)>>(
      'llama_kv_cache_view_free');
  late final _llama_kv_cache_view_free = _llama_kv_cache_view_freePtr
      .asFunction<void Function(ffi.Pointer<llama_kv_cache_view>)>();

  /// Update the KV cache view structure with the current state of the KV cache. (use only for debugging purposes)
  void llama_kv_cache_view_update(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<llama_kv_cache_view> view,
  ) {
    return _llama_kv_cache_view_update(
      ctx,
      view,
    );
  }

  late final _llama_kv_cache_view_updatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>,
              ffi.Pointer<llama_kv_cache_view>)>>('llama_kv_cache_view_update');
  late final _llama_kv_cache_view_update =
      _llama_kv_cache_view_updatePtr.asFunction<
          void Function(
              ffi.Pointer<llama_context>, ffi.Pointer<llama_kv_cache_view>)>();

  /// Returns the number of tokens in the KV cache (slow, use only for debug)
  /// If a KV cell has multiple sequences assigned to it, it will be counted multiple times
  int llama_get_kv_cache_token_count(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_kv_cache_token_count(
      ctx,
    );
  }

  late final _llama_get_kv_cache_token_countPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_get_kv_cache_token_count');
  late final _llama_get_kv_cache_token_count =
      _llama_get_kv_cache_token_countPtr
          .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Returns the number of used KV cells (i.e. have at least one sequence assigned to them)
  int llama_get_kv_cache_used_cells(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_kv_cache_used_cells(
      ctx,
    );
  }

  late final _llama_get_kv_cache_used_cellsPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_get_kv_cache_used_cells');
  late final _llama_get_kv_cache_used_cells = _llama_get_kv_cache_used_cellsPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Clear the KV cache - both cell info is erased and KV data is zeroed
  void llama_kv_cache_clear(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_clear(
      ctx,
    );
  }

  late final _llama_kv_cache_clearPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_clear');
  late final _llama_kv_cache_clear = _llama_kv_cache_clearPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
  /// seq_id < 0 : match any sequence
  /// p0 < 0     : [0,  p1]
  /// p1 < 0     : [p0, inf)
  bool llama_kv_cache_seq_rm(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
  ) {
    return _llama_kv_cache_seq_rm(
      ctx,
      seq_id,
      p0,
      p1,
    );
  }

  late final _llama_kv_cache_seq_rmPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos)>>('llama_kv_cache_seq_rm');
  late final _llama_kv_cache_seq_rm = _llama_kv_cache_seq_rmPtr
      .asFunction<bool Function(ffi.Pointer<llama_context>, int, int, int)>();

  /// Copy all tokens that belong to the specified sequence to another sequence
  /// Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_cp(
    ffi.Pointer<llama_context> ctx,
    int seq_id_src,
    int seq_id_dst,
    int p0,
    int p1,
  ) {
    return _llama_kv_cache_seq_cp(
      ctx,
      seq_id_src,
      seq_id_dst,
      p0,
      p1,
    );
  }

  late final _llama_kv_cache_seq_cpPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id,
              llama_seq_id, llama_pos, llama_pos)>>('llama_kv_cache_seq_cp');
  late final _llama_kv_cache_seq_cp = _llama_kv_cache_seq_cpPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Removes all tokens that do not belong to the specified sequence
  void llama_kv_cache_seq_keep(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_kv_cache_seq_keep(
      ctx,
      seq_id,
    );
  }

  late final _llama_kv_cache_seq_keepPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_kv_cache_seq_keep');
  late final _llama_kv_cache_seq_keep = _llama_kv_cache_seq_keepPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int)>();

  /// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
  /// If the KV cache is RoPEd, the KV data is updated accordingly:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_add(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
    int delta,
  ) {
    return _llama_kv_cache_seq_add(
      ctx,
      seq_id,
      p0,
      p1,
      delta,
    );
  }

  late final _llama_kv_cache_seq_addPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos, llama_pos)>>('llama_kv_cache_seq_add');
  late final _llama_kv_cache_seq_add = _llama_kv_cache_seq_addPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Integer division of the positions by factor of `d > 1`
  /// If the KV cache is RoPEd, the KV data is updated accordingly:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  /// p0 < 0 : [0,  p1]
  /// p1 < 0 : [p0, inf)
  void llama_kv_cache_seq_div(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
    int p0,
    int p1,
    int d,
  ) {
    return _llama_kv_cache_seq_div(
      ctx,
      seq_id,
      p0,
      p1,
      d,
    );
  }

  late final _llama_kv_cache_seq_divPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, llama_seq_id, llama_pos,
              llama_pos, ffi.Int)>>('llama_kv_cache_seq_div');
  late final _llama_kv_cache_seq_div = _llama_kv_cache_seq_divPtr.asFunction<
      void Function(ffi.Pointer<llama_context>, int, int, int, int)>();

  /// Returns the largest position present in the KV cache for the specified sequence
  int llama_kv_cache_seq_pos_max(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_kv_cache_seq_pos_max(
      ctx,
      seq_id,
    );
  }

  late final _llama_kv_cache_seq_pos_maxPtr = _lookup<
      ffi.NativeFunction<
          llama_pos Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_kv_cache_seq_pos_max');
  late final _llama_kv_cache_seq_pos_max = _llama_kv_cache_seq_pos_maxPtr
      .asFunction<int Function(ffi.Pointer<llama_context>, int)>();

  /// Defragment the KV cache
  /// This will be applied:
  /// - lazily on next llama_decode()
  /// - explicitly with llama_kv_cache_update()
  void llama_kv_cache_defrag(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_defrag(
      ctx,
    );
  }

  late final _llama_kv_cache_defragPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_defrag');
  late final _llama_kv_cache_defrag = _llama_kv_cache_defragPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Apply the KV cache updates (such as K-shifts, defragmentation, etc.)
  void llama_kv_cache_update(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_update(
      ctx,
    );
  }

  late final _llama_kv_cache_updatePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_update');
  late final _llama_kv_cache_update = _llama_kv_cache_updatePtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Check if the context supports KV cache shifting
  bool llama_kv_cache_can_shift(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_kv_cache_can_shift(
      ctx,
    );
  }

  late final _llama_kv_cache_can_shiftPtr = _lookup<
          ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_context>)>>(
      'llama_kv_cache_can_shift');
  late final _llama_kv_cache_can_shift = _llama_kv_cache_can_shiftPtr
      .asFunction<bool Function(ffi.Pointer<llama_context>)>();

  /// Returns the *actual* size in bytes of the state
  /// (logits, embedding and kv_cache)
  /// Only use when saving the state, not when restoring it, otherwise the size may be too small.
  int llama_state_get_size(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_state_get_size(
      ctx,
    );
  }

  late final _llama_state_get_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Size Function(ffi.Pointer<llama_context>)>>(
      'llama_state_get_size');
  late final _llama_state_get_size = _llama_state_get_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  int llama_get_state_size(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_state_size(
      ctx,
    );
  }

  late final _llama_get_state_sizePtr = _lookup<
          ffi.NativeFunction<ffi.Size Function(ffi.Pointer<llama_context>)>>(
      'llama_get_state_size');
  late final _llama_get_state_size = _llama_get_state_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Copies the state to the specified destination address.
  /// Destination needs to have allocated enough memory.
  /// Returns the number of bytes copied
  int llama_state_get_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
    int size,
  ) {
    return _llama_state_get_data(
      ctx,
      dst,
      size,
    );
  }

  late final _llama_state_get_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
              ffi.Size)>>('llama_state_get_data');
  late final _llama_state_get_data = _llama_state_get_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int)>();

  int llama_copy_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
  ) {
    return _llama_copy_state_data(
      ctx,
      dst,
    );
  }

  late final _llama_copy_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_copy_state_data');
  late final _llama_copy_state_data = _llama_copy_state_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>();

  /// Set the state reading from the specified address
  /// Returns the number of bytes read
  int llama_state_set_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
    int size,
  ) {
    return _llama_state_set_data(
      ctx,
      src,
      size,
    );
  }

  late final _llama_state_set_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
              ffi.Size)>>('llama_state_set_data');
  late final _llama_state_set_data = _llama_state_set_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int)>();

  int llama_set_state_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
  ) {
    return _llama_set_state_data(
      ctx,
      src,
    );
  }

  late final _llama_set_state_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Uint8>)>>('llama_set_state_data');
  late final _llama_set_state_data = _llama_set_state_dataPtr.asFunction<
      int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)>();

  /// Save/load session file
  bool llama_state_load_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_state_load_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_state_load_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>,
              ffi.Size,
              ffi.Pointer<ffi.Size>)>>('llama_state_load_file');
  late final _llama_state_load_file = _llama_state_load_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int, ffi.Pointer<ffi.Size>)>();

  bool llama_load_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_load_session_file(
      ctx,
      path_session,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_load_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>,
              ffi.Size,
              ffi.Pointer<ffi.Size>)>>('llama_load_session_file');
  late final _llama_load_session_file = _llama_load_session_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int, ffi.Pointer<ffi.Size>)>();

  bool llama_state_save_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_state_save_file(
      ctx,
      path_session,
      tokens,
      n_token_count,
    );
  }

  late final _llama_state_save_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>, ffi.Size)>>('llama_state_save_file');
  late final _llama_state_save_file = _llama_state_save_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int)>();

  bool llama_save_session_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> path_session,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_save_session_file(
      ctx,
      path_session,
      tokens,
      n_token_count,
    );
  }

  late final _llama_save_session_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_token>, ffi.Size)>>('llama_save_session_file');
  late final _llama_save_session_file = _llama_save_session_filePtr.asFunction<
      bool Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>,
          ffi.Pointer<llama_token>, int)>();

  /// Get the exact size needed to copy the KV cache of a single sequence
  int llama_state_seq_get_size(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_state_seq_get_size(
      ctx,
      seq_id,
    );
  }

  late final _llama_state_seq_get_sizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_state_seq_get_size');
  late final _llama_state_seq_get_size = _llama_state_seq_get_sizePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, int)>();

  /// Copy the KV cache of a single sequence into the specified buffer
  int llama_state_seq_get_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> dst,
    int size,
    int seq_id,
  ) {
    return _llama_state_seq_get_data(
      ctx,
      dst,
      size,
      seq_id,
    );
  }

  late final _llama_state_seq_get_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
              ffi.Size, llama_seq_id)>>('llama_state_seq_get_data');
  late final _llama_state_seq_get_data =
      _llama_state_seq_get_dataPtr.asFunction<
          int Function(
              ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int, int)>();

  /// Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
  /// Returns:
  /// - Positive: Ok
  /// - Zero: Failed to load
  int llama_state_seq_set_data(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Uint8> src,
    int size,
    int dest_seq_id,
  ) {
    return _llama_state_seq_set_data(
      ctx,
      src,
      size,
      dest_seq_id,
    );
  }

  late final _llama_state_seq_set_dataPtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>,
              ffi.Size, llama_seq_id)>>('llama_state_seq_set_data');
  late final _llama_state_seq_set_data =
      _llama_state_seq_set_dataPtr.asFunction<
          int Function(
              ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>, int, int)>();

  int llama_state_seq_save_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> filepath,
    int seq_id,
    ffi.Pointer<llama_token> tokens,
    int n_token_count,
  ) {
    return _llama_state_seq_save_file(
      ctx,
      filepath,
      seq_id,
      tokens,
      n_token_count,
    );
  }

  late final _llama_state_seq_save_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Char>,
              llama_seq_id,
              ffi.Pointer<llama_token>,
              ffi.Size)>>('llama_state_seq_save_file');
  late final _llama_state_seq_save_file =
      _llama_state_seq_save_filePtr.asFunction<
          int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>, int,
              ffi.Pointer<llama_token>, int)>();

  int llama_state_seq_load_file(
    ffi.Pointer<llama_context> ctx,
    ffi.Pointer<ffi.Char> filepath,
    int dest_seq_id,
    ffi.Pointer<llama_token> tokens_out,
    int n_token_capacity,
    ffi.Pointer<ffi.Size> n_token_count_out,
  ) {
    return _llama_state_seq_load_file(
      ctx,
      filepath,
      dest_seq_id,
      tokens_out,
      n_token_capacity,
      n_token_count_out,
    );
  }

  late final _llama_state_seq_load_filePtr = _lookup<
      ffi.NativeFunction<
          ffi.Size Function(
              ffi.Pointer<llama_context>,
              ffi.Pointer<ffi.Char>,
              llama_seq_id,
              ffi.Pointer<llama_token>,
              ffi.Size,
              ffi.Pointer<ffi.Size>)>>('llama_state_seq_load_file');
  late final _llama_state_seq_load_file =
      _llama_state_seq_load_filePtr.asFunction<
          int Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Char>, int,
              ffi.Pointer<llama_token>, int, ffi.Pointer<ffi.Size>)>();

  /// Return batch for single sequence of tokens
  /// The sequence ID will be fixed to 0
  /// The position of the tokens will be tracked automatically by llama_decode
  ///
  /// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
  llama_batch llama_batch_get_one(
    ffi.Pointer<llama_token> tokens,
    int n_tokens,
  ) {
    return _llama_batch_get_one(
      tokens,
      n_tokens,
    );
  }

  late final _llama_batch_get_onePtr = _lookup<
      ffi.NativeFunction<
          llama_batch Function(
              ffi.Pointer<llama_token>, ffi.Int32)>>('llama_batch_get_one');
  late final _llama_batch_get_one = _llama_batch_get_onePtr
      .asFunction<llama_batch Function(ffi.Pointer<llama_token>, int)>();

  /// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
  /// Each token can be assigned up to n_seq_max sequence ids
  /// The batch has to be freed with llama_batch_free()
  /// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
  /// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
  /// The rest of the llama_batch members are allocated with size n_tokens
  /// All members are left uninitialized
  llama_batch llama_batch_init(
    int n_tokens,
    int embd,
    int n_seq_max,
  ) {
    return _llama_batch_init(
      n_tokens,
      embd,
      n_seq_max,
    );
  }

  late final _llama_batch_initPtr = _lookup<
      ffi.NativeFunction<
          llama_batch Function(
              ffi.Int32, ffi.Int32, ffi.Int32)>>('llama_batch_init');
  late final _llama_batch_init =
      _llama_batch_initPtr.asFunction<llama_batch Function(int, int, int)>();

  /// Frees a batch of tokens allocated with llama_batch_init()
  void llama_batch_free(
    llama_batch batch,
  ) {
    return _llama_batch_free(
      batch,
    );
  }

  late final _llama_batch_freePtr =
      _lookup<ffi.NativeFunction<ffi.Void Function(llama_batch)>>(
          'llama_batch_free');
  late final _llama_batch_free =
      _llama_batch_freePtr.asFunction<void Function(llama_batch)>();

  /// Processes a batch of tokens with the ecoder part of the encoder-decoder model.
  /// Stores the encoder output internally for later use by the decoder cross-attention layers.
  /// 0 - success
  /// < 0 - error. the KV cache state is restored to the state before this call
  int llama_encode(
    ffi.Pointer<llama_context> ctx,
    llama_batch batch,
  ) {
    return _llama_encode(
      ctx,
      batch,
    );
  }

  late final _llama_encodePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_context>, llama_batch)>>('llama_encode');
  late final _llama_encode = _llama_encodePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, llama_batch)>();

  /// Positive return values does not mean a fatal error, but rather a warning.
  /// 0 - success
  /// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
  /// < 0 - error. the KV cache state is restored to the state before this call
  int llama_decode(
    ffi.Pointer<llama_context> ctx,
    llama_batch batch,
  ) {
    return _llama_decode(
      ctx,
      batch,
    );
  }

  late final _llama_decodePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_context>, llama_batch)>>('llama_decode');
  late final _llama_decode = _llama_decodePtr
      .asFunction<int Function(ffi.Pointer<llama_context>, llama_batch)>();

  /// Set the number of threads used for decoding
  /// n_threads is the number of threads used for generation (single token)
  /// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
  void llama_set_n_threads(
    ffi.Pointer<llama_context> ctx,
    int n_threads,
    int n_threads_batch,
  ) {
    return _llama_set_n_threads(
      ctx,
      n_threads,
      n_threads_batch,
    );
  }

  late final _llama_set_n_threadsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ffi.Int32,
              ffi.Int32)>>('llama_set_n_threads');
  late final _llama_set_n_threads = _llama_set_n_threadsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, int, int)>();

  /// Get the number of threads used for generation of a single token.
  int llama_n_threads(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_threads(
      ctx,
    );
  }

  late final _llama_n_threadsPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_threads');
  late final _llama_n_threads = _llama_n_threadsPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Get the number of threads used for prompt and batch processing (multiple token).
  int llama_n_threads_batch(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_n_threads_batch(
      ctx,
    );
  }

  late final _llama_n_threads_batchPtr = _lookup<
          ffi.NativeFunction<ffi.Int32 Function(ffi.Pointer<llama_context>)>>(
      'llama_n_threads_batch');
  late final _llama_n_threads_batch = _llama_n_threads_batchPtr
      .asFunction<int Function(ffi.Pointer<llama_context>)>();

  /// Set whether the model is in embeddings mode or not
  /// If true, embeddings will be returned but logits will not
  void llama_set_embeddings(
    ffi.Pointer<llama_context> ctx,
    bool embeddings,
  ) {
    return _llama_set_embeddings(
      ctx,
      embeddings,
    );
  }

  late final _llama_set_embeddingsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>, ffi.Bool)>>('llama_set_embeddings');
  late final _llama_set_embeddings = _llama_set_embeddingsPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, bool)>();

  /// Set whether to use causal attention or not
  /// If set to true, the model will only attend to the past tokens
  void llama_set_causal_attn(
    ffi.Pointer<llama_context> ctx,
    bool causal_attn,
  ) {
    return _llama_set_causal_attn(
      ctx,
      causal_attn,
    );
  }

  late final _llama_set_causal_attnPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_context>, ffi.Bool)>>('llama_set_causal_attn');
  late final _llama_set_causal_attn = _llama_set_causal_attnPtr
      .asFunction<void Function(ffi.Pointer<llama_context>, bool)>();

  /// Set abort callback
  void llama_set_abort_callback(
    ffi.Pointer<llama_context> ctx,
    ggml_abort_callback abort_callback,
    ffi.Pointer<ffi.Void> abort_callback_data,
  ) {
    return _llama_set_abort_callback(
      ctx,
      abort_callback,
      abort_callback_data,
    );
  }

  late final _llama_set_abort_callbackPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
              ffi.Pointer<ffi.Void>)>>('llama_set_abort_callback');
  late final _llama_set_abort_callback =
      _llama_set_abort_callbackPtr.asFunction<
          void Function(ffi.Pointer<llama_context>, ggml_abort_callback,
              ffi.Pointer<ffi.Void>)>();

  /// Wait until all computations are finished
  /// This is automatically done when using one of the functions below to obtain the computation results
  /// and is not necessary to call it explicitly in most cases
  void llama_synchronize(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_synchronize(
      ctx,
    );
  }

  late final _llama_synchronizePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_synchronize');
  late final _llama_synchronize = _llama_synchronizePtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// Token logits obtained from the last call to llama_decode()
  /// The logits for which llama_batch.logits[i] != 0 are stored contiguously
  /// in the order they have appeared in the batch.
  /// Rows: number of tokens for which llama_batch.logits[i] != 0
  /// Cols: n_vocab
  ffi.Pointer<ffi.Float> llama_get_logits(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_logits(
      ctx,
    );
  }

  late final _llama_get_logitsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>)>>('llama_get_logits');
  late final _llama_get_logits = _llama_get_logitsPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>();

  /// Logits for the ith token. For positive indices, Equivalent to:
  /// llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
  /// Negative indicies can be used to access logits in reverse order, -1 is the last logit.
  /// returns NULL for invalid ids.
  ffi.Pointer<ffi.Float> llama_get_logits_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_logits_ith(
      ctx,
      i,
    );
  }

  late final _llama_get_logits_ithPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>, ffi.Int32)>>('llama_get_logits_ith');
  late final _llama_get_logits_ith = _llama_get_logits_ithPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)>();

  /// Get all output token embeddings.
  /// when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
  /// the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
  /// in the order they have appeared in the batch.
  /// shape: [n_outputs*n_embd]
  /// Otherwise, returns NULL.
  ffi.Pointer<ffi.Float> llama_get_embeddings(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_get_embeddings(
      ctx,
    );
  }

  late final _llama_get_embeddingsPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(
              ffi.Pointer<llama_context>)>>('llama_get_embeddings');
  late final _llama_get_embeddings = _llama_get_embeddingsPtr.asFunction<
      ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>();

  /// Get the embeddings for the ith token. For positive indices, Equivalent to:
  /// llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
  /// Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
  /// shape: [n_embd] (1-dimensional)
  /// returns NULL for invalid ids.
  ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
    ffi.Pointer<llama_context> ctx,
    int i,
  ) {
    return _llama_get_embeddings_ith(
      ctx,
      i,
    );
  }

  late final _llama_get_embeddings_ithPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>,
              ffi.Int32)>>('llama_get_embeddings_ith');
  late final _llama_get_embeddings_ith =
      _llama_get_embeddings_ithPtr.asFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)>();

  /// Get the embeddings for a sequence id
  /// Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
  /// when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[1] with the rank of the sequence
  /// otherwise: float[n_embd] (1-dimensional)
  ffi.Pointer<ffi.Float> llama_get_embeddings_seq(
    ffi.Pointer<llama_context> ctx,
    int seq_id,
  ) {
    return _llama_get_embeddings_seq(
      ctx,
      seq_id,
    );
  }

  late final _llama_get_embeddings_seqPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>,
              llama_seq_id)>>('llama_get_embeddings_seq');
  late final _llama_get_embeddings_seq =
      _llama_get_embeddings_seqPtr.asFunction<
          ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, int)>();

  /// Vocab
  ffi.Pointer<ffi.Char> llama_token_get_text(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_text(
      model,
      token,
    );
  }

  late final _llama_token_get_textPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_text');
  late final _llama_token_get_text = _llama_token_get_textPtr.asFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>, int)>();

  double llama_token_get_score(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_score(
      model,
      token,
    );
  }

  late final _llama_token_get_scorePtr = _lookup<
      ffi.NativeFunction<
          ffi.Float Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_score');
  late final _llama_token_get_score = _llama_token_get_scorePtr
      .asFunction<double Function(ffi.Pointer<llama_model>, int)>();

  int llama_token_get_attr(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_get_attr(
      model,
      token,
    );
  }

  late final _llama_token_get_attrPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_get_attr');
  late final _llama_token_get_attr = _llama_token_get_attrPtr
      .asFunction<int Function(ffi.Pointer<llama_model>, int)>();

  /// Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
  bool llama_token_is_eog(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_is_eog(
      model,
      token,
    );
  }

  late final _llama_token_is_eogPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(
              ffi.Pointer<llama_model>, llama_token)>>('llama_token_is_eog');
  late final _llama_token_is_eog = _llama_token_is_eogPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>, int)>();

  /// Identify if Token Id is a control token or a render-able token
  bool llama_token_is_control(
    ffi.Pointer<llama_model> model,
    int token,
  ) {
    return _llama_token_is_control(
      model,
      token,
    );
  }

  late final _llama_token_is_controlPtr = _lookup<
      ffi.NativeFunction<
          ffi.Bool Function(ffi.Pointer<llama_model>,
              llama_token)>>('llama_token_is_control');
  late final _llama_token_is_control = _llama_token_is_controlPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>, int)>();

  /// Special tokens
  int llama_token_bos(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_bos(
      model,
    );
  }

  late final _llama_token_bosPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_bos');
  late final _llama_token_bos =
      _llama_token_bosPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_eos(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_eos(
      model,
    );
  }

  late final _llama_token_eosPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_eos');
  late final _llama_token_eos =
      _llama_token_eosPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_eot(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_eot(
      model,
    );
  }

  late final _llama_token_eotPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_eot');
  late final _llama_token_eot =
      _llama_token_eotPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_cls(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_cls(
      model,
    );
  }

  late final _llama_token_clsPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_cls');
  late final _llama_token_cls =
      _llama_token_clsPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_sep(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_sep(
      model,
    );
  }

  late final _llama_token_sepPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_sep');
  late final _llama_token_sep =
      _llama_token_sepPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_nl(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_nl(
      model,
    );
  }

  late final _llama_token_nlPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_nl');
  late final _llama_token_nl =
      _llama_token_nlPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_pad(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_pad(
      model,
    );
  }

  late final _llama_token_padPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_pad');
  late final _llama_token_pad =
      _llama_token_padPtr.asFunction<int Function(ffi.Pointer<llama_model>)>();

  bool llama_add_bos_token(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_add_bos_token(
      model,
    );
  }

  late final _llama_add_bos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
          'llama_add_bos_token');
  late final _llama_add_bos_token = _llama_add_bos_tokenPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  bool llama_add_eos_token(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_add_eos_token(
      model,
    );
  }

  late final _llama_add_eos_tokenPtr =
      _lookup<ffi.NativeFunction<ffi.Bool Function(ffi.Pointer<llama_model>)>>(
          'llama_add_eos_token');
  late final _llama_add_eos_token = _llama_add_eos_tokenPtr
      .asFunction<bool Function(ffi.Pointer<llama_model>)>();

  /// infill tokens
  int llama_token_prefix(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_prefix(
      model,
    );
  }

  late final _llama_token_prefixPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_prefix');
  late final _llama_token_prefix = _llama_token_prefixPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_middle(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_middle(
      model,
    );
  }

  late final _llama_token_middlePtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_middle');
  late final _llama_token_middle = _llama_token_middlePtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_suffix(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_suffix(
      model,
    );
  }

  late final _llama_token_suffixPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_suffix');
  late final _llama_token_suffix = _llama_token_suffixPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_pre(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_pre(
      model,
    );
  }

  late final _llama_token_fim_prePtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_pre');
  late final _llama_token_fim_pre = _llama_token_fim_prePtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_suf(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_suf(
      model,
    );
  }

  late final _llama_token_fim_sufPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_suf');
  late final _llama_token_fim_suf = _llama_token_fim_sufPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_mid(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_mid(
      model,
    );
  }

  late final _llama_token_fim_midPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_mid');
  late final _llama_token_fim_mid = _llama_token_fim_midPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_pad(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_pad(
      model,
    );
  }

  late final _llama_token_fim_padPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_pad');
  late final _llama_token_fim_pad = _llama_token_fim_padPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_rep(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_rep(
      model,
    );
  }

  late final _llama_token_fim_repPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_rep');
  late final _llama_token_fim_rep = _llama_token_fim_repPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  int llama_token_fim_sep(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_token_fim_sep(
      model,
    );
  }

  late final _llama_token_fim_sepPtr = _lookup<
          ffi.NativeFunction<llama_token Function(ffi.Pointer<llama_model>)>>(
      'llama_token_fim_sep');
  late final _llama_token_fim_sep = _llama_token_fim_sepPtr
      .asFunction<int Function(ffi.Pointer<llama_model>)>();

  /// @details Convert the provided text into tokens.
  /// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
  /// @return Returns the number of tokens on success, no more than n_tokens_max
  /// @return Returns a negative number on failure - the number of tokens that would have been returned
  /// @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
  /// @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
  /// as plaintext. Does not insert a leading space.
  int llama_tokenize(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> text,
    int text_len,
    ffi.Pointer<llama_token> tokens,
    int n_tokens_max,
    bool add_special,
    bool parse_special,
  ) {
    return _llama_tokenize(
      model,
      text,
      text_len,
      tokens,
      n_tokens_max,
      add_special,
      parse_special,
    );
  }

  late final _llama_tokenizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Int32,
              ffi.Pointer<llama_token>,
              ffi.Int32,
              ffi.Bool,
              ffi.Bool)>>('llama_tokenize');
  late final _llama_tokenize = _llama_tokenizePtr.asFunction<
      int Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, int,
          ffi.Pointer<llama_token>, int, bool, bool)>();

  /// Token Id -> Piece.
  /// Uses the vocabulary in the provided context.
  /// Does not write null terminator to the buffer.
  /// User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
  /// @param special If true, special tokens are rendered in the output.
  int llama_token_to_piece(
    ffi.Pointer<llama_model> model,
    int token,
    ffi.Pointer<ffi.Char> buf,
    int length,
    int lstrip,
    bool special,
  ) {
    return _llama_token_to_piece(
      model,
      token,
      buf,
      length,
      lstrip,
      special,
    );
  }

  late final _llama_token_to_piecePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              llama_token,
              ffi.Pointer<ffi.Char>,
              ffi.Int32,
              ffi.Int32,
              ffi.Bool)>>('llama_token_to_piece');
  late final _llama_token_to_piece = _llama_token_to_piecePtr.asFunction<
      int Function(ffi.Pointer<llama_model>, int, ffi.Pointer<ffi.Char>, int,
          int, bool)>();

  /// @details Convert the provided tokens into text (inverse of llama_tokenize()).
  /// @param text The char pointer must be large enough to hold the resulting text.
  /// @return Returns the number of chars/bytes on success, no more than text_len_max.
  /// @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
  /// @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
  /// @param unparse_special If true, special tokens are rendered in the output.
  int llama_detokenize(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<llama_token> tokens,
    int n_tokens,
    ffi.Pointer<ffi.Char> text,
    int text_len_max,
    bool remove_special,
    bool unparse_special,
  ) {
    return _llama_detokenize(
      model,
      tokens,
      n_tokens,
      text,
      text_len_max,
      remove_special,
      unparse_special,
    );
  }

  late final _llama_detokenizePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<llama_token>,
              ffi.Int32,
              ffi.Pointer<ffi.Char>,
              ffi.Int32,
              ffi.Bool,
              ffi.Bool)>>('llama_detokenize');
  late final _llama_detokenize = _llama_detokenizePtr.asFunction<
      int Function(ffi.Pointer<llama_model>, ffi.Pointer<llama_token>, int,
          ffi.Pointer<ffi.Char>, int, bool, bool)>();

  /// Apply chat template. Inspired by hf apply_chat_template() on python.
  /// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
  /// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
  /// @param tmpl A Jinja template to use for this chat. If this is nullptr, the models default chat template will be used instead.
  /// @param chat Pointer to a list of multiple llama_chat_message
  /// @param n_msg Number of llama_chat_message in this chat
  /// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
  /// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
  /// @param length The size of the allocated buffer
  /// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
  int llama_chat_apply_template(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> tmpl,
    ffi.Pointer<llama_chat_message> chat,
    int n_msg,
    bool add_ass,
    ffi.Pointer<ffi.Char> buf,
    int length,
  ) {
    return _llama_chat_apply_template(
      model,
      tmpl,
      chat,
      n_msg,
      add_ass,
      buf,
      length,
    );
  }

  late final _llama_chat_apply_templatePtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_chat_message>,
              ffi.Size,
              ffi.Bool,
              ffi.Pointer<ffi.Char>,
              ffi.Int32)>>('llama_chat_apply_template');
  late final _llama_chat_apply_template =
      _llama_chat_apply_templatePtr.asFunction<
          int Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<llama_chat_message>,
              int,
              bool,
              ffi.Pointer<ffi.Char>,
              int)>();

  /// Get list of built-in chat templates
  int llama_chat_builtin_templates(
    ffi.Pointer<ffi.Pointer<ffi.Char>> output,
    int len,
  ) {
    return _llama_chat_builtin_templates(
      output,
      len,
    );
  }

  late final _llama_chat_builtin_templatesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int32 Function(ffi.Pointer<ffi.Pointer<ffi.Char>>,
              ffi.Size)>>('llama_chat_builtin_templates');
  late final _llama_chat_builtin_templates = _llama_chat_builtin_templatesPtr
      .asFunction<int Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, int)>();

  /// mirror of llama_sampler_i:
  ffi.Pointer<ffi.Char> llama_sampler_name(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_name(
      smpl,
    );
  }

  late final _llama_sampler_namePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<ffi.Char> Function(
              ffi.Pointer<llama_sampler>)>>('llama_sampler_name');
  late final _llama_sampler_name = _llama_sampler_namePtr
      .asFunction<ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)>();

  void llama_sampler_accept(
    ffi.Pointer<llama_sampler> smpl,
    int token,
  ) {
    return _llama_sampler_accept(
      smpl,
      token,
    );
  }

  late final _llama_sampler_acceptPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler>,
              llama_token)>>('llama_sampler_accept');
  late final _llama_sampler_accept = _llama_sampler_acceptPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>, int)>();

  void llama_sampler_apply(
    ffi.Pointer<llama_sampler> smpl,
    ffi.Pointer<llama_token_data_array> cur_p,
  ) {
    return _llama_sampler_apply(
      smpl,
      cur_p,
    );
  }

  late final _llama_sampler_applyPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler>,
              ffi.Pointer<llama_token_data_array>)>>('llama_sampler_apply');
  late final _llama_sampler_apply = _llama_sampler_applyPtr.asFunction<
      void Function(
          ffi.Pointer<llama_sampler>, ffi.Pointer<llama_token_data_array>)>();

  void llama_sampler_reset(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_reset(
      smpl,
    );
  }

  late final _llama_sampler_resetPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>>(
      'llama_sampler_reset');
  late final _llama_sampler_reset = _llama_sampler_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  ffi.Pointer<llama_sampler> llama_sampler_clone(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_clone(
      smpl,
    );
  }

  late final _llama_sampler_clonePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_sampler>)>>('llama_sampler_clone');
  late final _llama_sampler_clone = _llama_sampler_clonePtr.asFunction<
      ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)>();

  /// important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
  void llama_sampler_free(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_free(
      smpl,
    );
  }

  late final _llama_sampler_freePtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>>(
      'llama_sampler_free');
  late final _llama_sampler_free = _llama_sampler_freePtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  /// llama_sampler_chain
  /// a type of llama_sampler that can chain multiple samplers one after another
  ffi.Pointer<llama_sampler> llama_sampler_chain_init(
    llama_sampler_chain_params params,
  ) {
    return _llama_sampler_chain_init(
      params,
    );
  }

  late final _llama_sampler_chain_initPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              llama_sampler_chain_params)>>('llama_sampler_chain_init');
  late final _llama_sampler_chain_init =
      _llama_sampler_chain_initPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)>();

  /// important: takes ownership of the sampler object and will free it when llama_sampler_free is called
  void llama_sampler_chain_add(
    ffi.Pointer<llama_sampler> chain,
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_chain_add(
      chain,
      smpl,
    );
  }

  late final _llama_sampler_chain_addPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler>,
              ffi.Pointer<llama_sampler>)>>('llama_sampler_chain_add');
  late final _llama_sampler_chain_add = _llama_sampler_chain_addPtr.asFunction<
      void Function(ffi.Pointer<llama_sampler>, ffi.Pointer<llama_sampler>)>();

  ffi.Pointer<llama_sampler> llama_sampler_chain_get(
    ffi.Pointer<llama_sampler> chain,
    int i,
  ) {
    return _llama_sampler_chain_get(
      chain,
      i,
    );
  }

  late final _llama_sampler_chain_getPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>,
              ffi.Int32)>>('llama_sampler_chain_get');
  late final _llama_sampler_chain_get = _llama_sampler_chain_getPtr.asFunction<
      ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>, int)>();

  int llama_sampler_chain_n(
    ffi.Pointer<llama_sampler> chain,
  ) {
    return _llama_sampler_chain_n(
      chain,
    );
  }

  late final _llama_sampler_chain_nPtr =
      _lookup<ffi.NativeFunction<ffi.Int Function(ffi.Pointer<llama_sampler>)>>(
          'llama_sampler_chain_n');
  late final _llama_sampler_chain_n = _llama_sampler_chain_nPtr
      .asFunction<int Function(ffi.Pointer<llama_sampler>)>();

  /// after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
  ffi.Pointer<llama_sampler> llama_sampler_chain_remove(
    ffi.Pointer<llama_sampler> chain,
    int i,
  ) {
    return _llama_sampler_chain_remove(
      chain,
      i,
    );
  }

  late final _llama_sampler_chain_removePtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>,
              ffi.Int32)>>('llama_sampler_chain_remove');
  late final _llama_sampler_chain_remove =
      _llama_sampler_chain_removePtr.asFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_sampler>, int)>();

  /// available samplers:
  ffi.Pointer<llama_sampler> llama_sampler_init_greedy() {
    return _llama_sampler_init_greedy();
  }

  late final _llama_sampler_init_greedyPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<llama_sampler> Function()>>(
          'llama_sampler_init_greedy');
  late final _llama_sampler_init_greedy = _llama_sampler_init_greedyPtr
      .asFunction<ffi.Pointer<llama_sampler> Function()>();

  ffi.Pointer<llama_sampler> llama_sampler_init_dist(
    int seed,
  ) {
    return _llama_sampler_init_dist(
      seed,
    );
  }

  late final _llama_sampler_init_distPtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Uint32)>>(
      'llama_sampler_init_dist');
  late final _llama_sampler_init_dist = _llama_sampler_init_distPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(int)>();

  /// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.
  /// NOTE: Avoid using on the full vocabulary as the sorting can become slow. For example, apply top-k or top-p sampling first.
  ffi.Pointer<llama_sampler> llama_sampler_init_softmax() {
    return _llama_sampler_init_softmax();
  }

  late final _llama_sampler_init_softmaxPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<llama_sampler> Function()>>(
          'llama_sampler_init_softmax');
  late final _llama_sampler_init_softmax = _llama_sampler_init_softmaxPtr
      .asFunction<ffi.Pointer<llama_sampler> Function()>();

  /// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  ffi.Pointer<llama_sampler> llama_sampler_init_top_k(
    int k,
  ) {
    return _llama_sampler_init_top_k(
      k,
    );
  }

  late final _llama_sampler_init_top_kPtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Int32)>>(
      'llama_sampler_init_top_k');
  late final _llama_sampler_init_top_k = _llama_sampler_init_top_kPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(int)>();

  /// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
  ffi.Pointer<llama_sampler> llama_sampler_init_top_p(
    double p,
    int min_keep,
  ) {
    return _llama_sampler_init_top_p(
      p,
      min_keep,
    );
  }

  late final _llama_sampler_init_top_pPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Float, ffi.Size)>>('llama_sampler_init_top_p');
  late final _llama_sampler_init_top_p = _llama_sampler_init_top_pPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841
  ffi.Pointer<llama_sampler> llama_sampler_init_min_p(
    double p,
    int min_keep,
  ) {
    return _llama_sampler_init_min_p(
      p,
      min_keep,
    );
  }

  late final _llama_sampler_init_min_pPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Float, ffi.Size)>>('llama_sampler_init_min_p');
  late final _llama_sampler_init_min_p = _llama_sampler_init_min_pPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
  ffi.Pointer<llama_sampler> llama_sampler_init_typical(
    double p,
    int min_keep,
  ) {
    return _llama_sampler_init_typical(
      p,
      min_keep,
    );
  }

  late final _llama_sampler_init_typicalPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Float, ffi.Size)>>('llama_sampler_init_typical');
  late final _llama_sampler_init_typical = _llama_sampler_init_typicalPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double, int)>();

  /// #details Updates the logits l_i` = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf
  ffi.Pointer<llama_sampler> llama_sampler_init_temp(
    double t,
  ) {
    return _llama_sampler_init_temp(
      t,
    );
  }

  late final _llama_sampler_init_tempPtr = _lookup<
          ffi.NativeFunction<ffi.Pointer<llama_sampler> Function(ffi.Float)>>(
      'llama_sampler_init_temp');
  late final _llama_sampler_init_temp = _llama_sampler_init_tempPtr
      .asFunction<ffi.Pointer<llama_sampler> Function(double)>();

  /// @details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.
  ffi.Pointer<llama_sampler> llama_sampler_init_temp_ext(
    double t,
    double delta,
    double exponent,
  ) {
    return _llama_sampler_init_temp_ext(
      t,
      delta,
      exponent,
    );
  }

  late final _llama_sampler_init_temp_extPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Float, ffi.Float, ffi.Float)>>('llama_sampler_init_temp_ext');
  late final _llama_sampler_init_temp_ext =
      _llama_sampler_init_temp_extPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(double, double, double)>();

  /// @details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335
  ffi.Pointer<llama_sampler> llama_sampler_init_xtc(
    double p,
    double t,
    int min_keep,
    int seed,
  ) {
    return _llama_sampler_init_xtc(
      p,
      t,
      min_keep,
      seed,
    );
  }

  late final _llama_sampler_init_xtcPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Float, ffi.Size,
              ffi.Uint32)>>('llama_sampler_init_xtc');
  late final _llama_sampler_init_xtc = _llama_sampler_init_xtcPtr.asFunction<
      ffi.Pointer<llama_sampler> Function(double, double, int, int)>();

  /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  ffi.Pointer<llama_sampler> llama_sampler_init_mirostat(
    int n_vocab,
    int seed,
    double tau,
    double eta,
    int m,
  ) {
    return _llama_sampler_init_mirostat(
      n_vocab,
      seed,
      tau,
      eta,
      m,
    );
  }

  late final _llama_sampler_init_mirostatPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Int32, ffi.Uint32, ffi.Float,
              ffi.Float, ffi.Int32)>>('llama_sampler_init_mirostat');
  late final _llama_sampler_init_mirostat =
      _llama_sampler_init_mirostatPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(int, int, double, double, int)>();

  /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
  /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
  /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
  /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
  /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
  ffi.Pointer<llama_sampler> llama_sampler_init_mirostat_v2(
    int seed,
    double tau,
    double eta,
  ) {
    return _llama_sampler_init_mirostat_v2(
      seed,
      tau,
      eta,
    );
  }

  late final _llama_sampler_init_mirostat_v2Ptr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Uint32, ffi.Float,
              ffi.Float)>>('llama_sampler_init_mirostat_v2');
  late final _llama_sampler_init_mirostat_v2 =
      _llama_sampler_init_mirostat_v2Ptr.asFunction<
          ffi.Pointer<llama_sampler> Function(int, double, double)>();

  ffi.Pointer<llama_sampler> llama_sampler_init_grammar(
    ffi.Pointer<llama_model> model,
    ffi.Pointer<ffi.Char> grammar_str,
    ffi.Pointer<ffi.Char> grammar_root,
  ) {
    return _llama_sampler_init_grammar(
      model,
      grammar_str,
      grammar_root,
    );
  }

  late final _llama_sampler_init_grammarPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>,
              ffi.Pointer<ffi.Char>)>>('llama_sampler_init_grammar');
  late final _llama_sampler_init_grammar =
      _llama_sampler_init_grammarPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_model>,
              ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>();

  ffi.Pointer<llama_sampler> llama_sampler_init_penalties(
    int n_vocab,
    int special_eos_id,
    int linefeed_id,
    int penalty_last_n,
    double penalty_repeat,
    double penalty_freq,
    double penalty_present,
    bool penalize_nl,
    bool ignore_eos,
  ) {
    return _llama_sampler_init_penalties(
      n_vocab,
      special_eos_id,
      linefeed_id,
      penalty_last_n,
      penalty_repeat,
      penalty_freq,
      penalty_present,
      penalize_nl,
      ignore_eos,
    );
  }

  late final _llama_sampler_init_penaltiesPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Int32,
              llama_token,
              llama_token,
              ffi.Int32,
              ffi.Float,
              ffi.Float,
              ffi.Float,
              ffi.Bool,
              ffi.Bool)>>('llama_sampler_init_penalties');
  late final _llama_sampler_init_penalties =
      _llama_sampler_init_penaltiesPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(
              int, int, int, int, double, double, double, bool, bool)>();

  /// @details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982
  ffi.Pointer<llama_sampler> llama_sampler_init_dry(
    ffi.Pointer<llama_model> model,
    double dry_multiplier,
    double dry_base,
    int dry_allowed_length,
    int dry_penalty_last_n,
    ffi.Pointer<ffi.Pointer<ffi.Char>> seq_breakers,
    int num_breakers,
  ) {
    return _llama_sampler_init_dry(
      model,
      dry_multiplier,
      dry_base,
      dry_allowed_length,
      dry_penalty_last_n,
      seq_breakers,
      num_breakers,
    );
  }

  late final _llama_sampler_init_dryPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_model>,
              ffi.Float,
              ffi.Float,
              ffi.Int32,
              ffi.Int32,
              ffi.Pointer<ffi.Pointer<ffi.Char>>,
              ffi.Size)>>('llama_sampler_init_dry');
  late final _llama_sampler_init_dry = _llama_sampler_init_dryPtr.asFunction<
      ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_model>, double,
          double, int, int, ffi.Pointer<ffi.Pointer<ffi.Char>>, int)>();

  ffi.Pointer<llama_sampler> llama_sampler_init_logit_bias(
    int n_vocab,
    int n_logit_bias,
    ffi.Pointer<llama_logit_bias> logit_bias,
  ) {
    return _llama_sampler_init_logit_bias(
      n_vocab,
      n_logit_bias,
      logit_bias,
    );
  }

  late final _llama_sampler_init_logit_biasPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Int32, ffi.Int32,
              ffi.Pointer<llama_logit_bias>)>>('llama_sampler_init_logit_bias');
  late final _llama_sampler_init_logit_bias =
      _llama_sampler_init_logit_biasPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(
              int, int, ffi.Pointer<llama_logit_bias>)>();

  /// this sampler is meant to be used for fill-in-the-middle infilling
  /// it's supposed to be used after top_k + top_p sampling
  ///
  /// 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
  /// 2. combine probs of tokens that have the same prefix
  ///
  /// example:
  ///
  /// - before:
  /// "hel":   0.5
  /// "hell":  0.2
  /// "hello": 0.1
  /// "dummy": 0.1
  ///
  /// - after:
  /// "hel":   0.8
  /// "dummy": 0.1
  ///
  /// 3. discard non-EOG tokens with low prob
  /// 4. if no tokens are left -> pick EOT
  ffi.Pointer<llama_sampler> llama_sampler_init_infill(
    ffi.Pointer<llama_model> model,
  ) {
    return _llama_sampler_init_infill(
      model,
    );
  }

  late final _llama_sampler_init_infillPtr = _lookup<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_model>)>>('llama_sampler_init_infill');
  late final _llama_sampler_init_infill =
      _llama_sampler_init_infillPtr.asFunction<
          ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_model>)>();

  /// Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
  int llama_sampler_get_seed(
    ffi.Pointer<llama_sampler> smpl,
  ) {
    return _llama_sampler_get_seed(
      smpl,
    );
  }

  late final _llama_sampler_get_seedPtr = _lookup<
          ffi.NativeFunction<ffi.Uint32 Function(ffi.Pointer<llama_sampler>)>>(
      'llama_sampler_get_seed');
  late final _llama_sampler_get_seed = _llama_sampler_get_seedPtr
      .asFunction<int Function(ffi.Pointer<llama_sampler>)>();

  /// @details Sample and accept a token from the idx-th output of the last evaluation
  ///
  /// Shorthand for:
  /// const auto * logits = llama_get_logits_ith(ctx, idx);
  /// llama_token_data_array cur_p = { ... init from logits ... };
  /// llama_sampler_apply(smpl, &cur_p);
  /// auto token = cur_p.data[cur_p.selected].id;
  /// llama_sampler_accept(smpl, token);
  /// return token;
  /// Returns the sampled token
  int llama_sampler_sample(
    ffi.Pointer<llama_sampler> smpl,
    ffi.Pointer<llama_context> ctx,
    int idx,
  ) {
    return _llama_sampler_sample(
      smpl,
      ctx,
      idx,
    );
  }

  late final _llama_sampler_samplePtr = _lookup<
      ffi.NativeFunction<
          llama_token Function(ffi.Pointer<llama_sampler>,
              ffi.Pointer<llama_context>, ffi.Int32)>>('llama_sampler_sample');
  late final _llama_sampler_sample = _llama_sampler_samplePtr.asFunction<
      int Function(
          ffi.Pointer<llama_sampler>, ffi.Pointer<llama_context>, int)>();

  /// @details Build a split GGUF final path for this chunk.
  /// llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf"
  /// Returns the split_path length.
  int llama_split_path(
    ffi.Pointer<ffi.Char> split_path,
    int maxlen,
    ffi.Pointer<ffi.Char> path_prefix,
    int split_no,
    int split_count,
  ) {
    return _llama_split_path(
      split_path,
      maxlen,
      path_prefix,
      split_no,
      split_count,
    );
  }

  late final _llama_split_pathPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size,
              ffi.Pointer<ffi.Char>, ffi.Int, ffi.Int)>>('llama_split_path');
  late final _llama_split_path = _llama_split_pathPtr.asFunction<
      int Function(
          ffi.Pointer<ffi.Char>, int, ffi.Pointer<ffi.Char>, int, int)>();

  /// @details Extract the path prefix from the split_path if and only if the split_no and split_count match.
  /// llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0"
  /// Returns the split_prefix length.
  int llama_split_prefix(
    ffi.Pointer<ffi.Char> split_prefix,
    int maxlen,
    ffi.Pointer<ffi.Char> split_path,
    int split_no,
    int split_count,
  ) {
    return _llama_split_prefix(
      split_prefix,
      maxlen,
      split_path,
      split_no,
      split_count,
    );
  }

  late final _llama_split_prefixPtr = _lookup<
      ffi.NativeFunction<
          ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Size,
              ffi.Pointer<ffi.Char>, ffi.Int, ffi.Int)>>('llama_split_prefix');
  late final _llama_split_prefix = _llama_split_prefixPtr.asFunction<
      int Function(
          ffi.Pointer<ffi.Char>, int, ffi.Pointer<ffi.Char>, int, int)>();

  /// Print system information
  ffi.Pointer<ffi.Char> llama_print_system_info() {
    return _llama_print_system_info();
  }

  late final _llama_print_system_infoPtr =
      _lookup<ffi.NativeFunction<ffi.Pointer<ffi.Char> Function()>>(
          'llama_print_system_info');
  late final _llama_print_system_info = _llama_print_system_infoPtr
      .asFunction<ffi.Pointer<ffi.Char> Function()>();

  /// Set callback for all future logging events.
  /// If this is not called, or NULL is supplied, everything is output on stderr.
  void llama_log_set(
    ggml_log_callback log_callback,
    ffi.Pointer<ffi.Void> user_data,
  ) {
    return _llama_log_set(
      log_callback,
      user_data,
    );
  }

  late final _llama_log_setPtr = _lookup<
      ffi.NativeFunction<
          ffi.Void Function(
              ggml_log_callback, ffi.Pointer<ffi.Void>)>>('llama_log_set');
  late final _llama_log_set = _llama_log_setPtr
      .asFunction<void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>();

  llama_perf_context_data llama_perf_context(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_perf_context(
      ctx,
    );
  }

  late final _llama_perf_contextPtr = _lookup<
      ffi.NativeFunction<
          llama_perf_context_data Function(
              ffi.Pointer<llama_context>)>>('llama_perf_context');
  late final _llama_perf_context = _llama_perf_contextPtr.asFunction<
      llama_perf_context_data Function(ffi.Pointer<llama_context>)>();

  void llama_perf_context_print(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_perf_context_print(
      ctx,
    );
  }

  late final _llama_perf_context_printPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_perf_context_print');
  late final _llama_perf_context_print = _llama_perf_context_printPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  void llama_perf_context_reset(
    ffi.Pointer<llama_context> ctx,
  ) {
    return _llama_perf_context_reset(
      ctx,
    );
  }

  late final _llama_perf_context_resetPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_context>)>>(
      'llama_perf_context_reset');
  late final _llama_perf_context_reset = _llama_perf_context_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_context>)>();

  /// NOTE: the following work only with samplers constructed via llama_sampler_chain_init
  llama_perf_sampler_data llama_perf_sampler(
    ffi.Pointer<llama_sampler> chain,
  ) {
    return _llama_perf_sampler(
      chain,
    );
  }

  late final _llama_perf_samplerPtr = _lookup<
      ffi.NativeFunction<
          llama_perf_sampler_data Function(
              ffi.Pointer<llama_sampler>)>>('llama_perf_sampler');
  late final _llama_perf_sampler = _llama_perf_samplerPtr.asFunction<
      llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)>();

  void llama_perf_sampler_print(
    ffi.Pointer<llama_sampler> chain,
  ) {
    return _llama_perf_sampler_print(
      chain,
    );
  }

  late final _llama_perf_sampler_printPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>>(
      'llama_perf_sampler_print');
  late final _llama_perf_sampler_print = _llama_perf_sampler_printPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();

  void llama_perf_sampler_reset(
    ffi.Pointer<llama_sampler> chain,
  ) {
    return _llama_perf_sampler_reset(
      chain,
    );
  }

  late final _llama_perf_sampler_resetPtr = _lookup<
          ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler>)>>(
      'llama_perf_sampler_reset');
  late final _llama_perf_sampler_reset = _llama_perf_sampler_resetPtr
      .asFunction<void Function(ffi.Pointer<llama_sampler>)>();
}

abstract class ggml_status {
  static const int GGML_STATUS_ALLOC_FAILED = -2;
  static const int GGML_STATUS_FAILED = -1;
  static const int GGML_STATUS_SUCCESS = 0;
  static const int GGML_STATUS_ABORTED = 1;
}

/// ieee 754-2008 half-precision float16
/// todo: make this not an integral type
typedef ggml_fp16_t = ffi.Uint16;
typedef Dartggml_fp16_t = int;

/// google brain half-precision bfloat16
final class ggml_bf16_t extends ffi.Struct {
  @ffi.Uint16()
  external int bits;
}

final class ggml_object extends ffi.Opaque {}

final class ggml_context extends ffi.Opaque {}

final class ggml_cgraph extends ffi.Opaque {}

/// NOTE: always add types at the end of the enum to keep backward compatibility
abstract class ggml_type {
  static const int GGML_TYPE_F32 = 0;
  static const int GGML_TYPE_F16 = 1;
  static const int GGML_TYPE_Q4_0 = 2;
  static const int GGML_TYPE_Q4_1 = 3;

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 = 5, support has been removed
  static const int GGML_TYPE_Q5_0 = 6;
  static const int GGML_TYPE_Q5_1 = 7;
  static const int GGML_TYPE_Q8_0 = 8;
  static const int GGML_TYPE_Q8_1 = 9;
  static const int GGML_TYPE_Q2_K = 10;
  static const int GGML_TYPE_Q3_K = 11;
  static const int GGML_TYPE_Q4_K = 12;
  static const int GGML_TYPE_Q5_K = 13;
  static const int GGML_TYPE_Q6_K = 14;
  static const int GGML_TYPE_Q8_K = 15;
  static const int GGML_TYPE_IQ2_XXS = 16;
  static const int GGML_TYPE_IQ2_XS = 17;
  static const int GGML_TYPE_IQ3_XXS = 18;
  static const int GGML_TYPE_IQ1_S = 19;
  static const int GGML_TYPE_IQ4_NL = 20;
  static const int GGML_TYPE_IQ3_S = 21;
  static const int GGML_TYPE_IQ2_S = 22;
  static const int GGML_TYPE_IQ4_XS = 23;
  static const int GGML_TYPE_I8 = 24;
  static const int GGML_TYPE_I16 = 25;
  static const int GGML_TYPE_I32 = 26;
  static const int GGML_TYPE_I64 = 27;
  static const int GGML_TYPE_F64 = 28;
  static const int GGML_TYPE_IQ1_M = 29;
  static const int GGML_TYPE_BF16 = 30;
  static const int GGML_TYPE_Q4_0_4_4 = 31;
  static const int GGML_TYPE_Q4_0_4_8 = 32;
  static const int GGML_TYPE_Q4_0_8_8 = 33;
  static const int GGML_TYPE_TQ1_0 = 34;
  static const int GGML_TYPE_TQ2_0 = 35;
  static const int GGML_TYPE_IQ4_NL_4_4 = 36;

  /// GGML_TYPE_IQ4_NL_4_8 = 37,
  /// GGML_TYPE_IQ4_NL_8_8 = 38,
  static const int GGML_TYPE_COUNT = 37;
}

/// precision
abstract class ggml_prec {
  static const int GGML_PREC_DEFAULT = 0;
  static const int GGML_PREC_F32 = 1;
}

abstract class ggml_backend_type {
  static const int GGML_BACKEND_TYPE_CPU = 0;
  static const int GGML_BACKEND_TYPE_GPU = 10;
  static const int GGML_BACKEND_TYPE_GPU_SPLIT = 20;
}

/// model file types
abstract class ggml_ftype {
  static const int GGML_FTYPE_UNKNOWN = -1;
  static const int GGML_FTYPE_ALL_F32 = 0;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_F16 = 1;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_0 = 2;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_1 = 3;

  /// tok_embeddings.weight and output.weight are F16
  static const int GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q8_0 = 7;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q5_0 = 8;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q5_1 = 9;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q2_K = 10;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q3_K = 11;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_K = 12;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q5_K = 13;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q6_K = 14;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ2_XXS = 15;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ2_XS = 16;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ3_XXS = 17;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ1_S = 18;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ4_NL = 19;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ3_S = 20;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ2_S = 21;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ4_XS = 22;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_IQ1_M = 23;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_BF16 = 24;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_0_4_4 = 25;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_0_4_8 = 26;

  /// except 1d tensors
  static const int GGML_FTYPE_MOSTLY_Q4_0_8_8 = 27;
}

/// available tensor operations:
abstract class ggml_op {
  static const int GGML_OP_NONE = 0;
  static const int GGML_OP_DUP = 1;
  static const int GGML_OP_ADD = 2;
  static const int GGML_OP_ADD1 = 3;
  static const int GGML_OP_ACC = 4;
  static const int GGML_OP_SUB = 5;
  static const int GGML_OP_MUL = 6;
  static const int GGML_OP_DIV = 7;
  static const int GGML_OP_SQR = 8;
  static const int GGML_OP_SQRT = 9;
  static const int GGML_OP_LOG = 10;
  static const int GGML_OP_SIN = 11;
  static const int GGML_OP_COS = 12;
  static const int GGML_OP_SUM = 13;
  static const int GGML_OP_SUM_ROWS = 14;
  static const int GGML_OP_MEAN = 15;
  static const int GGML_OP_ARGMAX = 16;
  static const int GGML_OP_COUNT_EQUAL = 17;
  static const int GGML_OP_REPEAT = 18;
  static const int GGML_OP_REPEAT_BACK = 19;
  static const int GGML_OP_CONCAT = 20;
  static const int GGML_OP_SILU_BACK = 21;

  /// normalize
  static const int GGML_OP_NORM = 22;
  static const int GGML_OP_RMS_NORM = 23;
  static const int GGML_OP_RMS_NORM_BACK = 24;
  static const int GGML_OP_GROUP_NORM = 25;
  static const int GGML_OP_MUL_MAT = 26;
  static const int GGML_OP_MUL_MAT_ID = 27;
  static const int GGML_OP_OUT_PROD = 28;
  static const int GGML_OP_SCALE = 29;
  static const int GGML_OP_SET = 30;
  static const int GGML_OP_CPY = 31;
  static const int GGML_OP_CONT = 32;
  static const int GGML_OP_RESHAPE = 33;
  static const int GGML_OP_VIEW = 34;
  static const int GGML_OP_PERMUTE = 35;
  static const int GGML_OP_TRANSPOSE = 36;
  static const int GGML_OP_GET_ROWS = 37;
  static const int GGML_OP_GET_ROWS_BACK = 38;
  static const int GGML_OP_DIAG = 39;
  static const int GGML_OP_DIAG_MASK_INF = 40;
  static const int GGML_OP_DIAG_MASK_ZERO = 41;
  static const int GGML_OP_SOFT_MAX = 42;
  static const int GGML_OP_SOFT_MAX_BACK = 43;
  static const int GGML_OP_ROPE = 44;
  static const int GGML_OP_ROPE_BACK = 45;
  static const int GGML_OP_CLAMP = 46;
  static const int GGML_OP_CONV_TRANSPOSE_1D = 47;
  static const int GGML_OP_IM2COL = 48;
  static const int GGML_OP_IM2COL_BACK = 49;
  static const int GGML_OP_CONV_TRANSPOSE_2D = 50;
  static const int GGML_OP_POOL_1D = 51;
  static const int GGML_OP_POOL_2D = 52;
  static const int GGML_OP_POOL_2D_BACK = 53;

  /// nearest interpolate
  static const int GGML_OP_UPSCALE = 54;
  static const int GGML_OP_PAD = 55;
  static const int GGML_OP_PAD_REFLECT_1D = 56;
  static const int GGML_OP_ARANGE = 57;
  static const int GGML_OP_TIMESTEP_EMBEDDING = 58;
  static const int GGML_OP_ARGSORT = 59;
  static const int GGML_OP_LEAKY_RELU = 60;
  static const int GGML_OP_FLASH_ATTN_EXT = 61;
  static const int GGML_OP_FLASH_ATTN_BACK = 62;
  static const int GGML_OP_SSM_CONV = 63;
  static const int GGML_OP_SSM_SCAN = 64;
  static const int GGML_OP_WIN_PART = 65;
  static const int GGML_OP_WIN_UNPART = 66;
  static const int GGML_OP_GET_REL_POS = 67;
  static const int GGML_OP_ADD_REL_POS = 68;
  static const int GGML_OP_RWKV_WKV6 = 69;
  static const int GGML_OP_UNARY = 70;
  static const int GGML_OP_MAP_UNARY = 71;
  static const int GGML_OP_MAP_BINARY = 72;
  static const int GGML_OP_MAP_CUSTOM1_F32 = 73;
  static const int GGML_OP_MAP_CUSTOM2_F32 = 74;
  static const int GGML_OP_MAP_CUSTOM3_F32 = 75;
  static const int GGML_OP_MAP_CUSTOM1 = 76;
  static const int GGML_OP_MAP_CUSTOM2 = 77;
  static const int GGML_OP_MAP_CUSTOM3 = 78;
  static const int GGML_OP_CROSS_ENTROPY_LOSS = 79;
  static const int GGML_OP_CROSS_ENTROPY_LOSS_BACK = 80;
  static const int GGML_OP_OPT_STEP_ADAMW = 81;
  static const int GGML_OP_COUNT = 82;
}

abstract class ggml_unary_op {
  static const int GGML_UNARY_OP_ABS = 0;
  static const int GGML_UNARY_OP_SGN = 1;
  static const int GGML_UNARY_OP_NEG = 2;
  static const int GGML_UNARY_OP_STEP = 3;
  static const int GGML_UNARY_OP_TANH = 4;
  static const int GGML_UNARY_OP_ELU = 5;
  static const int GGML_UNARY_OP_RELU = 6;
  static const int GGML_UNARY_OP_SIGMOID = 7;
  static const int GGML_UNARY_OP_GELU = 8;
  static const int GGML_UNARY_OP_GELU_QUICK = 9;
  static const int GGML_UNARY_OP_SILU = 10;
  static const int GGML_UNARY_OP_HARDSWISH = 11;
  static const int GGML_UNARY_OP_HARDSIGMOID = 12;
  static const int GGML_UNARY_OP_EXP = 13;
  static const int GGML_UNARY_OP_COUNT = 14;
}

abstract class ggml_object_type {
  static const int GGML_OBJECT_TYPE_TENSOR = 0;
  static const int GGML_OBJECT_TYPE_GRAPH = 1;
  static const int GGML_OBJECT_TYPE_WORK_BUFFER = 2;
}

abstract class ggml_log_level {
  static const int GGML_LOG_LEVEL_NONE = 0;
  static const int GGML_LOG_LEVEL_DEBUG = 1;
  static const int GGML_LOG_LEVEL_INFO = 2;
  static const int GGML_LOG_LEVEL_WARN = 3;
  static const int GGML_LOG_LEVEL_ERROR = 4;

  /// continue previous log
  static const int GGML_LOG_LEVEL_CONT = 5;
}

/// this tensor...
abstract class ggml_tensor_flag {
  /// ...is an input for the GGML compute graph
  static const int GGML_TENSOR_FLAG_INPUT = 1;

  /// ...is an output for the GGML compute graph
  static const int GGML_TENSOR_FLAG_OUTPUT = 2;

  /// ...contains trainable parameters
  static const int GGML_TENSOR_FLAG_PARAM = 4;

  /// ...defines loss for numerical optimization (multiple loss tensors add up)
  static const int GGML_TENSOR_FLAG_LOSS = 8;
}

final class ggml_init_params extends ffi.Struct {
  /// bytes
  @ffi.Size()
  external int mem_size;

  /// if NULL, memory will be allocated internally
  external ffi.Pointer<ffi.Void> mem_buffer;

  /// don't allocate memory for the tensor data
  @ffi.Bool()
  external bool no_alloc;
}

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.Int32()
  external int type;

  @ffi.Int32()
  external int backend;

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.Int32()
  external int op;

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// source tensor and offset for views
  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

final class ggml_backend_buffer extends ffi.Opaque {}

typedef ggml_guid_t = ffi.Pointer<ffi.Pointer<ffi.Uint8>>;
typedef FILE = _IO_FILE;

final class _IO_FILE extends ffi.Struct {
  @ffi.Int()
  external int _flags;

  external ffi.Pointer<ffi.Char> _IO_read_ptr;

  external ffi.Pointer<ffi.Char> _IO_read_end;

  external ffi.Pointer<ffi.Char> _IO_read_base;

  external ffi.Pointer<ffi.Char> _IO_write_base;

  external ffi.Pointer<ffi.Char> _IO_write_ptr;

  external ffi.Pointer<ffi.Char> _IO_write_end;

  external ffi.Pointer<ffi.Char> _IO_buf_base;

  external ffi.Pointer<ffi.Char> _IO_buf_end;

  external ffi.Pointer<ffi.Char> _IO_save_base;

  external ffi.Pointer<ffi.Char> _IO_backup_base;

  external ffi.Pointer<ffi.Char> _IO_save_end;

  external ffi.Pointer<_IO_marker> _markers;

  external ffi.Pointer<_IO_FILE> _chain;

  @ffi.Int()
  external int _fileno;

  @ffi.Int()
  external int _flags2;

  @__off_t()
  external int _old_offset;

  @ffi.UnsignedShort()
  external int _cur_column;

  @ffi.SignedChar()
  external int _vtable_offset;

  @ffi.Array.multi([1])
  external ffi.Array<ffi.Char> _shortbuf;

  external ffi.Pointer<_IO_lock_t> _lock;

  @__off64_t()
  external int _offset;

  external ffi.Pointer<_IO_codecvt> _codecvt;

  external ffi.Pointer<_IO_wide_data> _wide_data;

  external ffi.Pointer<_IO_FILE> _freeres_list;

  external ffi.Pointer<ffi.Void> _freeres_buf;

  @ffi.Size()
  external int __pad5;

  @ffi.Int()
  external int _mode;

  @ffi.Array.multi([20])
  external ffi.Array<ffi.Char> _unused2;
}

final class _IO_marker extends ffi.Opaque {}

typedef __off_t = ffi.Long;
typedef Dart__off_t = int;
typedef _IO_lock_t = ffi.Void;
typedef Dart_IO_lock_t = void;
typedef __off64_t = ffi.Long;
typedef Dart__off64_t = int;

final class _IO_codecvt extends ffi.Opaque {}

final class _IO_wide_data extends ffi.Opaque {}

abstract class ggml_op_pool {
  static const int GGML_OP_POOL_MAX = 0;
  static const int GGML_OP_POOL_AVG = 1;
  static const int GGML_OP_POOL_COUNT = 2;
}

/// sort rows
abstract class ggml_sort_order {
  static const int GGML_SORT_ORDER_ASC = 0;
  static const int GGML_SORT_ORDER_DESC = 1;
}

/// custom operators
typedef ggml_unary_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_unary_op_f32_tFunction>>;
typedef ggml_unary_op_f32_tFunction = ffi.Void Function(
    ffi.Int, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef Dartggml_unary_op_f32_tFunction = void Function(
    int, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef ggml_binary_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_binary_op_f32_tFunction>>;
typedef ggml_binary_op_f32_tFunction = ffi.Void Function(ffi.Int,
    ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef Dartggml_binary_op_f32_tFunction = void Function(int,
    ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>);
typedef ggml_custom1_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_f32_tFunction>>;
typedef ggml_custom1_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom1_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>);
typedef ggml_custom2_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_f32_tFunction>>;
typedef ggml_custom2_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom2_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef ggml_custom3_op_f32_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_f32_tFunction>>;
typedef ggml_custom3_op_f32_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);
typedef Dartggml_custom3_op_f32_tFunction = void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>);

/// custom operators v2
typedef ggml_custom1_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_tFunction>>;
typedef ggml_custom1_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom1_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef ggml_custom2_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_tFunction>>;
typedef ggml_custom2_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom2_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef ggml_custom3_op_t
    = ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_tFunction>>;
typedef ggml_custom3_op_tFunction = ffi.Void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    ffi.Int ith,
    ffi.Int nth,
    ffi.Pointer<ffi.Void> userdata);
typedef Dartggml_custom3_op_tFunction = void Function(
    ffi.Pointer<ggml_tensor> dst,
    ffi.Pointer<ggml_tensor> a,
    ffi.Pointer<ggml_tensor> b,
    ffi.Pointer<ggml_tensor> c,
    int ith,
    int nth,
    ffi.Pointer<ffi.Void> userdata);

/// TODO these functions were sandwiched in the old optimization interface, is there a better place for them?
typedef ggml_log_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_log_callbackFunction = ffi.Void Function(ffi.Int32 level,
    ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_log_callbackFunction = void Function(
    int level, ffi.Pointer<ffi.Char> text, ffi.Pointer<ffi.Void> user_data);

/// gguf
abstract class gguf_type {
  static const int GGUF_TYPE_UINT8 = 0;
  static const int GGUF_TYPE_INT8 = 1;
  static const int GGUF_TYPE_UINT16 = 2;
  static const int GGUF_TYPE_INT16 = 3;
  static const int GGUF_TYPE_UINT32 = 4;
  static const int GGUF_TYPE_INT32 = 5;
  static const int GGUF_TYPE_FLOAT32 = 6;
  static const int GGUF_TYPE_BOOL = 7;
  static const int GGUF_TYPE_STRING = 8;
  static const int GGUF_TYPE_ARRAY = 9;
  static const int GGUF_TYPE_UINT64 = 10;
  static const int GGUF_TYPE_INT64 = 11;
  static const int GGUF_TYPE_FLOAT64 = 12;

  /// marks the end of the enum
  static const int GGUF_TYPE_COUNT = 13;
}

final class gguf_context extends ffi.Opaque {}

final class gguf_init_params extends ffi.Struct {
  @ffi.Bool()
  external bool no_alloc;

  /// if not NULL, create a ggml_context and allocate the tensor data in it
  external ffi.Pointer<ffi.Pointer<ggml_context>> ctx;
}

final class ggml_type_traits extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type_name;

  @ffi.Int64()
  external int blck_size;

  /// interleave elements in blocks
  @ffi.Int64()
  external int blck_size_interleave;

  @ffi.Size()
  external int type_size;

  @ffi.Bool()
  external bool is_quantized;

  external ggml_to_float_t to_float;

  external ggml_from_float_t from_float_ref;
}

typedef ggml_to_float_t
    = ffi.Pointer<ffi.NativeFunction<ggml_to_float_tFunction>>;
typedef ggml_to_float_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, ffi.Int64 k);
typedef Dartggml_to_float_tFunction = void Function(
    ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, int k);
typedef ggml_from_float_t
    = ffi.Pointer<ffi.NativeFunction<ggml_from_float_tFunction>>;
typedef ggml_from_float_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, ffi.Int64 k);
typedef Dartggml_from_float_tFunction = void Function(
    ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, int k);

/// scheduling priorities
abstract class ggml_sched_priority {
  static const int GGML_SCHED_PRIO_NORMAL = 0;
  static const int GGML_SCHED_PRIO_MEDIUM = 1;
  static const int GGML_SCHED_PRIO_HIGH = 2;
  static const int GGML_SCHED_PRIO_REALTIME = 3;
}

/// threadpool params
/// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults
final class ggml_threadpool_params extends ffi.Struct {
  /// mask of cpu cores (all-zeros means use default affinity settings)
  @ffi.Array.multi([512])
  external ffi.Array<ffi.Bool> cpumask;

  /// number of threads
  @ffi.Int()
  external int n_threads;

  /// thread priority
  @ffi.Int32()
  external int prio;

  /// polling level (0 - no polling, 100 - aggressive polling)
  @ffi.Uint32()
  external int poll;

  /// strict cpu placement
  @ffi.Bool()
  external bool strict_cpu;

  /// start in paused state
  @ffi.Bool()
  external bool paused;
}

final class ggml_threadpool extends ffi.Opaque {}

final class ggml_backend_buffer_type extends ffi.Opaque {}

final class ggml_backend extends ffi.Opaque {}

/// Tensor allocator
final class ggml_tallocr extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ffi.Void> base;

  @ffi.Size()
  external int alignment;

  @ffi.Size()
  external int offset;
}

typedef ggml_backend_buffer_t = ffi.Pointer<ggml_backend_buffer>;

final class ggml_gallocr extends ffi.Opaque {}

/// special tensor flags for use with the graph allocator:
/// ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses
/// ggml_set_output(): output tensors are never freed and never overwritten
typedef ggml_gallocr_t = ffi.Pointer<ggml_gallocr>;
typedef ggml_backend_buffer_type_t = ffi.Pointer<ggml_backend_buffer_type>;
typedef ggml_backend_t = ffi.Pointer<ggml_backend>;

final class ggml_backend_event extends ffi.Opaque {}

final class ggml_backend_reg extends ffi.Opaque {}

final class ggml_backend_device extends ffi.Opaque {}

typedef ggml_backend_dev_t = ffi.Pointer<ggml_backend_device>;

/// Backend buffer
abstract class ggml_backend_buffer_usage {
  static const int GGML_BACKEND_BUFFER_USAGE_ANY = 0;
  static const int GGML_BACKEND_BUFFER_USAGE_WEIGHTS = 1;
  static const int GGML_BACKEND_BUFFER_USAGE_COMPUTE = 2;
}

typedef ggml_backend_graph_plan_t = ffi.Pointer<ffi.Void>;
typedef ggml_backend_event_t = ffi.Pointer<ggml_backend_event>;

/// Backend device
abstract class ggml_backend_dev_type {
  /// CPU device using system memory
  static const int GGML_BACKEND_DEVICE_TYPE_CPU = 0;

  /// GPU device using dedicated memory
  static const int GGML_BACKEND_DEVICE_TYPE_GPU = 1;

  /// accelerator devices intended to be used together with the CPU backend (e.g. BLAS or AMX)
  static const int GGML_BACKEND_DEVICE_TYPE_ACCEL = 2;
}

/// functionality supported by the device
final class ggml_backend_dev_caps extends ffi.Struct {
  /// asynchronous operations
  @ffi.Bool()
  external bool async1;

  /// pinned host buffer
  @ffi.Bool()
  external bool host_buffer;

  /// creating buffers from host ptr
  @ffi.Bool()
  external bool buffer_from_host_ptr;

  /// event synchronization
  @ffi.Bool()
  external bool events;
}

/// all the device properties
final class ggml_backend_dev_props extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> description;

  @ffi.Size()
  external int memory_free;

  @ffi.Size()
  external int memory_total;

  @ffi.Int32()
  external int type;

  external ggml_backend_dev_caps caps;
}

typedef ggml_backend_reg_t = ffi.Pointer<ggml_backend_reg>;

/// Get a list of feature flags supported by the backend (returns a NULL-terminated array)
final class ggml_backend_feature extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> value;
}

final class ggml_backend_sched extends ffi.Opaque {}

/// The backend scheduler allows for multiple backend devices to be used together
/// Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends
/// The backends are selected based on:
/// - the backend that supports the operation
/// - the location of the pre-allocated tensors (e.g. the weights)
///     /*
///       Example usage:
///
/// operations that use tensors allocated in a buffer with USAGE_WEIGHTS will be assigned
/// preferrably to run on the same backend as the buffer
///         ggml_backend_buffer_set_usage(buf_weights, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);
///
///         sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, NULL, num_backends, GGML_DEFAULT_GRAPH_SIZE, false);
///
/// initialize buffers from a max size graph (optional)
///         reserve_graph = build_graph(sched, max_batch_size);
///
/// manually assign nodes to a backend (optional, should not be needed in most cases)
///         struct ggml_tensor * node = ggml_mul_mat(ctx, ...);
///         ggml_backend_sched_set_tensor_backend(sched, node, backend_gpu);
///
///         ggml_backend_sched_reserve(sched, reserve_graph);
///
/// compute
///         graph = build_graph(sched); // the graph and its tensors are single-use in terms of allocation, multi-use in terms of computation
///         for (int i = 0; i < 10; ++i) {
///             ggml_backend_sched_graph_compute(sched, graph); // on the first iteration the graph is allocated automatically
///         }
///
/// if there are graph inputs:
///         graph = build_graph(sched); // get a new graph that is not allocated (the metadata for the old graph is freed once ggml_free is called)
///         ggml_backend_sched_reset(sched); // clear the allocation of the previous graph
///         ggml_backend_sched_alloc_graph(sched, graph); // explicitly allocate the new graph but do not execute it
///         ggml_backend_tensor_set(input_tensor, ...); // copy data to the newly allocated graph tensors
///         ggml_backend_sched_graph_compute(sched, graph); // execute the graph
///
/// as an alternative to the above it is also possible to assign the inputs to a dedicated context and
/// allocate them statically via ggml_backend_alloc_ctx_tensors
///     }
///     */
typedef ggml_backend_sched_t = ffi.Pointer<ggml_backend_sched>;

/// Evaluation callback for each node in the graph (set with ggml_backend_sched_set_eval_callback)
/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;
typedef ggml_backend_sched_eval_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ggml_tensor> t, ffi.Bool ask, ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_sched_eval_callbackFunction = bool Function(
    ffi.Pointer<ggml_tensor> t, bool ask, ffi.Pointer<ffi.Void> user_data);

/// Utils
final class ggml_backend_graph_copy extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ggml_context> ctx_allocated;

  external ffi.Pointer<ggml_context> ctx_unallocated;

  external ffi.Pointer<ggml_cgraph> graph;
}

typedef ggml_backend_eval_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_backend_eval_callbackFunction>>;
typedef ggml_backend_eval_callbackFunction = ffi.Bool Function(
    ffi.Int node_index,
    ffi.Pointer<ggml_tensor> t1,
    ffi.Pointer<ggml_tensor> t2,
    ffi.Pointer<ffi.Void> user_data);
typedef Dartggml_backend_eval_callbackFunction = bool Function(
    int node_index,
    ffi.Pointer<ggml_tensor> t1,
    ffi.Pointer<ggml_tensor> t2,
    ffi.Pointer<ffi.Void> user_data);

/// the compute plan that needs to be prepared for ggml_graph_compute()
/// since https://github.com/ggerganov/ggml/issues/287
final class ggml_cplan extends ffi.Struct {
  /// size of work buffer, calculated by `ggml_graph_plan()`
  @ffi.Size()
  external int work_size;

  /// work buffer, to be allocated by caller before calling to `ggml_graph_compute()`
  external ffi.Pointer<ffi.Uint8> work_data;

  @ffi.Int()
  external int n_threads;

  external ffi.Pointer<ggml_threadpool> threadpool;

  /// abort ggml_graph_compute when true
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback
    = ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_abort_callbackFunction = ffi.Bool Function(
    ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction = bool Function(
    ffi.Pointer<ffi.Void> data);

/// numa strategies
abstract class ggml_numa_strategy {
  static const int GGML_NUMA_STRATEGY_DISABLED = 0;
  static const int GGML_NUMA_STRATEGY_DISTRIBUTE = 1;
  static const int GGML_NUMA_STRATEGY_ISOLATE = 2;
  static const int GGML_NUMA_STRATEGY_NUMACTL = 3;
  static const int GGML_NUMA_STRATEGY_MIRROR = 4;
  static const int GGML_NUMA_STRATEGY_COUNT = 5;
}

final class ggml_type_traits_cpu extends ffi.Struct {
  external ggml_from_float_t from_float;

  external ggml_from_float_to_mat_t from_float_to_mat;

  external ggml_vec_dot_t vec_dot;

  @ffi.Int32()
  external int vec_dot_type;

  /// number of rows to process simultaneously
  @ffi.Int64()
  external int nrows;

  /// number of columns to process simultaneously
  @ffi.Int64()
  external int ncols;

  external ggml_gemv_t gemv;

  external ggml_gemm_t gemm;
}

/// Internal types and functions exposed for tests and benchmarks
typedef ggml_from_float_to_mat_t
    = ffi.Pointer<ffi.NativeFunction<ggml_from_float_to_mat_tFunction>>;
typedef ggml_from_float_to_mat_tFunction = ffi.Void Function(
    ffi.Pointer<ffi.Float> x,
    ffi.Pointer<ffi.Void> y,
    ffi.Int64 nr,
    ffi.Int64 k,
    ffi.Int64 bs);
typedef Dartggml_from_float_to_mat_tFunction = void Function(
    ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, int nr, int k, int bs);
typedef ggml_vec_dot_t
    = ffi.Pointer<ffi.NativeFunction<ggml_vec_dot_tFunction>>;
typedef ggml_vec_dot_tFunction = ffi.Void Function(
    ffi.Int n,
    ffi.Pointer<ffi.Float> s,
    ffi.Size bs,
    ffi.Pointer<ffi.Void> x,
    ffi.Size bx,
    ffi.Pointer<ffi.Void> y,
    ffi.Size by,
    ffi.Int nrc);
typedef Dartggml_vec_dot_tFunction = void Function(
    int n,
    ffi.Pointer<ffi.Float> s,
    int bs,
    ffi.Pointer<ffi.Void> x,
    int bx,
    ffi.Pointer<ffi.Void> y,
    int by,
    int nrc);
typedef ggml_gemv_t = ffi.Pointer<ffi.NativeFunction<ggml_gemv_tFunction>>;
typedef ggml_gemv_tFunction = ffi.Void Function(
    ffi.Int n,
    ffi.Pointer<ffi.Float> s,
    ffi.Size bs,
    ffi.Pointer<ffi.Void> x,
    ffi.Pointer<ffi.Void> y,
    ffi.Int nr,
    ffi.Int nc);
typedef Dartggml_gemv_tFunction = void Function(int n, ffi.Pointer<ffi.Float> s,
    int bs, ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Void> y, int nr, int nc);
typedef ggml_gemm_t = ffi.Pointer<ffi.NativeFunction<ggml_gemm_tFunction>>;
typedef ggml_gemm_tFunction = ffi.Void Function(
    ffi.Int n,
    ffi.Pointer<ffi.Float> s,
    ffi.Size bs,
    ffi.Pointer<ffi.Void> x,
    ffi.Pointer<ffi.Void> y,
    ffi.Int nr,
    ffi.Int nc);
typedef Dartggml_gemm_tFunction = void Function(int n, ffi.Pointer<ffi.Float> s,
    int bs, ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Void> y, int nr, int nc);
typedef ggml_threadpool_t = ffi.Pointer<ggml_threadpool>;

/// struct llama_vocab; // TODO: add in the future
final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

final class llama_sampler extends ffi.Struct {
  external ffi.Pointer<llama_sampler_i> iface;

  external llama_sampler_context_t ctx;
}

/// user code can implement the interface below in order to create custom llama_sampler
final class llama_sampler_i extends ffi.Struct {
  /// can be NULL
  external ffi.Pointer<
          ffi.NativeFunction<
              ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler> smpl)>>
      name;

  /// can be NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(
              ffi.Pointer<llama_sampler> smpl, llama_token token)>> accept;

  /// required
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Void Function(ffi.Pointer<llama_sampler> smpl,
              ffi.Pointer<llama_token_data_array> cur_p)>> apply;

  /// can be NULL
  external ffi.Pointer<
          ffi
          .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>>
      reset;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi.NativeFunction<
          ffi.Pointer<llama_sampler> Function(
              ffi.Pointer<llama_sampler> smpl)>> clone;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
      ffi
      .NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>> free;
}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

final class llama_token_data_array extends ffi.Struct {
  /// TODO: consider SoA
  /// NOTE: this pointer can be modified by the samplers
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  /// this is the index in the data array (i.e. not the token id)
  @ffi.Int64()
  external int selected;

  @ffi.Bool()
  external bool sorted;
}

/// TODO: simplify (https://github.com/ggerganov/llama.cpp/pull/9294#pullrequestreview-2286561979)
final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

/// Sampling API
///
/// Sample usage:
///
/// // prepare the sampling chain at the start
/// auto sparams = llama_sampler_chain_default_params();
///
/// llama_sampler * smpl = llama_sampler_chain_init(sparams);
///
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_k(50));
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_p(0.9, 1));
/// llama_sampler_chain_add(smpl, llama_sampler_init_temp (0.8));
///
/// // typically, the chain should end with a sampler such as "greedy", "dist" or "mirostat"
/// // this sampler will be responsible to select the actual token
/// llama_sampler_chain_add(smpl, llama_sampler_init_dist(seed));
///
/// ...
///
/// // decoding loop:
/// while (...) {
/// ...
///
/// llama_decode(ctx, batch);
///
/// // sample from the logits of the last token in the batch
/// const llama_token id = llama_sampler_sample(smpl, ctx, -1);
///
/// // accepting the token updates the internal state of certain samplers (e.g. grammar, repetition, etc.)
/// llama_sampler_accept(smpl, id);
/// ...
/// }
///
/// llama_sampler_free(smpl);
///
/// TODO: In the future, llama_sampler will be utilized to offload the sampling to the backends (e.g. GPU).
/// TODO: in the future, the entire sampling API that uses llama_model should start using llama_vocab
typedef llama_sampler_context_t = ffi.Pointer<ffi.Void>;

abstract class llama_vocab_type {
  /// For models without vocab
  static const int LLAMA_VOCAB_TYPE_NONE = 0;

  /// LLaMA tokenizer based on byte-level BPE with byte fallback
  static const int LLAMA_VOCAB_TYPE_SPM = 1;

  /// GPT-2 tokenizer based on byte-level BPE
  static const int LLAMA_VOCAB_TYPE_BPE = 2;

  /// BERT tokenizer based on WordPiece
  static const int LLAMA_VOCAB_TYPE_WPM = 3;

  /// T5 tokenizer based on Unigram
  static const int LLAMA_VOCAB_TYPE_UGM = 4;

  /// RWKV tokenizer based on greedy tokenization
  static const int LLAMA_VOCAB_TYPE_RWKV = 5;
}

/// pre-tokenization types
abstract class llama_vocab_pre_type {
  static const int LLAMA_VOCAB_PRE_TYPE_DEFAULT = 0;
  static const int LLAMA_VOCAB_PRE_TYPE_LLAMA3 = 1;
  static const int LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_LLM = 2;
  static const int LLAMA_VOCAB_PRE_TYPE_DEEPSEEK_CODER = 3;
  static const int LLAMA_VOCAB_PRE_TYPE_FALCON = 4;
  static const int LLAMA_VOCAB_PRE_TYPE_MPT = 5;
  static const int LLAMA_VOCAB_PRE_TYPE_STARCODER = 6;
  static const int LLAMA_VOCAB_PRE_TYPE_GPT2 = 7;
  static const int LLAMA_VOCAB_PRE_TYPE_REFACT = 8;
  static const int LLAMA_VOCAB_PRE_TYPE_COMMAND_R = 9;
  static const int LLAMA_VOCAB_PRE_TYPE_STABLELM2 = 10;
  static const int LLAMA_VOCAB_PRE_TYPE_QWEN2 = 11;
  static const int LLAMA_VOCAB_PRE_TYPE_OLMO = 12;
  static const int LLAMA_VOCAB_PRE_TYPE_DBRX = 13;
  static const int LLAMA_VOCAB_PRE_TYPE_SMAUG = 14;
  static const int LLAMA_VOCAB_PRE_TYPE_PORO = 15;
  static const int LLAMA_VOCAB_PRE_TYPE_CHATGLM3 = 16;
  static const int LLAMA_VOCAB_PRE_TYPE_CHATGLM4 = 17;
  static const int LLAMA_VOCAB_PRE_TYPE_VIKING = 18;
  static const int LLAMA_VOCAB_PRE_TYPE_JAIS = 19;
  static const int LLAMA_VOCAB_PRE_TYPE_TEKKEN = 20;
  static const int LLAMA_VOCAB_PRE_TYPE_SMOLLM = 21;
  static const int LLAMA_VOCAB_PRE_TYPE_CODESHELL = 22;
  static const int LLAMA_VOCAB_PRE_TYPE_BLOOM = 23;
  static const int LLAMA_VOCAB_PRE_TYPE_GPT3_FINNISH = 24;
  static const int LLAMA_VOCAB_PRE_TYPE_EXAONE = 25;
  static const int LLAMA_VOCAB_PRE_TYPE_CHAMELEON = 26;
  static const int LLAMA_VOCAB_PRE_TYPE_MINERVA = 27;
}

abstract class llama_rope_type {
  static const int LLAMA_ROPE_TYPE_NONE = -1;
  static const int LLAMA_ROPE_TYPE_NORM = 0;
  static const int LLAMA_ROPE_TYPE_NEOX = 2;
}

abstract class llama_token_type {
  static const int LLAMA_TOKEN_TYPE_UNDEFINED = 0;
  static const int LLAMA_TOKEN_TYPE_NORMAL = 1;
  static const int LLAMA_TOKEN_TYPE_UNKNOWN = 2;
  static const int LLAMA_TOKEN_TYPE_CONTROL = 3;
  static const int LLAMA_TOKEN_TYPE_USER_DEFINED = 4;
  static const int LLAMA_TOKEN_TYPE_UNUSED = 5;
  static const int LLAMA_TOKEN_TYPE_BYTE = 6;
}

abstract class llama_token_attr {
  static const int LLAMA_TOKEN_ATTR_UNDEFINED = 0;
  static const int LLAMA_TOKEN_ATTR_UNKNOWN = 1;
  static const int LLAMA_TOKEN_ATTR_UNUSED = 2;
  static const int LLAMA_TOKEN_ATTR_NORMAL = 4;

  /// SPECIAL?
  static const int LLAMA_TOKEN_ATTR_CONTROL = 8;
  static const int LLAMA_TOKEN_ATTR_USER_DEFINED = 16;
  static const int LLAMA_TOKEN_ATTR_BYTE = 32;
  static const int LLAMA_TOKEN_ATTR_NORMALIZED = 64;
  static const int LLAMA_TOKEN_ATTR_LSTRIP = 128;
  static const int LLAMA_TOKEN_ATTR_RSTRIP = 256;
  static const int LLAMA_TOKEN_ATTR_SINGLE_WORD = 512;
}

/// model file types
abstract class llama_ftype {
  static const int LLAMA_FTYPE_ALL_F32 = 0;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_F16 = 1;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0 = 2;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_1 = 3;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q8_0 = 7;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_0 = 8;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_1 = 9;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K = 10;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_S = 11;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_M = 12;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q3_K_L = 13;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_S = 14;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_K_M = 15;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_S = 16;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q5_K_M = 17;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q6_K = 18;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XXS = 19;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_XS = 20;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q2_K_S = 21;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XS = 22;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_XXS = 23;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ1_S = 24;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_NL = 25;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_S = 26;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ3_M = 27;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_S = 28;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ2_M = 29;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ4_XS = 30;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_IQ1_M = 31;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_BF16 = 32;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0_4_4 = 33;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0_4_8 = 34;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_Q4_0_8_8 = 35;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_TQ1_0 = 36;

  /// except 1d tensors
  static const int LLAMA_FTYPE_MOSTLY_TQ2_0 = 37;

  /// not specified in the model file
  static const int LLAMA_FTYPE_GUESSED = 1024;
}

abstract class llama_rope_scaling_type {
  static const int LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_ROPE_SCALING_TYPE_NONE = 0;
  static const int LLAMA_ROPE_SCALING_TYPE_LINEAR = 1;
  static const int LLAMA_ROPE_SCALING_TYPE_YARN = 2;
  static const int LLAMA_ROPE_SCALING_TYPE_LONGROPE = 3;
  static const int LLAMA_ROPE_SCALING_TYPE_MAX_VALUE = 3;
}

abstract class llama_pooling_type {
  static const int LLAMA_POOLING_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_POOLING_TYPE_NONE = 0;
  static const int LLAMA_POOLING_TYPE_MEAN = 1;
  static const int LLAMA_POOLING_TYPE_CLS = 2;
  static const int LLAMA_POOLING_TYPE_LAST = 3;

  /// used by reranking models to attach the classification head to the graph
  static const int LLAMA_POOLING_TYPE_RANK = 4;
}

abstract class llama_attention_type {
  static const int LLAMA_ATTENTION_TYPE_UNSPECIFIED = -1;
  static const int LLAMA_ATTENTION_TYPE_CAUSAL = 0;
  static const int LLAMA_ATTENTION_TYPE_NON_CAUSAL = 1;
}

abstract class llama_split_mode {
  /// single GPU
  static const int LLAMA_SPLIT_MODE_NONE = 0;

  /// split layers and KV across GPUs
  static const int LLAMA_SPLIT_MODE_LAYER = 1;

  /// split layers and KV across GPUs, use tensor parallelism if supported
  static const int LLAMA_SPLIT_MODE_ROW = 2;
}

/// Input data for llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// (if set to NULL, the token position will be tracked automatically by llama_decode)
/// - seq_id : the sequence to which the respective token belongs
/// (if set to NULL, the sequence ID will be assumed to be 0)
/// - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output
/// (if set to NULL, only the logits for last token will be returned)
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  /// TODO: rename this to "output"
  external ffi.Pointer<ffi.Int8> logits;
}

typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

abstract class llama_model_kv_override_type {
  static const int LLAMA_KV_OVERRIDE_TYPE_INT = 0;
  static const int LLAMA_KV_OVERRIDE_TYPE_FLOAT = 1;
  static const int LLAMA_KV_OVERRIDE_TYPE_BOOL = 2;
  static const int LLAMA_KV_OVERRIDE_TYPE_STR = 3;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.Int32()
  external int tag;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  external UnnamedUnion1 unnamed;
}

final class UnnamedUnion1 extends ffi.Union {
  @ffi.Int64()
  external int val_i64;

  @ffi.Double()
  external double val_f64;

  @ffi.Bool()
  external bool val_bool;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> val_str;
}

final class llama_model_params extends ffi.Struct {
  /// NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
  external ffi.Pointer<ggml_backend_dev_t> devices;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.Int32()
  external int split_mode;

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// comma separated list of RPC servers to use for offloading
  external ffi.Pointer<ffi.Char> rpc_servers;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;
}

typedef llama_progress_callback
    = ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;
typedef llama_progress_callbackFunction = ffi.Bool Function(
    ffi.Float progress, ffi.Pointer<ffi.Void> user_data);
typedef Dartllama_progress_callbackFunction = bool Function(
    double progress, ffi.Pointer<ffi.Void> user_data);

/// NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
/// https://github.com/ggerganov/llama.cpp/pull/7544
final class llama_context_params extends ffi.Struct {
  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// logical maximum batch size that can be submitted to llama_decode
  @ffi.Uint32()
  external int n_batch;

  /// physical maximum batch size
  @ffi.Uint32()
  external int n_ubatch;

  /// max number of sequences (i.e. distinct states for recurrent models)
  @ffi.Uint32()
  external int n_seq_max;

  /// number of threads to use for generation
  @ffi.Int32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Int32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int32()
  external int rope_scaling_type;

  /// whether to pool (sum) embedding results by sequence id
  @ffi.Int32()
  external int pooling_type;

  /// attention type to use for embeddings
  @ffi.Int32()
  external int attention_type;

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// defragment the KV cache if holes/size > thold, < 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache [EXPERIMENTAL]
  @ffi.Int32()
  external int type_k;

  /// data type for V cache [EXPERIMENTAL]
  @ffi.Int32()
  external int type_v;

  /// the llama_decode() call computes all logits, not just the last one (DEPRECATED - set llama_batch.logits instead)
  @ffi.Bool()
  external bool logits_all;

  /// if true, extract embeddings (together with logits)
  @ffi.Bool()
  external bool embeddings;

  /// whether to offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// whether to use flash attention [EXPERIMENTAL]
  @ffi.Bool()
  external bool flash_attn;

  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.Int32()
  external int ftype;

  /// output tensor type
  @ffi.Int32()
  external int output_tensor_type;

  /// token embeddings tensor type
  @ffi.Int32()
  external int token_embedding_type;

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// quantize all tensors to the default type
  @ffi.Bool()
  external bool pure;

  /// quantize to the same number of shards
  @ffi.Bool()
  external bool keep_split;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;

  /// pointer to vector containing overrides
  external ffi.Pointer<ffi.Void> kv_overrides;
}

final class llama_logit_bias extends ffi.Struct {
  @llama_token()
  external int token;

  @ffi.Float()
  external double bias;
}

final class llama_sampler_chain_params extends ffi.Struct {
  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// lora adapter
final class llama_lora_adapter extends ffi.Opaque {}

/// Information associated with an individual cell in the KV cache view.
final class llama_kv_cache_view_cell extends ffi.Struct {
  /// The position for this cell. Takes KV cache shifts into account.
  /// May be negative if the cell is not populated.
  @llama_pos()
  external int pos;
}

/// An updateable view of the KV cache.
final class llama_kv_cache_view extends ffi.Struct {
  /// Number of KV cache cells. This will be the same as the context size.
  @ffi.Int32()
  external int n_cells;

  /// Maximum number of sequences that can exist in a cell. It's not an error
  /// if there are more sequences in a cell than this value, however they will
  /// not be visible in the view cells_sequences.
  @ffi.Int32()
  external int n_seq_max;

  /// Number of tokens in the cache. For example, if there are two populated
  /// cells, the first with 1 sequence id in it and the second with 2 sequence
  /// ids then you'll have 3 tokens.
  @ffi.Int32()
  external int token_count;

  /// Number of populated cache cells.
  @ffi.Int32()
  external int used_cells;

  /// Maximum contiguous empty slots in the cache.
  @ffi.Int32()
  external int max_contiguous;

  /// Index to the start of the max_contiguous slot range. Can be negative
  /// when cache is full.
  @ffi.Int32()
  external int max_contiguous_idx;

  /// Information for an individual cell.
  external ffi.Pointer<llama_kv_cache_view_cell> cells;

  /// The sequences for each cell. There will be n_seq_max items per cell.
  external ffi.Pointer<llama_seq_id> cells_sequences;
}

/// Performance utils
///
/// NOTE: Used by llama.cpp examples, avoid using in third-party apps. Instead, do your own performance measurements.
final class llama_perf_context_data extends ffi.Struct {
  @ffi.Double()
  external double t_start_ms;

  @ffi.Double()
  external double t_load_ms;

  @ffi.Double()
  external double t_p_eval_ms;

  @ffi.Double()
  external double t_eval_ms;

  @ffi.Int32()
  external int n_p_eval;

  @ffi.Int32()
  external int n_eval;
}

final class llama_perf_sampler_data extends ffi.Struct {
  @ffi.Double()
  external double t_sample_ms;

  @ffi.Int32()
  external int n_sample;
}

const int GGML_FILE_MAGIC = 1734831468;

const int GGML_FILE_VERSION = 2;

const int GGML_QNT_VERSION = 2;

const int GGML_QNT_VERSION_FACTOR = 1000;

const int GGML_MAX_DIMS = 4;

const int GGML_MAX_PARAMS = 2048;

const int GGML_MAX_SRC = 10;

const int GGML_MAX_N_THREADS = 512;

const int GGML_MAX_OP_PARAMS = 64;

const int GGML_MAX_NAME = 64;

const int GGML_DEFAULT_N_THREADS = 4;

const int GGML_DEFAULT_GRAPH_SIZE = 2048;

const int GGML_MEM_ALIGN = 16;

const int GGML_EXIT_SUCCESS = 0;

const int GGML_EXIT_ABORTED = 1;

const int GGML_ROPE_TYPE_NEOX = 2;

const String GGUF_MAGIC = 'GGUF';

const int GGUF_VERSION = 3;

const int GGUF_DEFAULT_ALIGNMENT = 32;

const int GGML_KQ_MASK_PAD = 32;

const int GGML_N_TASKS_MAX = -1;

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_TOKEN_NULL = -1;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_FILE_MAGIC_GGSQ = 1734833009;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 9;

const int LLAMA_STATE_SEQ_MAGIC = 1734833009;

const int LLAMA_STATE_SEQ_VERSION = 2;
